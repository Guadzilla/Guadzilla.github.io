<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"guadzilla.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Task02&amp;emsp;&amp;emsp;Task02：精排模型 DeepFM DIN &amp;emsp;&amp;emsp;参考资料：FunRec文档，RecHub源码 推荐模型发展的时间线 &amp;emsp;&amp;emsp;这张图来自[1]，放出这张图的原因是便于从时间线上感受这些模型的发展。本期学习的 DIN 还算是比较独立的存在，它在前面模型 DNN 思想的基础上加入了注意力机制。而 DeepFM ，从时间线上可以看到">
<meta property="og:type" content="article">
<meta property="og:title" content="RecHub推荐项目学习2：精排模型 DeepFM、DIN">
<meta property="og:url" content="http://guadzilla.github.io/2022/06/19/RecHub%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A02/index.html">
<meta property="og:site_name" content="瓜斯拉的逆袭">
<meta property="og:description" content="Task02&amp;emsp;&amp;emsp;Task02：精排模型 DeepFM DIN &amp;emsp;&amp;emsp;参考资料：FunRec文档，RecHub源码 推荐模型发展的时间线 &amp;emsp;&amp;emsp;这张图来自[1]，放出这张图的原因是便于从时间线上感受这些模型的发展。本期学习的 DIN 还算是比较独立的存在，它在前面模型 DNN 思想的基础上加入了注意力机制。而 DeepFM ，从时间线上可以看到">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618103615992.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618155318348.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618155325444.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619134325217.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101907599.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101925322.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101941306.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101451894.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619102848286.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619104157758.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619104313037.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619123835759.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619132058752.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619133002341.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619135133347.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619135849196.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619143513920.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152139173.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152209082.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152251762.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152310615.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619150451559.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152328870.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619153802390.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619154134742.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619154214878.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619134325217.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619154603820.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619155836040.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619163102998.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619222927973.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619224333801.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619224732551.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619230308830.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620012113828.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620012500712.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620014907354.png">
<meta property="og:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620022342747.png">
<meta property="article:published_time" content="2022-06-19T12:39:02.000Z">
<meta property="article:modified_time" content="2022-06-22T14:05:41.026Z">
<meta property="article:author" content="Guadzilla">
<meta property="article:tag" content="Datawhale组队学习">
<meta property="article:tag" content="RecHub">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618103615992.png">

<link rel="canonical" href="http://guadzilla.github.io/2022/06/19/RecHub%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A02/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>RecHub推荐项目学习2：精排模型 DeepFM、DIN | 瓜斯拉的逆袭</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?41fc030db57d5570dd22f78997dc4a7e";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">瓜斯拉的逆袭</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://guadzilla.github.io/2022/06/19/RecHub%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/touxiang.jpg">
      <meta itemprop="name" content="Guadzilla">
      <meta itemprop="description" content="大连理工大学在读硕士，研究方向为：NLP/序列推荐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="瓜斯拉的逆袭">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RecHub推荐项目学习2：精排模型 DeepFM、DIN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-06-19 20:39:02" itemprop="dateCreated datePublished" datetime="2022-06-19T20:39:02+08:00">2022-06-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-06-22 22:05:41" itemprop="dateModified" datetime="2022-06-22T22:05:41+08:00">2022-06-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">推荐系统</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>48 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Task02"><a href="#Task02" class="headerlink" title="Task02"></a>Task02</h1><p>&emsp;&emsp;Task02：精排模型 DeepFM DIN</p>
<p>&emsp;&emsp;参考资料：<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/fun-rec/#/">FunRec文档</a>，<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/torch-rechub">RecHub源码</a></p>
<h1 id="推荐模型发展的时间线"><a href="#推荐模型发展的时间线" class="headerlink" title="推荐模型发展的时间线"></a>推荐模型发展的时间线</h1><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618103615992.png" alt="image-20220618103615992" style="zoom:50%;" /></p>
<p>&emsp;&emsp;这张图来自[1]，放出这张图的原因是便于从时间线上感受这些模型的发展。本期学习的 DIN 还算是比较独立的存在，它在前面模型 DNN 思想的基础上加入了注意力机制。而 DeepFM ，从时间线上可以看到 DeepFM 模型是在 FM、FNN、PNN、Wide&amp;Deep 之后推出的，其实也是对这些模型的改进，为了更好地理解 DeepFM，至少得先了解它们。</p>
<span id="more"></span>
<h1 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h1><p>&emsp;&emsp;DeepFM 提出的动机主要有两点：</p>
<ul>
<li><p>CTR 预估任务中特征交叉至关重要，但是特征交叉通常需要非常专业的特征工程。例如Wide&amp;Deep的Wide部分需要手工构造pairwise特征，huge size并且费人力，复杂。</p>
</li>
<li><p>一些模型例如FNN、PNN只能学习到高阶特征组合；Wide&amp;Deep在输出层直接将低阶和高阶特征相加组合，很容易让模型最终偏向（bias）学习到低阶或者高阶的特征，而不能很好地结合。</p>
</li>
</ul>
<p>针对上面两个问题，DeepFM的解决方案分别是：</p>
<ul>
<li>改进 Wide&amp;Deep 的 Wide 部分，使用 FM 代替手工构造特征</li>
<li>Wide 部分和 Deep 部分使用相同的 embedding 输入，不会导致 bias</li>
</ul>
<h2 id="DeepFM-的前辈们"><a href="#DeepFM-的前辈们" class="headerlink" title="DeepFM 的前辈们"></a>DeepFM 的前辈们</h2><p>&emsp;&emsp;上面提到了FM、FNN、PNN、Wide&amp;Deep，为了更好地理解 DeepFM ，这里简单介绍下：FM 捕捉低阶特征组合，FNN、PNN 捕捉高阶特征组合，Wide&amp;Deep 的 Wide 部分捕捉低阶特征组合，Deep 部分捕捉高阶特征组合。更详细的介绍如下</p>
<h3 id="低阶特征组合：FM"><a href="#低阶特征组合：FM" class="headerlink" title="低阶特征组合：FM"></a>低阶特征组合：FM</h3><p>&emsp;&emsp;用[2]的例子介绍 FM ，假设一个广告分类的问题，根据用户和广告位相关的特征，预测用户是否点击了广告。源数据如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618155318348.png" alt="image-20220618155318348"></p>
<p>&emsp;&emsp;Clicked? 是 label ，Country、Day、Ad_type 是三类特征，并且都是类别型特征，需要经过 one-hot 编码才能转换为数值型特征，经过 one-hot 编码后如下所示：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618155325444.png" alt="image-20220618155325444"></p>
<p>&emsp;&emsp;可以看出，经过 one-hot 编码，每条数据都变稀疏了，每个样本有7维特征，但仅有3维特征具有非零值。在实际场景中，商品种类、用户职业、地区等类别非常多，经过 one-hot 编码后样本维度会迅速上升，且非常稀疏。</p>
<h4 id="POLY2"><a href="#POLY2" class="headerlink" title="POLY2"></a>POLY2</h4><p>&emsp;&emsp;这时候其实可以用 LR 训练了，但是通过观察大量的样本数据可以发现，某些特征经过<strong>关联</strong>之后，与label之间的相关性就会提高。例如，“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。因此，引入两个<strong>特征的组合</strong>是非常有意义的，这也就是常说的<strong>特征交叉</strong>。因此，一个非常自然的想法诞生了，在 LR 的基础上加入<strong>二阶特征组合</strong>，这也是 <strong>POLY2</strong> 的思路：</p>
<script type="math/tex; mode=display">
y(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{i j} x_{i} x_{j}</script><p>&emsp;&emsp;公式中的 $x_i \in \{0,1\}$ 表示每个特征。</p>
<p>&emsp;&emsp;这样做考虑了<strong>二阶特征交叉</strong>，但是也产生了新的问题。</p>
<ul>
<li>假设经过 one-hot 编码以后的样本特征维度为 $n$ ，那么上述公式中的 $w_{ij}$ 共有 $\frac{n(n-1)}{2}$ 个，权重参数量从 $O(n)$ 上升到了 $O(n^2)$ 。</li>
<li>并且 $w_{ij}$ 还有一个“训练难”的问题，因为每个参数 $w_{ij}$ 的训练需要大量 $x_i$ 和 $x_j$ 都非零的样本；由于样本数据本来就比较稀疏，满足“ $x_i$ 和 $x_j$ 都非零”的样本将会非常少。训练样本的不足，很容易导致参数 $w_{ij}$ 不准确，最终将严重影响模型的性能。</li>
</ul>
<script type="math/tex; mode=display">
y(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n} <\mathbf{w}_i,\mathbf{w}_j> x_{i} x_{j}</script><p>&emsp;&emsp;受矩阵分解算法的启发，FM 提出隐向量的概念，用两个向量的内积 $&lt;\mathbf{w}_i,\mathbf{w}_j&gt;$ 代替 $w_{ij}$ ，也就是每一维特征都对应一个隐向量，共有 $n$ 个隐向量，在做二阶特征交叉时，用两个向量的内积表示这两个组合特征的权重 $w_{ij}$，这样一来就解决了 POLY2 的两个问题：</p>
<ul>
<li>权重参数量由 $O(n^2)$ 减少到  $O(nk),k&lt;&lt;n$ 。</li>
<li>隐向量的引入使得 $ x_{h} x_{i}$ 的参数和 $ x_{i} x_{j}$ 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计 FM 的二次项参数。具体来说， $ x_{h} x_{i}$ 和 $ x_{i} x_{j}$ 的系数分别为 $&lt;\mathbf{w}_h,\mathbf{w}_i&gt;$ 和$&lt;\mathbf{w}_i,\mathbf{w}_j&gt;$，它们之间有共同项 $\mathbf{w}_i$。也就是说，所有包含“ $x_i$ 的非零组合特征”（存在某个 $j\neq i$，使得 $ x_{i} x_{j}\neq 0$ ）的样本都可以用来学习隐向量 $\mathbf{w}_i$，这很大程度上缓解了数据稀疏性的问题。相比POLY2， FM虽然丢失了某些具体特征组合的精确记忆能力， 但是泛化能力大大提高。  </li>
</ul>
<h4 id="计算优化"><a href="#计算优化" class="headerlink" title="计算优化"></a>计算优化</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619134325217.png" alt="image-20220619134325217" style="zoom: 80%;" /></p>
<p>&emsp;&emsp;补一个小知识点，FM的计算公式可以<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343174108">化简</a>[4]，将时间复杂度从 $O(n^2)$ 减少到 $O(n)$ 。</p>
<h3 id="高阶特征组合：DNN、FNN"><a href="#高阶特征组合：DNN、FNN" class="headerlink" title="高阶特征组合：DNN、FNN"></a>高阶特征组合：DNN、FNN</h3><h4 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h4><p>&emsp;&emsp;比起 FM 的二阶特征组合能力，DNN 能实现<strong>更高阶的特征组合</strong>。但是 DNN 也存在一些问题[3]：</p>
<p>&emsp;&emsp;当我们使用DNN网络解决推荐问题的时候，存在网络参数过于庞大的问题，这是因为在进行特征处理的时候我们需要使用one-hot编码来处理离散特征，这会导致输入的维度猛增。这里借用AI大会的一张图片：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101907599.png" alt="image-20220619101907599"></p>
<p>&emsp;&emsp;这样庞大的参数量也是不实际的。为了解决 DNN 参数量过大的局限性，可以采用非常经典的 Field 思想，将 OneHot 特征转换为 Dense Vector </p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101925322.png" alt="image-20220619101925322" style="zoom:50%;" /></p>
<p>&emsp;&emsp;此时通过增加全连接层就可以实现<strong>高阶的特征组合</strong>，如下图所示：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101941306.png" alt="image-20220619101941306" style="zoom:50%;" /></p>
<h4 id="FNN"><a href="#FNN" class="headerlink" title="FNN"></a>FNN</h4><p>&emsp;&emsp;DNN 能够实现高阶特征组合，但是低阶的特征组合也很重要，于是一些模型例如 FNN ，在 DNN 基础上，增加 FM 来表示低阶的特征组合，以下是 FNN 的模型图。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101451894.png" alt="image-20220619101451894" style="zoom: 60%;" />       <img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619102848286.png" alt="image-20220619102848286" style="zoom: 60%;" />   </p>
<p>&emsp;&emsp;可以看出 FNN 在Embedding 层做了改进，利用 FM 的权重进行参数初始化，这样做其实有两点好处：</p>
<ul>
<li>因为 1）Embedding 层的参数数量巨大；2）在进行梯度下降优化时，只有与非零特征相连的 Embedding 层权重会被更新。这两点原因导致 Embedding 层收敛速度很慢，利用 FM 训练好的隐向量初始化 Embedding 层的参数，相当于在初始化神经网络参数时，已经引入了有价值的先验信息。 也就是说， 神经网络训练的起点更接近目标最优点， 自然<strong>加速了整个神经网络的收敛过程</strong>。  </li>
<li>模型图的 Embedding 层， $w_0$ 是 FM 公式里的偏置项，$w_1$ 是一阶特征组合项，剩下的是二阶特征组合项，所以 FNN 也加入了低阶特征组合（虽然经过 DNN 后这些低阶特征组合几乎没有了）</li>
</ul>
<h3 id="Wide-amp-Deep"><a href="#Wide-amp-Deep" class="headerlink" title="Wide&amp;Deep"></a>Wide&amp;Deep</h3><p>&emsp;&emsp;紧接着上面，“经过 DNN 后这些低阶特征组合几乎没有了”，这是实际上是由于 FNN 中，“ FM 预训练，再用 DNN 训练最终的模型”，这样<strong>串行</strong>的模式导致的，也就是虽然 FM 学到了低阶特征组合，但是 DNN 的全连接结构导致低阶特征在 DNN 中又被组合成了高阶特征组合，所以没有保留下低阶特征组合。看来我们已经找到问题了，将<strong>串行</strong>方式改进为<strong>并行</strong>方式能比较好的解决这个问题。于是Google提出了 Wide&amp;Deep 模型，见下图。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619104157758.png" alt="image-20220619104157758"></p>
<p>&emsp;&emsp;Wide&amp;Deep 的设计初衷是为了赋予模型<strong>记忆能力</strong>和<strong>泛化能力</strong>，记忆能力通过 Wide 部分实现，泛化能力通过 Deep 部分实现。</p>
<blockquote>
<p>“记忆能力” 可以被理解为模型直接学习并利用历史数据中物品或者特征的“共现频率”的能力。 一般来说， 协同过滤、 逻辑回归等简单模型有较强的“记忆能力”。 由于这类模型的结构简单， 原始数据往往可以直接影响推荐结果， 产生类似于“如果点击过A， 就推荐B”这类规则式的推荐， 这就相当于模型直接记住了历史数据的分布特点， 并利用这些记忆进行推荐。</p>
<p>“泛化能力” 可以被理解为模型传递特征的相关性， 以及发掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力。 矩阵分解比协同过滤的泛化能力强， 因为矩阵分解引入了隐向量这样的结构， 使得数据稀少的用户或者物品也能生成隐向量， 从而获得有数据支撑的推荐得分， 这就是非常典型的将全局数据传递到稀疏物品上， 从而提高泛化能力的例子。 再比如， 深度神经网络通过特征的多次自动组合， 可以深度发掘数据中潜在的模式， 即使是非常稀疏的特征向量输入， 也能得到较稳定平滑的推荐概率， 这就是简单模型所缺乏的“泛化能力”。</p>
</blockquote>
<p>&emsp;&emsp;但是如果深入探究 Wide&amp;Deep 的构成方式，虽然将整个模型的结构调整为了并行结构，在实际的使用中 Wide 部分需要较为精巧的特征工程，换句话说人工处理对于模型的效果具有比较大的影响，大家可以看到下图红圈内的 Wide 部分采用了两个 id 类特征的乘积，这是 Google 团队<strong>根据业务精心选择</strong>的想让模型<strong>直接记忆</strong>的特征组合。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619104313037.png" alt="image-20220619104313037"></p>
<p>&emsp;&emsp;Wide&amp;Deep 其实还有一个没那么容易能够发现的问题，我们看下面这张图（图片来自FunRec）：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619123835759.png" alt="image-20220619123835759"></p>
<p>&emsp;&emsp;在模型前向计算的时候，Wide 和 Deep 部分的输入不同，Wide 的输入只有低阶特征组合，Deep 则可以输入低阶和高阶；两部分各自输出一个标量 logits，最后学习它们的权重系数加权求和，再过 sigmoid 激活。DeepFM 的论文里，指出这样做可能使得模型最终<strong>偏向学习到低阶或者高阶的特征</strong>，不能做到很好的结合，究其原因还是高阶和低阶特征的输入是分开的。</p>
<h2 id="回到DeepFM"><a href="#回到DeepFM" class="headerlink" title="回到DeepFM"></a>回到DeepFM</h2><p>&emsp;&emsp;综合上述几个模型，FM 能够高效进行特征交叉捕捉<strong>低阶</strong>特征组合；DNN、FNN、PNN 能够捕捉<strong>高阶特征组合</strong>；Wide&amp;Deep 结合两者，同时捕捉低阶和高阶特征，但仍有两个问题：1）学习有偏，最后会偏向学习低阶或者高阶特征。2）Wide 部分需要手工设计特征，费时费力。</p>
<p>&emsp;&emsp;至此，终于轮到 DeepFM 登场。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619132058752.png" alt="image-20220619132058752"></p>
<p>&emsp;&emsp;DeepFM 仍然沿用了 Wide&amp;Deep “记忆+泛化”两部分建模的思想，设计 FM Component 负责“记忆”，Deep Component 负责“泛化”。其中巧妙地利用 FM 的思想解决了 Wide&amp;Deep 的两个问题：<strong>特征工程困难</strong>和<strong>学习有偏</strong>。</p>
<p>&emsp;&emsp;具体是如何用 FM 思想解决这两部分问题的呢？下面看 FM 部分的模型图：</p>
<h3 id="FM-Component"><a href="#FM-Component" class="headerlink" title="FM Component"></a>FM Component</h3><p>&emsp;&emsp;FM 部分，改进 Wide&amp;Deep  的 Wide，使得不再需要手动构造（二阶）交叉特征，也能捕捉低阶特征组合。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619133002341.png" alt="image-20220619133002341" style="zoom:67%;" /></p>
<p>&emsp;公式：</p>
<script type="math/tex; mode=display">
y_{F M}=\langle w, x\rangle+\sum_{i=1}^{d} \sum_{j=i+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{i} \cdot x_{j}</script><blockquote>
<p>这里每个 field 都是 one-hot ，原文里说的，所以不用纠结同一个 field 里是否是 multi-hot 了，如果是 multi-hot 也可以用各种 pooling 方式转化成一个向量。</p>
</blockquote>
<p>&emsp;&emsp;在 FM Layer，共有两种操作，Addition 和 Inner Product（分别用绿色和蓝色箭头标出）。</p>
<ul>
<li><strong>Addition</strong>. 对 Sparse Feature（Field level）线性加和，即 $\langle w, x\rangle$ .</li>
<li><strong>Inner Product</strong>. 将每个 field 的 one-hot 向量转化成 dense embedding，把它看作 FM 的 latent vector（Dense Embeddings 层在 Deep Component 里介绍），然后做点积操作即  $\sum_{i=1}^{d} \sum_{j=i+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{i} \cdot x_{j}$ . </li>
</ul>
<p>&emsp;&emsp;这里我曾经纠结了很久，因为不知道特征究竟是如何组合的，是 $\frac{n(n-1)}{2}$ 次组合，还是 $\frac{m(m-1)}{2}$ 次组合。最后查阅资料加上自己整理，回答是这样的：</p>
<ul>
<li>$\sum_{i=1}^{d} \sum_{j=i+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{i} \cdot x_{j}$  反映二阶特征交叉，$V_i$ , $V_j$ 是特征对应的 latent vector，也就是对应的embedding。<br>所以这里其实不是所有 feature（所有field里的所有feature，num=$n$）的两两交叉，其实是 $m$ 个域的特征交叉，不过每个域是可以涵盖到域里所有的 feature 的（每个域从域包含的特征里选择一个，因为是 one-hot）。也就是说，当一条数据输入进去的时候，不会对所有 feature 做特征交叉（即 $\frac{n(n-1)}{2}$ 次组合），而是会对所有域做特征交叉（即 $\frac{m(m-1)}{2}$ 次组合），但是当数据量足够多时，就能涵盖到所有feature的交叉。</li>
</ul>
<h3 id="Deep-Component"><a href="#Deep-Component" class="headerlink" title="Deep Component"></a>Deep Component</h3><p>&emsp;&emsp;Deep 部分和 DNN 一样，捕捉高阶特征组合。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619135133347.png" alt="image-20220619135133347"></p>
<p>&emsp;&emsp;用全连接的方式将 Dense Embedding 输入到 Hidden Layer ，这里面 Dense Embeddings 就是用 Field 思想解决 DNN 中的参数爆炸问题，这也是推荐模型中常用的处理方法。然后 Dense Embeddings 拼接以后传入 DNN 。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619135849196.png" alt="image-20220619135849196"></p>
<p>&emsp;&emsp;上图是 Dense Embeddings 层的结构，这里有两点需要指出：1）尽管不同 field 的长度可能不同，但 embedding 维度 $k$ 都是相同的；2）FM 里的 latent vector 现在充当作为网络的权重参数（回忆一下矩阵乘法，有 1 的地方对应的一列权重参数就是 latent vector），它们是学习得到的，被用来将 field 的 one-hot 向量压缩成 embedding 向量。</p>
<p>&emsp;与 FNN 不同的是，这里的 latent vector 不是预训练而是随机初始化得到的，并且是不断学习优化的。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619143513920.png" alt="image-20220619143513920" style="zoom:67%;" /></p>
<p>&emsp;&emsp;至此我们还可以发现，FM 部分和 Deep 部分是共享 embedding 的，这就解决了“<strong>学习有偏</strong>”的问题，因为高阶和低阶特征都是从同一个 embedding 层获得的。再回想 FM 部分，用 latent vector 做内积组合二阶特征的方式避免了“<strong>特征工程困难</strong>”的问题。DeepFM 的主要贡献就是在于对 Wide&amp;Deep 进行了这两方面的改进。</p>
<h1 id="DeepFM-代码"><a href="#DeepFM-代码" class="headerlink" title="DeepFM 代码"></a>DeepFM 代码</h1><p>&emsp;&emsp;开源代码见：<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/torch-rechub/blob/main/tutorials/DeepFM.ipynb">torch-rechub/DeepFM.ipynb at main · datawhalechina/torch-rechub (github.com)</a></p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>&emsp;&emsp;使用的是 Criteo 的一个 sample</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152139173.png" alt="image-20220619152139173"></p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152209082.png" alt="image-20220619152209082"></p>
<p>&emsp;&emsp;至此都比较好理解。</p>
<h3 id="Dense-特征"><a href="#Dense-特征" class="headerlink" title="Dense 特征"></a>Dense 特征</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152251762.png" alt="image-20220619152251762"></p>
<p>&emsp;&emsp;这里 <code>convert_numeric_feature()</code> 有点令人费解，据说是比赛中冠军队伍使用的方法，emm，EDA做得好，同时不得不佩服大佬们的创造力~ 总之经过这样的变换，将 dense 特征都转化成了新的 sparse 特征列。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152310615.png" alt="image-20220619152310615"></p>
<p>&emsp;&emsp;将 dense 特征转化成新的 sparse 特征后，dense 特征本身还要做一些归一化操作，这里使用 <code>MinMaxScaler()</code> 。</p>
<p>&emsp;&emsp;其实 Wide&amp;Deep 里做完归一化以后还做了分组，见下图：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619150451559.png" alt="image-20220619150451559" style="zoom:50%;" /></p>
<h3 id="Sparse-特征"><a href="#Sparse-特征" class="headerlink" title="Sparse 特征"></a>Sparse 特征</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152328870.png" alt="image-20220619152328870"></p>
<p>&emsp;&emsp;直接用 <code>LabelEncoder()</code> 编码。</p>
<h2 id="定义-DataGenerator-Dataset-Dataloader"><a href="#定义-DataGenerator-Dataset-Dataloader" class="headerlink" title="定义 DataGenerator (Dataset + Dataloader)"></a>定义 DataGenerator (Dataset + Dataloader)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重点：将每个特征定义为torch-rechub所支持的特征基类，dense特征只需指定特征名，sparse特征需指定特征名、特征取值个数(vocab_size)、embedding维度(embed_dim)</span></span><br><span class="line">dense_features = [DenseFeature(feature_name) <span class="keyword">for</span> feature_name <span class="keyword">in</span> dense_cols]</span><br><span class="line">sparse_features = [SparseFeature(feature_name, vocab_size=data[feature_name].nunique(), embed_dim=<span class="number">16</span>) <span class="keyword">for</span> feature_name <span class="keyword">in</span> sparse_cols]</span><br><span class="line">y = data[<span class="string">&quot;label&quot;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">x = data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型输入所需要的dataloader，区分验证集、测试集，指定batch大小</span></span><br><span class="line"><span class="comment">#split_ratio=[0.7,0.1] 指的是训练集占比70%，验证集占比10%，剩下的全部为测试集</span></span><br><span class="line">dg = DataGenerator(x, y) </span><br><span class="line">train_dataloader, val_dataloader, test_dataloader = dg.generate_dataloader(split_ratio=[<span class="number">0.7</span>, <span class="number">0.1</span>], batch_size=<span class="number">256</span>, num_workers=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<h2 id="定义-Model"><a href="#定义-Model" class="headerlink" title="定义 Model"></a>定义 Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.models.ranking <span class="keyword">import</span> DeepFM</span><br><span class="line"><span class="keyword">from</span> torch_rechub.trainers <span class="keyword">import</span> CTRTrainer</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line">model = DeepFM(</span><br><span class="line">        deep_features=dense_features+sparse_features,</span><br><span class="line">        fm_features=sparse_features,</span><br><span class="line">        mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>], <span class="string">&quot;dropout&quot;</span>: <span class="number">0.2</span>, <span class="string">&quot;activation&quot;</span>: <span class="string">&quot;relu&quot;</span>&#125;,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;我们进入 DeepFM 模型内部看一看：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619153802390.png" alt="image-20220619153802390"></p>
<p>&emsp;&emsp;从 forward() 里可以看出，在代码实现时，其实不完全是分成 deep 和 fm 两部分，而是分成 deep 、fm、linear 三部分。论文里的 fm 部分是包含一阶特征和二阶特征交叉的，代码实现的时候把一阶特征单独拿出来用 linear 实现。</p>
<p>&emsp;&emsp;LR 的实现如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619154134742.png" alt="image-20220619154134742"></p>
<p>&emsp;&emsp;FM 的实现如下（不是完全体的 FM ，这里是只计算二阶特征的 FM ）：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619154214878.png" alt="image-20220619154214878"></p>
<p>&emsp;&emsp;这里 FM 的公式是计算优化后的，可以参考下面的公式：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619134325217.png" alt="image-20220619134325217" style="zoom: 80%;" /></p>
<p>&emsp;&emsp;MLP 的实现如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619154603820.png" alt="image-20220619154603820"></p>
<p>&emsp;&emsp;我们在传参的时候：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>], <span class="string">&quot;dropout&quot;</span>: <span class="number">0.2</span>, <span class="string">&quot;activation&quot;</span>: <span class="string">&quot;relu&quot;</span>&#125;,</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里的 dims 的参数列表就表示从输入到输出，维度依次是多少，这个例子中就是： input_dim —&gt; 256 —&gt; 128 —&gt; 1 。没有指定 “output_layer” 的话，会默认再过一个 nn.linear() 让维度变成 1 ，当然 output_layer 后是不接激活函数的。 </p>
<p>另外注意这里添加了 BatchNorm 。</p>
<h2 id="定义-trainer"><a href="#定义-trainer" class="headerlink" title="定义 trainer"></a>定义 trainer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练，需要学习率、设备等一般的参数，此外我们还支持earlystoping策略，及时发现过拟合</span></span><br><span class="line">ctr_trainer = CTRTrainer(</span><br><span class="line">    model,</span><br><span class="line">    optimizer_params=&#123;<span class="string">&quot;lr&quot;</span>: <span class="number">1e-4</span>, <span class="string">&quot;weight_decay&quot;</span>: <span class="number">1e-5</span>&#125;,</span><br><span class="line">    n_epoch=<span class="number">1</span>,</span><br><span class="line">    earlystop_patience=<span class="number">3</span>,</span><br><span class="line">    device=<span class="string">&#x27;cpu&#x27;</span>, <span class="comment">#如果有gpu，可设置成cuda:0</span></span><br><span class="line">    model_path=<span class="string">&#x27;./&#x27;</span>, <span class="comment">#模型存储路径</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;不需要定义损失函数，因为 ctr 预估任务都是 BCE loss，默认评价指标是 auc 。</p>
<h2 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ctr_trainer.fit(train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看在测试集上的性能</span></span><br><span class="line">auc = ctr_trainer.evaluate(ctr_trainer.model, test_dataloader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;test auc: <span class="subst">&#123;auc&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619155836040.png" alt="image-20220619155836040"></p>
<h2 id="使用其它排序模型"><a href="#使用其它排序模型" class="headerlink" title="使用其它排序模型"></a>使用其它排序模型</h2><h3 id="调用现成模型"><a href="#调用现成模型" class="headerlink" title="调用现成模型"></a>调用现成模型</h3><p>&emsp;&emsp;调用现成的模型非常容易，只需要修改 model 参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义相应的模型，用同样的方式训练</span></span><br><span class="line">model = WideDeep(wide_features=dense_features, deep_features=sparse_features, mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>], <span class="string">&quot;dropout&quot;</span>: <span class="number">0.2</span>, <span class="string">&quot;activation&quot;</span>: <span class="string">&quot;relu&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">model = DCN(features=dense_features + sparse_features, n_cross_layers=<span class="number">3</span>, mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619163102998.png" alt="image-20220619163102998"></p>
<h1 id="DIN"><a href="#DIN" class="headerlink" title="DIN"></a>DIN</h1><p>&emsp;&emsp;DIN 还算是比较独立的存在，它在前面模型 DNN 思想的基础上加入了注意力机制。</p>
<h2 id="Base-Model"><a href="#Base-Model" class="headerlink" title="Base Model"></a>Base Model</h2><p>&emsp;&emsp;DIN 也是广告推荐场景，一般来说，模型的输入特征自然分为三部分：一部分是用户 $u$ 的特征（下图的 User Profile 和 User Behaviors），一部分是候选广告 $a$ 的特征（Candidate Ad），一部分是上下文特征（Context Features）。我们把用户的 User Behaviors 和广告的 Candidate Ad 两类特征组单独拿出来看，为什么要单独挑出来？因为它们都含有两个非常重要的特征——商品 id 和商铺 id。用户特征里的商品 id 是一个序列，代表用户曾经点击过的商品集合，商铺 id 统里；而广告特征里的商品 id  和商铺 id 就是广告对应的商品 id 和商铺 id 。下图是论文中用到的一个 base model，也是 DIN 之前绝大多数模型的做法，即给模型输入 one-hot 或 multi-hot 向量，再经过 Embedding 层转化成 $1 \times d$  的 dense embedding 向量，multi-hot 向量还得在特征组（Field）内进行 pooling 操作转化才能转化成 $1 \times d$ 的向量，论文选用最常用的 sum pooling。在得到每个特征组的向量后，对所有向量进行 concat 然后送入 DNN ，这就是最一般的做法。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619222927973.png" alt="image-20220619222927973"></p>
<p>&emsp;&emsp;这些模型在这种个性化广告点击预测任务中存在的问题就是<strong>无法表达用户广泛的兴趣</strong>，因为这些模型在得到各个特征的embedding之后，就蛮力拼接了，然后就各种交叉等。这时候根本没有考虑之前用户历史行为商品具体是什么，究竟用户历史行为中的哪个会对当前的点击预测带来积极的作用。 而实际上，对于用户点不点击当前的商品广告，很大程度上是依赖于他的历史行为的，王喆老师[1]举了个例子</p>
<blockquote>
<p>假设广告中的商品是键盘， 如果用户历史点击的商品中有化妆品， 包包，衣服， 洗面奶等商品， 那么大概率上该用户可能是对键盘不感兴趣的， 而如果用户历史行为中的商品有鼠标， 电脑，iPad，手机等， 那么大概率该用户对键盘是感兴趣的， 而如果用户历史商品中有鼠标， 化妆品， T-shirt和洗面奶， 鼠标这个商品embedding对预测“键盘”广告的点击率的重要程度应该大于后面的那三个。</p>
</blockquote>
<p>&emsp;&emsp;这里也就是说如果是之前的那些深度学习模型，是没法很好的去表达出用户这广泛多样的兴趣的，如果想表达的准确些， 那么就得加大隐向量的维度，让每个特征的信息更加丰富， 那这样带来的问题就是计算量上去了，毕竟真实情景尤其是电商广告推荐的场景，特征维度的规模是非常大的。 并且根据上面的例子， 也<strong>并不是用户所有的历史行为特征都会对某个商品广告点击预测起到作用</strong>。所以对于当前某个商品广告的点击预测任务，没必要考虑之前所有的用户历史行为。</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>&emsp;&emsp;这样， DIN的动机就出来了，在业务的角度，我们应该自适应的去捕捉用户的兴趣变化，这样才能较为准确的实施广告推荐；而放到模型的角度， 我们应该<strong>考虑到用户的历史行为商品与当前商品广告的一个关联性</strong>，如果用户历史商品中很多与当前商品关联，那么说明该商品可能符合用户的品味，就把该广告推荐给他。而一谈到关联性的话， 我们就容易想到“注意力”的思想了， 所以为了更好的从用户的历史行为中学习到与当前商品广告的关联性，学习到用户的兴趣变化， 作者把<strong>注意力</strong>引入到了模型，设计了一个”local activation unit”结构，利用候选商品和历史问题商品之间的相关性计算出权重，这个就代表了对于当前商品广告的预测，用户历史行为的各个商品的重要程度大小， 而加入了注意力权重的深度学习网络，就是这次的主角DIN， 下面具体来看下该模型。</p>
<p>&emsp;&emsp;论文原版图：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619224333801.png" alt="image-20220619224333801"></p>
<p>&emsp;&emsp;王喆老师的配图：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619224732551.png" alt="image-20220619224732551"></p>
<h2 id="注意力激活单元"><a href="#注意力激活单元" class="headerlink" title="注意力激活单元"></a>注意力激活单元</h2><p>&emsp;&emsp;注意力机制的公式可以定义如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{V}_{\mathrm{u}}=f\left(\boldsymbol{V}_{\mathrm{a}}\right)=\sum_{i=1}^{N} w_{i} \cdot \boldsymbol{V}_{i}=\sum_{i=1}^{N} g\left(\boldsymbol{V}_{i}, \boldsymbol{V}_{\mathrm{a}}\right) \cdot \boldsymbol{V}_{i}</script><p>&emsp;&emsp;其中， $\boldsymbol{V}_{\mathrm{u}}$ 是用户的Embedding向量， $\boldsymbol{V}_{\mathrm{a}}$ 是候选广告商品的 Embedding 向量， $\boldsymbol{V}_{\mathrm{i}}$ 是用户 $u$ 的第 $i$ 次行为的 Embedding 向量。 这里用户的行为就是浏览商品或店铺， 因此行为的 Embedding 向量就是那次浏览的商品或店铺的 Embedding 向量。  </p>
<p>&emsp;&emsp;因为加入了注意力机制， 所以 $\boldsymbol{V}_{\mathrm{u}}$ 从过去 $\boldsymbol{V}_{\mathrm{i}}$ 的加和变成了 $\boldsymbol{V}_{\mathrm{i}}$ 的加权和，  $\boldsymbol{V}_{\mathrm{i}}$ 的权重 $w_i$ 就由 $\boldsymbol{V}_{\mathrm{i}}$ 与 $\boldsymbol{V}_{\mathrm{a}}$ 的关系决定， 也就是公式中的$ g\left(\boldsymbol{V}_{i}, \boldsymbol{V}_{\mathrm{a}}\right)$， 即“注意力得分”。  </p>
<p>&emsp;&emsp;那么到底应该如何计算注意力得分呢，论文设计了“local activation unit”，即注意力激活单元。这个注意力激活单元本质上也是小的神经网络，看王喆老师的配图比较清晰，在图的右上角。</p>
<p>&emsp;&emsp;可以看出， 激活单元的输入层是两个 Embedding 向量， 经过元素减（element-wise minus） 操作后， 与原Embedding向量一同连接后形成全连接层的输入， 最后通过单神经元输出层生成注意力得分。  </p>
<p>&emsp;&emsp;注意商铺 id 只跟用户历史行为中的商铺 id 序列发生作用， 商品 id 只跟用户的商品 id 序列发生作用， 因为注意力的轻重更应该由同类信息的相关性决定。  </p>
<h2 id="有意思的发现"><a href="#有意思的发现" class="headerlink" title="有意思的发现"></a>有意思的发现</h2><p>&emsp;&emsp;论文作者发现用 LSTM 建模用户的历史行为，效果没有提升。作者提出，可能原因是会引入噪声。挺有意思哈哈哈哈，因为后面几年<strong>序列推荐</strong>火起来了，专门研究用户历史行为！这里作者说后续研究是不是 DIEN，还没看过论文，还不知道。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619230308830.png" alt="image-20220619230308830"></p>
<h1 id="DIN代码"><a href="#DIN代码" class="headerlink" title="DIN代码"></a>DIN代码</h1><p>&emsp;&emsp;这里我们以Amazon-Electronics为例，原数据是json格式，我们提取所需要的信息预处理为一个仅包含user_id, item_id, cate_id, time四个特征列的CSV文件。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>&emsp;&emsp;完整的数据长这样，共有 1,689,188 条用户交互记录。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620012113828.png" alt="image-20220620012113828" style="zoom:67%;" /></p>
<p>&emsp;&emsp;在该数据集上，我们没有 dense feature，只有 sparse feature，除此之外我们还要构造 sequence feature。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620012500712.png" alt="image-20220620012500712"></p>
<p>&emsp;&emsp;这里用一个关键函数 <code>create_seq_features()</code> 构造序列特征，我们进到函数内看一看，解析见注释。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_seq_features</span>(<span class="params">data, seq_feature_col=[<span class="string">&#x27;item_id&#x27;</span>, <span class="string">&#x27;cate_id&#x27;</span>], max_len=<span class="number">50</span>, drop_short=<span class="number">3</span>, shuffle=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Build a sequence of user&#x27;s history by time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data (pd.DataFrame): must contain keys: `user_id, item_id, cate_id, time`.</span></span><br><span class="line"><span class="string">        seq_feature_col (list): specify the column name that needs to generate sequence features, and its sequence features will be generated according to userid.</span></span><br><span class="line"><span class="string">        max_len (int): the max length of a user history sequence.</span></span><br><span class="line"><span class="string">        drop_short (int): remove some inactive user who&#x27;s sequence length &lt; drop_short.</span></span><br><span class="line"><span class="string">        shuffle (bool): shuffle data if true.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        train (pd.DataFrame): target item will be each item before last two items.</span></span><br><span class="line"><span class="string">        val (pd.DataFrame): target item is the second to last item of user&#x27;s history sequence.</span></span><br><span class="line"><span class="string">        test (pd.DataFrame): target item is the last item of user&#x27;s history sequence.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> data:   <span class="comment"># 对 data 的每一列都进行 labelEncode</span></span><br><span class="line">        le = LabelEncoder()</span><br><span class="line">        data[feat] = le.fit_transform(data[feat])</span><br><span class="line">        data[feat] = data[feat].apply(<span class="keyword">lambda</span> x: x + <span class="number">1</span>)  <span class="comment"># LabelEncoder() 是从 0 开始编号的，这里留出 0 用来后续 padding</span></span><br><span class="line">    data = data.astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    n_items = data[<span class="string">&quot;item_id&quot;</span>].<span class="built_in">max</span>() <span class="comment"># labelEncode 的id都+1了，所以 物品数=最大物品id</span></span><br><span class="line"></span><br><span class="line">    item_cate_map = data[[<span class="string">&#x27;item_id&#x27;</span>, <span class="string">&#x27;cate_id&#x27;</span>]]</span><br><span class="line">    item2cate_dict = item_cate_map.set_index([<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;cate_id&#x27;</span>].to_dict()  <span class="comment"># 生成 &#123;item:category&#125; 字典</span></span><br><span class="line"></span><br><span class="line">    data = data.sort_values([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;time&#x27;</span>]).groupby(<span class="string">&#x27;user_id&#x27;</span>).agg(click_hist_list=(<span class="string">&#x27;item_id&#x27;</span>, <span class="built_in">list</span>), cate_hist_hist=(<span class="string">&#x27;cate_id&#x27;</span>, <span class="built_in">list</span>)).reset_index()</span><br><span class="line">    <span class="comment"># 按user_id,time 这样的优先级排序整个 data ，在以user_id来分组，聚合时生成“用户点击商品序列”和“点击商品类别序列”</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Sliding window to construct negative samples</span></span><br><span class="line">    train_data, val_data, test_data = [], [], []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data.itertuples():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="number">2</span>]) &lt; drop_short:   <span class="comment"># 序列长度小于阈值直接丢弃</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        user_id = item[<span class="number">1</span>]</span><br><span class="line">        click_hist_list = item[<span class="number">2</span>][:max_len] <span class="comment"># 上面排序是从小到大排的,阶段前段的maxlen是否不合理？</span></span><br><span class="line">        cate_hist_list = item[<span class="number">3</span>][:max_len]</span><br><span class="line"></span><br><span class="line">        neg_list = [neg_sample(click_hist_list, n_items) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(click_hist_list))]  <span class="comment"># 这里的负采样是全局负采样，没有剔除ground truth标签</span></span><br><span class="line">        hist_list = []</span><br><span class="line">        cate_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(click_hist_list)):    <span class="comment"># 前 n-2 条交互记录做训练集, 第 n-1 条作为验证集标签, 第 n 条作为测试集标签</span></span><br><span class="line">            hist_list.append(click_hist_list[i - <span class="number">1</span>])</span><br><span class="line">            cate_list.append(cate_hist_list[i - <span class="number">1</span>])</span><br><span class="line">            hist_list_pad = hist_list + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(hist_list))</span><br><span class="line">            cate_list_pad = cate_list + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(cate_list))</span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(click_hist_list) - <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># 用最后一位标记正负样本，1是正样本，0是负样本</span></span><br><span class="line">                test_data.append([user_id, hist_list_pad, cate_list_pad, click_hist_list[i], cate_hist_list[i], <span class="number">1</span>])</span><br><span class="line">                test_data.append([user_id, hist_list_pad, cate_list_pad, neg_list[i], item2cate_dict[neg_list[i]], <span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(click_hist_list) - <span class="number">2</span>:</span><br><span class="line">                val_data.append([user_id, hist_list_pad, cate_list_pad, click_hist_list[i], cate_hist_list[i], <span class="number">1</span>])</span><br><span class="line">                val_data.append([user_id, hist_list_pad, cate_list_pad, neg_list[i], item2cate_dict[neg_list[i]], <span class="number">0</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                train_data.append([user_id, hist_list_pad, cate_list_pad, click_hist_list[i], cate_hist_list[i], <span class="number">1</span>])</span><br><span class="line">                train_data.append([user_id, hist_list_pad, cate_list_pad, neg_list[i], item2cate_dict[neg_list[i]], <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># shuffle</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.shuffle(train_data)</span><br><span class="line">        random.shuffle(val_data)</span><br><span class="line">        random.shuffle(test_data)</span><br><span class="line"></span><br><span class="line">    col_name = [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;history_item&#x27;</span>, <span class="string">&#x27;history_cate&#x27;</span>, <span class="string">&#x27;target_item&#x27;</span>, <span class="string">&#x27;target_cate&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    train = pd.DataFrame(train_data, columns=col_name)</span><br><span class="line">    val = pd.DataFrame(val_data, columns=col_name)</span><br><span class="line">    test = pd.DataFrame(test_data, columns=col_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train, val, test</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;构建完序列特征后：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620014907354.png" alt="image-20220620014907354"></p>
<p><strong>让模型明白如何处理每一类特征</strong></p>
<p>&emsp;&emsp;在DIN模型中，我们讲使用了两种类别的特征，分别是类别特征和序列特征。对于类别特征，我们希望模型将其输入Embedding层，而对于序列特征，我们不仅希望模型将其输入Embedding层，还需要计算target-attention分数，所以需要指定DataFrame中每一列的含义，让模型能够正确处理。</p>
<p>&emsp;&emsp;在这个案例中，因为我们使用user_id,item_id和item_cate这三个类别特征，使用用户的item_id和cate的历史序列作为序列特征。在torch-rechub我们只需要调用DenseFeature, SparseFeature, SequenceFeature这三个类，就能自动正确处理每一类特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.basic.features <span class="keyword">import</span> DenseFeature, SparseFeature, SequenceFeature</span><br><span class="line"></span><br><span class="line">n_users, n_items, n_cates = data[<span class="string">&quot;user_id&quot;</span>].<span class="built_in">max</span>(), data[<span class="string">&quot;item_id&quot;</span>].<span class="built_in">max</span>(), data[<span class="string">&quot;cate_id&quot;</span>].<span class="built_in">max</span>()</span><br><span class="line"><span class="comment"># 这里指定每一列特征的处理方式，对于sparsefeature，需要输入embedding层，所以需要指定特征空间大小和输出的维度</span></span><br><span class="line">features = [SparseFeature(<span class="string">&quot;target_item&quot;</span>, vocab_size=n_items + <span class="number">2</span>, embed_dim=<span class="number">8</span>),	<span class="comment"># +2 ？？</span></span><br><span class="line">            SparseFeature(<span class="string">&quot;target_cate&quot;</span>, vocab_size=n_cates + <span class="number">2</span>, embed_dim=<span class="number">8</span>),</span><br><span class="line">            SparseFeature(<span class="string">&quot;user_id&quot;</span>, vocab_size=n_users + <span class="number">2</span>, embed_dim=<span class="number">8</span>)]</span><br><span class="line">target_features = features</span><br><span class="line"><span class="comment"># 对于序列特征，除了需要和类别特征一样处理意外，item序列和候选item应该属于同一个空间，我们希望模型共享它们的embedding，所以可以通过shared_with参数指定</span></span><br><span class="line">history_features = [</span><br><span class="line">    SequenceFeature(<span class="string">&quot;history_item&quot;</span>, vocab_size=n_items + <span class="number">2</span>, embed_dim=<span class="number">8</span>, pooling=<span class="string">&quot;concat&quot;</span>, shared_with=<span class="string">&quot;target_item&quot;</span>),</span><br><span class="line">    SequenceFeature(<span class="string">&quot;history_cate&quot;</span>, vocab_size=n_cates + <span class="number">2</span>, embed_dim=<span class="number">8</span>, pooling=<span class="string">&quot;concat&quot;</span>, shared_with=<span class="string">&quot;target_cate&quot;</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上面 + 2 的原因是：这里建立 embedding 蹭，需要按照特征表大小建立查表，用 max 获得被 labelEncoder 后的最大值， +的第一个 1 是因为把 0 作为 mask 了，+的第二个 1 是空出来以为冗余，这是编程呢个的习惯，也可以不加。</p>
<h2 id="定义数据集"><a href="#定义数据集" class="headerlink" title="定义数据集"></a>定义数据集</h2><p>&emsp;&emsp;在上述步骤中，我们制定了每一列的数据如何处理、数据维度、embed后的维度，目的就是在构建模型中，让模型知道每一层的参数。接下来我们生成训练数据，用于训练，一般情况下，我们只需要定义一个字典装入每一列特征即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.utils.data <span class="keyword">import</span> df_to_dict, DataGenerator</span><br><span class="line"><span class="comment"># 指定label，生成模型的输入，这一步是转换为字典结构</span></span><br><span class="line">train = df_to_dict(train)</span><br><span class="line">val = df_to_dict(val)</span><br><span class="line">test = df_to_dict(test)</span><br><span class="line"></span><br><span class="line">train_y, val_y, test_y = train[<span class="string">&quot;label&quot;</span>], val[<span class="string">&quot;label&quot;</span>], test[<span class="string">&quot;label&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> train[<span class="string">&quot;label&quot;</span>]</span><br><span class="line"><span class="keyword">del</span> val[<span class="string">&quot;label&quot;</span>]</span><br><span class="line"><span class="keyword">del</span> test[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">train_x, val_x, test_x = train, val, test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后查看一次输入模型的数据格式</span></span><br><span class="line">train_x</span><br></pre></td></tr></table></figure>
<h2 id="定义-DataGenerator-Dataset-Dataloader-1"><a href="#定义-DataGenerator-Dataset-Dataloader-1" class="headerlink" title="定义 DataGenerator (Dataset + Dataloader)"></a>定义 DataGenerator (Dataset + Dataloader)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建dataloader，指定模型读取数据的方式，和区分验证集测试集、指定batch大小</span></span><br><span class="line">dg = DataGenerator(train_x, train_y)</span><br><span class="line">train_dataloader, val_dataloader, test_dataloader = dg.generate_dataloader(x_val=val_x, y_val=val_y, x_test=test_x, y_test=test_y, batch_size=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.models.ranking <span class="keyword">import</span> DIN</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型，模型的参数需要我们之前的feature类，用于构建模型的输入层，mlp指定模型后续DNN的结构，attention_mlp指定attention层的结构</span></span><br><span class="line">model = DIN(features=features, history_features=history_features, target_features=target_features, mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>]&#125;, attention_mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;我们进入 DIN 模型看一看~</p>
<h2 id="定义训练器"><a href="#定义训练器" class="headerlink" title="定义训练器"></a>定义训练器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.trainers <span class="keyword">import</span> CTRTrainer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练，需要学习率、设备等一般的参数，此外我们还支持earlystoping策略，及时发现过拟合</span></span><br><span class="line">ctr_trainer = CTRTrainer(model, optimizer_params=&#123;<span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>, <span class="string">&quot;weight_decay&quot;</span>: <span class="number">1e-3</span>&#125;, n_epoch=<span class="number">3</span>, earlystop_patience=<span class="number">4</span>, device=<span class="string">&#x27;cpu&#x27;</span>, model_path=<span class="string">&#x27;./&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="训练和评估-1"><a href="#训练和评估-1" class="headerlink" title="训练和评估"></a>训练和评估</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">ctr_trainer.fit(train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看在测试集上的性能</span></span><br><span class="line">auc = ctr_trainer.evaluate(ctr_trainer.model, test_dataloader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;test auc: <span class="subst">&#123;auc&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;训练比较慢，只跑了一个 epoch：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620022342747.png" alt="image-20220620022342747"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&emsp;&emsp;以 DeepFM 为切入点，学习了 FM、POLY2、FNN、PNN、Wide&amp;Deep 以及 DeepFM 模型，对这几个模型的发展脉络有了比较清晰的认识。用 Torch-RecHub 实现 DeepFM 也非常容易。DIN 是本次学习中学习的第一个序列模型，只做了 target-attention，放在现在来看 DIN 还处于序列建模比较萌芽的阶段。</p>
<p>&emsp;&emsp;Task2，以两个精排模型为线索，学到了很多！期待下一节的召回模型。</p>
<p>&emsp;&emsp;虽然忙面试，也要坚持打卡！</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1]《深度学习推荐系统》王喆</p>
<p>[2] <a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf">FM (cmu.edu)</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://datawhalechina.github.io/fun-rec/#/ch02/ch2.2/ch2.2.3/DeepFM">https://datawhalechina.github.io/fun-rec/#/ch02/ch2.2/ch2.2.3/DeepFM</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343174108">https://zhuanlan.zhihu.com/p/343174108</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/" rel="tag"># Datawhale组队学习</a>
              <a href="/tags/RecHub/" rel="tag"># RecHub</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/06/14/RecHub%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A01/" rel="prev" title="RecHub推荐项目学习1：Torch-RecHub框架">
      <i class="fa fa-chevron-left"></i> RecHub推荐项目学习1：Torch-RecHub框架
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81NDcyNC8zMTE5NQ=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Task02"><span class="nav-number">1.</span> <span class="nav-text">Task02</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E7%9A%84%E6%97%B6%E9%97%B4%E7%BA%BF"><span class="nav-number">2.</span> <span class="nav-text">推荐模型发展的时间线</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DeepFM"><span class="nav-number">3.</span> <span class="nav-text">DeepFM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepFM-%E7%9A%84%E5%89%8D%E8%BE%88%E4%BB%AC"><span class="nav-number">3.1.</span> <span class="nav-text">DeepFM 的前辈们</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%8E%E9%98%B6%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88%EF%BC%9AFM"><span class="nav-number">3.1.1.</span> <span class="nav-text">低阶特征组合：FM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#POLY2"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">POLY2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E4%BC%98%E5%8C%96"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">计算优化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E9%98%B6%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88%EF%BC%9ADNN%E3%80%81FNN"><span class="nav-number">3.1.2.</span> <span class="nav-text">高阶特征组合：DNN、FNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DNN"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">DNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FNN"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">FNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Wide-amp-Deep"><span class="nav-number">3.1.3.</span> <span class="nav-text">Wide&amp;Deep</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%88%B0DeepFM"><span class="nav-number">3.2.</span> <span class="nav-text">回到DeepFM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FM-Component"><span class="nav-number">3.2.1.</span> <span class="nav-text">FM Component</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Component"><span class="nav-number">3.2.2.</span> <span class="nav-text">Deep Component</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DeepFM-%E4%BB%A3%E7%A0%81"><span class="nav-number">4.</span> <span class="nav-text">DeepFM 代码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">4.1.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="nav-number">4.2.</span> <span class="nav-text">特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dense-%E7%89%B9%E5%BE%81"><span class="nav-number">4.2.1.</span> <span class="nav-text">Dense 特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sparse-%E7%89%B9%E5%BE%81"><span class="nav-number">4.2.2.</span> <span class="nav-text">Sparse 特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-DataGenerator-Dataset-Dataloader"><span class="nav-number">4.3.</span> <span class="nav-text">定义 DataGenerator (Dataset + Dataloader)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-Model"><span class="nav-number">4.4.</span> <span class="nav-text">定义 Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-trainer"><span class="nav-number">4.5.</span> <span class="nav-text">定义 trainer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0"><span class="nav-number">4.6.</span> <span class="nav-text">训练和评估</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%85%B6%E5%AE%83%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.7.</span> <span class="nav-text">使用其它排序模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B0%83%E7%94%A8%E7%8E%B0%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.7.1.</span> <span class="nav-text">调用现成模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DIN"><span class="nav-number">5.</span> <span class="nav-text">DIN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Base-Model"><span class="nav-number">5.1.</span> <span class="nav-text">Base Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivation"><span class="nav-number">5.2.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%BF%80%E6%B4%BB%E5%8D%95%E5%85%83"><span class="nav-number">5.3.</span> <span class="nav-text">注意力激活单元</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E5%8F%91%E7%8E%B0"><span class="nav-number">5.4.</span> <span class="nav-text">有意思的发现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DIN%E4%BB%A3%E7%A0%81"><span class="nav-number">6.</span> <span class="nav-text">DIN代码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">6.1.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">6.2.</span> <span class="nav-text">定义数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-DataGenerator-Dataset-Dataloader-1"><span class="nav-number">6.3.</span> <span class="nav-text">定义 DataGenerator (Dataset + Dataloader)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.4.</span> <span class="nav-text">定义模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%99%A8"><span class="nav-number">6.5.</span> <span class="nav-text">定义训练器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0-1"><span class="nav-number">6.6.</span> <span class="nav-text">训练和评估</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">8.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Guadzilla"
      src="/images/touxiang.jpg">
  <p class="site-author-name" itemprop="name">Guadzilla</p>
  <div class="site-description" itemprop="description">大连理工大学在读硕士，研究方向为：NLP/序列推荐</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">26</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/guadzilla" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;guadzilla" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/374733751@qq.com" title="E-Mail → 374733751@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/1567748478" title="Study → https:&#x2F;&#x2F;space.bilibili.com&#x2F;1567748478" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>Study</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.hellohank.cn/" title="https:&#x2F;&#x2F;www.hellohank.cn" rel="noopener" target="_blank">极客Hank</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zhang-xiaokun.github.io/" title="https:&#x2F;&#x2F;zhang-xiaokun.github.io&#x2F;" rel="noopener" target="_blank">学术Star堃</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zyh</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
