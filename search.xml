<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Docker容器数据卷</title>
    <url>/2022/04/05/Docker%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7/</url>
    <content><![CDATA[<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405151429046.png" alt="image-20220405151429046" style="zoom:50%;" />

<h2 id="什么是数据卷"><a href="#什么是数据卷" class="headerlink" title="什么是数据卷"></a>什么是数据卷</h2><p>数据不应该放在容器中，如果容器删除，数据就会丢失！==需求：数据 持久化==</p>
<p>MYSQL，容器删了=删库跑路！==需求：MYSQL数据可以存储在本地==</p>
<p>==&gt; 需要容器之间有一个数据共享的技术！Docker容器中产生的数据，同步到本地！</p>
<p>这就是卷技术！说白了就是目录的挂载，将容器内的目录，挂载到 Linux 上</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405151429046.png" alt="image-20220405151429046" style="zoom:50%;" />

<p><strong>总结：容器的持久化和同步操作！ 容器间也可以数据共享！</strong></p>
<h2 id="使用数据卷"><a href="#使用数据卷" class="headerlink" title="使用数据卷"></a>使用数据卷</h2><blockquote>
<p>方式一：直接使用命令来挂载 -V</p>
</blockquote>
<span id="more"></span>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run -it -v 主机目录：容器内目录 -p 主机端口：容器内端口 </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~$ sudo docker run -it -v /home/dutir/xxx/ceshi:/home centos /bin/bash</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看容器id</span></span><br><span class="line">xxx@data:~$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                  PORTS                                                  NAMES</span><br><span class="line">43b6593461e7        centos                     &quot;/bin/bash&quot;              2 minutes ago       Up 2 minutes                                                                   quizzical_varahamihira</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看容器详细信息</span></span><br><span class="line">xxx@data:~$ sudo docker inspect 43b6593461e7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 看到挂载</span></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405153329429.png" alt="image-20220405153329429" style="zoom:67%;" />、</p>
<p><strong>测试文件的同步</strong></p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183512106.png" alt="image-20220405183512106" style="zoom:80%;" />



<p><strong>再来测试</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.停止容器</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.宿主机上修改文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.启动容器</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.容器内的数据依旧是同步的</span></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183540730.png" alt="image-20220405183540730"></p>
<p>好处：我们以后修改只需要在本地修改即可，容器内会自动同步！</p>
<h2 id="实战：安装MySQL"><a href="#实战：安装MySQL" class="headerlink" title="实战：安装MySQL"></a>实战：安装MySQL</h2><p>思考：MySQL 的数据持久化问题，</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405160002622.png" alt="image-20220405160002622" style="zoom: 67%;" />

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取镜像</span></span><br><span class="line">docker pull mysql:5.7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行容器，需要做数据挂载！  <span class="comment"># 安装启动mysql。需要配置密码，这是要注意的，去docker hub官方文档看（上图）</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 官方测试 $ docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -e 配置环境，这里要配置密码</span> </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动我们的</span></span><br><span class="line">-d 后台运行</span><br><span class="line">-p 端口映射</span><br><span class="line">-v 数据卷挂载，可挂载多个</span><br><span class="line">-e 环境配置</span><br><span class="line">--name 容器名字</span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker run -d -p 3310:3306 -v /home/dutir/xxx/mysql_test/conf:/etc/mysql/conf.d -v /home/dutir/xxx/mysql_test/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动成功之后，我们在本地使用 sqlyog 连接测试</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sqlyog 连接到本地的3310，本地3310侦听服务器宿主机的3310，宿主机的3310和容器内的3306映射，这个时候就可以连接上了</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在本地sqlyog测试创建一个数据库，查看一下我们映射的路径是否ok！</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> test_for_docker以后，docker里多出这个数据库</span></span><br></pre></td></tr></table></figure>

<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183614297.png" alt="image-20220405183614297" style="zoom:67%;" />

<p>即使把容器删除</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183721449.png" alt="image-20220405183721449" style="zoom:67%;" />

<p>发现，我们挂载到本地的数据卷依旧没有丢失，这就实现了容器持久化功能！</p>
<h2 id="具名和匿名挂载"><a href="#具名和匿名挂载" class="headerlink" title="具名和匿名挂载"></a>具名和匿名挂载</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 匿名挂载（不指定主机名）</span></span><br><span class="line">-v 容器内路径</span><br><span class="line">docker run -d -P --name nginx01 -v /etc/nginx nginx</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有本地volume（数据卷）	乱码的都是匿名卷</span></span><br><span class="line">xxx@data:~$ sudo docker volume ls</span><br><span class="line">DRIVER              VOLUME NAME</span><br><span class="line">local               51ed233d17e5c740a92f70e075335dd59cbcf913dd03390d64880893a6a6b043</span><br><span class="line">local               mysql_lawbda_my-db</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里发现，这种就是匿名挂在，我们在 -v 时只写了容器内的路径，没有写容器外的路径！</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 具名挂载</span></span><br><span class="line">docker run -d -P --name nginx01 -v juming-nginx:/etc/nginx nginx</span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker volume ls</span><br><span class="line">DRIVER              VOLUME NAME</span><br><span class="line">local               51ed233d17e5c740a92f70e075335dd59cbcf913dd03390d64880893a6a6b043</span><br><span class="line">local				juming-nginx</span><br><span class="line">local               mysql_lawbda_my-db</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过 -v 卷名：容器内路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看一下这个卷（这里查看mysql_lawbda_my-db）</span></span><br><span class="line">xxx@data:~$ sudo docker volume inspect mysql_lawbda_my-db</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;CreatedAt&quot;: &quot;2022-04-04T17:14:49+08:00&quot;,</span><br><span class="line">        &quot;Driver&quot;: &quot;local&quot;,</span><br><span class="line">        &quot;Labels&quot;: &#123;</span><br><span class="line">            &quot;com.docker.compose.project&quot;: &quot;mysql_lawbda&quot;,</span><br><span class="line">            &quot;com.docker.compose.version&quot;: &quot;1.27.4&quot;,</span><br><span class="line">            &quot;com.docker.compose.volume&quot;: &quot;my-db&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/mysql_lawbda_my-db/_data&quot;,</span><br><span class="line">        &quot;Name&quot;: &quot;mysql_lawbda_my-db&quot;,</span><br><span class="line">        &quot;Options&quot;: null,</span><br><span class="line">        &quot;Scope&quot;: &quot;local&quot;</span><br></pre></td></tr></table></figure>

<p>所有达到docker容器内的卷，没有指定目录的情况下都是在<code>/val/lib/docker/volumes/xxxx/_data</code></p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405164118444.png" alt="image-20220405164118444" style="zoom: 67%;" />

<p>我们通过具名挂载 可以方便找到我们的一个卷，大多数情况在使用<code>具名挂载</code> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如何确定是具名挂载还是匿名挂在，还是指定路径挂载！</span></span><br><span class="line"></span><br><span class="line">-v 容器内路径				 # 匿名挂载</span><br><span class="line">-v 卷名:容器内路径    			# 具名挂载</span><br><span class="line">-v /宿主机路径:容器内路径	   	  # 指定路径挂载 </span><br></pre></td></tr></table></figure>

<p>拓展：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 通过 -v 容器内路径:ro  rw  改变读写权限</span></span><br><span class="line">ro	read only	# 只读</span><br><span class="line">rw  readwrite	# 可读可写</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一旦设置了容器权限，容器对我们挂载出来的内容就有限定了！</span></span><br><span class="line">docker run -d -P --name nginx01 -v juming-nginx:/etc/nginx:ro nginx</span><br><span class="line">docker run -d -P --name nginx01 -v juming-nginx:/etc/nginx:rw nginx</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ro  只要看到ro，就说明这个路径只能通过宿主机来改变，容器内部是无法操作的</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="初识Dockerfile"><a href="#初识Dockerfile" class="headerlink" title="初识Dockerfile"></a>初识Dockerfile</h2><blockquote>
<p>方式二：生成镜像的时候就挂载出来</p>
</blockquote>
<p>Dockerfile 就是用来构建 docker 镜像的构建文件！ 就是一段命令脚本！先体验一下！</p>
<p>通过这个脚本可以生成镜像，镜像是一层一层的，脚本就是一个一个的命令，每个命令就是一层。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建一个dockerfile文件，名字可以随机， 建议 Dockerfile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件中的内容  指令（都是大写） 参数</span></span><br><span class="line"></span><br><span class="line">FROM centos</span><br><span class="line"></span><br><span class="line">VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]	</span><br><span class="line"></span><br><span class="line">CMD echo &quot;----end----&quot;</span><br><span class="line"></span><br><span class="line">CMD /bin/bash</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里的每个命令，就是镜像的一层！</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker build 构建镜像</span></span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# docker build -f /home/dutir/xxx/docker-test-volume/dockerfile1 -t wjm/centos:1.0 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/4 : FROM centos</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 5d0da3dc9764</span></span><br><span class="line">Step 2/4 : VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> eedc2cf7a5b3</span></span><br><span class="line">Removing intermediate container eedc2cf7a5b3</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 80396b388983</span></span><br><span class="line">Step 3/4 : CMD echo &quot;----end----&quot;</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 734dc31604b0</span></span><br><span class="line">Removing intermediate container 734dc31604b0</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 5ef9da00ac40</span></span><br><span class="line">Step 4/4 : CMD /bin/bash</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 593084c92533</span></span><br><span class="line">Removing intermediate container 593084c92533</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 8edda645979e</span></span><br><span class="line">Successfully built 8edda645979e</span><br><span class="line">Successfully tagged wjm/centos:1.0</span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动一下自己生成的镜像</span></span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# docker images</span><br><span class="line">REPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">wjm/centos                  1.0                 8edda645979e        4 minutes ago       231MB</span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# docker run -it 8edda645979e /bin/bash</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405170430808.png" alt="image-20220405170430808"></p>
<p>这个目录就是我们生成镜像时自动挂载的，数据卷目录。</p>
<p>这个和外部一定有一个同步的目录！dockerfile里只指定了容器内目录，一定是匿名挂载，所以容器外、宿主机上肯定有乱码的挂载。</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405170800369.png" alt="image-20220405170800369" style="zoom: 67%;" />

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker inspect 查看容器信息</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 找到 Mounts，查看卷挂载的路径，符合匿名挂载的默认路径/var/lib/docker/volumes/xxx/</span></span><br></pre></td></tr></table></figure>

<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405171126981.png" alt="image-20220405171126981" style="zoom:67%;" />

<p>在容器里的 volume01 下新建一个 container.txt 文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@4b4b467cf777 /]# ls</span><br><span class="line">bin  etc   lib	  lost+found  mnt  proc  run   srv  tmp  var	   volume02</span><br><span class="line">dev  home  lib64  media       opt  root  sbin  sys  usr  volume01</span><br><span class="line">[root@4b4b467cf777 /]# cd volume01</span><br><span class="line">[root@4b4b467cf777 volume01]# ls</span><br><span class="line">[root@4b4b467cf777 volume01]# touch container.txt</span><br><span class="line">[root@4b4b467cf777 volume01]# ls</span><br><span class="line">container.txt</span><br><span class="line">[root@4b4b467cf777 volume01]# </span><br></pre></td></tr></table></figure>

<p>测试一下刚才的文件是否同步出去。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 复制 Mounts 下的路径 /var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> 到这里</span></span><br><span class="line"></span><br><span class="line">root@data:~# cd /var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data</span><br><span class="line">root@data:/var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data# ls</span><br><span class="line">container.txt		# 发现新建的 container.txt 文件</span><br><span class="line">root@data:/var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data#</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这种方式我们未来使用的十分多，因为我们通常会构建自己的镜像！</p>
<p>假设构建镜像时没有挂载卷，要手动镜像挂载 -v 卷名:容器内路径 </p>
<h2 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h2><blockquote>
<p>即容器和容器间同步</p>
</blockquote>
<p>多个 Mysql 数据如何同步？</p>
<p>==所有 docker 之间共享的卷都是独立的，每个docker有自己的数据卷。新建时拷贝父容器的备份。所以只要有一份存在，数据都不会丢失。==</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405172422841.png" alt="image-20220405172422841"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动3个容器，通过我们刚才自己写的镜像启动</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 首先启动 docker01</span> </span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405172959557.png" alt="image-20220405172959557"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动 docker02，volume 从 docker01 继承</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker01 就叫做数据卷容器</span></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405173218738.png" alt="image-20220405173218738"></p>
<p>在 docker01 修改数据， docker02 会同步吗？测试一下。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405173548023.png" alt="image-20220405173548023"></p>
<p>回到 docker02 查看</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405173743825.png" alt="image-20220405173743825"></p>
<p>docker01 创建的内容同步到 docker02 了</p>
<p>再从 docker01 继承，启动 docker03，新建 docker03.txt</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405174426597.png" alt="image-20220405174426597"></p>
<p>在 docker03 里创建的文件，在 docker01 和 docker02 里都同步了。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405175122829.png" alt="image-20220405175122829"></p>
<p>只要是通过 –volume-from ，我们就可以实现容器间的数据共享。</p>
<p>把 docker01 整个 rm 掉，docker02 和 docker03 的文件都还在。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 测试：可以删除docker01，查看一下docker02和docker03是否还可以访问这个文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 依旧可以访问</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试：查看宿主机的volume，还是存在</span></span><br><span class="line">root@data:~# cd /var/lib/docker/volumes/2bdf8b38df84ffea344d0f758e7be2c2045c54f270a9da362b12246a1d78d636/_data</span><br><span class="line">root@data:/var/lib/docker/volumes/2bdf8b38df84ffea344d0f758e7be2c2045c54f270a9da362b12246a1d78d636/_data# ls</span><br><span class="line">docker01.txt  docker03.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>==所有 docker 之间共享的卷都是独立的，每个docker有自己的数据卷。新建时 <strong>拷贝</strong> 父容器的备份。所以只要有一份存在，数据都不会丢失。==</p>
<p>回到问题：多个 Mysql 数据如何同步？</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> -v 容器内路径，匿名挂载</span></span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker run -d -p 3310:3306 -v /etc/mysql/conf.d -v /var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 用 --volumes-from 可以实现两个容器同步</span></span><br><span class="line">xxx@data:~$ sudo docker run -d -p 3310:3306  -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 --volumes-from mysql01 mysql:5.7</span><br></pre></td></tr></table></figure>



<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止（所有容器都停止）。</p>
<p>但是一旦你持久化到了本地，这个时候，本地的数据是不会删除的。所以重要的文件存在宿主机上就好啦。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以上是看 b 站教学视频记的笔记。</p>
<p>教程地址：<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=9">【狂神说Java】Docker最新超详细版教程通俗易懂_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker常用命令</title>
    <url>/2022/04/05/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a>帮助命令</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker version		# 显示docker 的版本信息</span><br><span class="line">docker info			# 显示docker 的系统信息，包括镜像和容器的数量</span><br><span class="line">docker 命令 --help 	# 帮助命令</span><br></pre></td></tr></table></figure>

<p>帮助文档的地址：<a href="https://docs.docker.com/reference/">参考文档|Docker 文档</a></p>
<h2 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h2><p><strong>docker images 查看所有本地的主机上的镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xxx@data:~/solrdata$ sudo docker images </span><br><span class="line">REPOSITORY                        TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">solr                              8                   ce1fcccc6f5e        5 days ago          563MB</span><br><span class="line">solr                              latest              ce1fcccc6f5e        5 days ago          563MB</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解释</span></span><br><span class="line">REPOSITORY 镜像的仓库源</span><br><span class="line">TAG        镜像的标签</span><br><span class="line">IMAGE ID   镜像的id</span><br><span class="line">CREATE     镜像的创建时间</span><br><span class="line">SIZE       镜像的大小</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可选项</span></span><br><span class="line">Options:</span><br><span class="line">  -a, --all             # 列出所有镜像</span><br><span class="line">  -q, --quiet           # 只显示镜像的id</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker images -aq</span><br><span class="line">ce1fcccc6f5e</span><br><span class="line">ce1fcccc6f5e</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<p><strong>docker search 搜索镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xxx@data:~/solrdata$ sudo docker search mysql</span><br><span class="line">NAME                             DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">mysql                            MySQL is a widely used, open-source relation…   12350               [OK]                </span><br><span class="line">mariadb                          MariaDB Server is a high performing open sou…   4753                [OK]                </span><br><span class="line">mysql/mysql-server               Optimized MySQL Server Docker images. Create…   916                                     [OK]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可选项，通过收藏来过滤</span></span><br><span class="line">--filter=STARS=3000		# 搜索出来的镜像就是STARS大于3000的</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker search mysql --filter=STARS=3000</span><br><span class="line">NAME                DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">mysql               MySQL is a widely used, open-source relation…   12350               [OK]                </span><br><span class="line">mariadb             MariaDB Server is a high performing open sou…   4753                [OK]                </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>docker pull 下载镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载镜像 docker pull 镜像名[:tag]</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker pull mysql</span><br><span class="line">Using default tag: latest	# 如果不写tag,就默认latest最新版</span><br><span class="line">latest: Pulling from library/mysql</span><br><span class="line">f003217c5aae: Pull complete 	# 分层下载,docker image的核心 联合文件系统</span><br><span class="line">65d94f01a09f: Pull complete </span><br><span class="line">43d78aaa6078: Pull complete </span><br><span class="line">a0f91ffbdf69: Pull complete </span><br><span class="line">59ee9e07e12f: Pull complete </span><br><span class="line">04d82978082c: Pull complete </span><br><span class="line">70f46ebb971a: Pull complete </span><br><span class="line">db6ea71d471d: Pull complete </span><br><span class="line">c2920c795b25: Pull complete </span><br><span class="line">26c3bdf75ff5: Pull complete </span><br><span class="line">9ec1f1f78b0e: Pull complete </span><br><span class="line">4607fa685ac6: Pull complete </span><br><span class="line">Digest: sha256:1c75ba7716c6f73fc106dacedfdcf13f934ea8c161c8b3b3e4618bcd5fbcf195	# 签名</span><br><span class="line">Status: Downloaded newer image for mysql:latest</span><br><span class="line">docker.io/library/mysql:latest	# 真实地址</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 等价于它</span></span><br><span class="line">docker pull mysql</span><br><span class="line">docker pull docker.io/library/mysql:latest</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定版本下载</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker pull mysql:5.7</span><br><span class="line">5.7: Pulling from library/mysql</span><br><span class="line">f003217c5aae: Already exists </span><br><span class="line">65d94f01a09f: Already exists </span><br><span class="line">43d78aaa6078: Already exists </span><br><span class="line">a0f91ffbdf69: Already exists </span><br><span class="line">59ee9e07e12f: Already exists </span><br><span class="line">04d82978082c: Already exists </span><br><span class="line">70f46ebb971a: Already exists 	</span><br><span class="line">ba61822c65c2: Pull complete 	# 指定版本只需要下载和之前不同的部分</span><br><span class="line">dec59acdf78a: Pull complete </span><br><span class="line">0a05235a6981: Pull complete </span><br><span class="line">c87d621d6916: Pull complete </span><br><span class="line">Digest: sha256:1a73b6a8f507639a8f91ed01ace28965f4f74bb62a9d9b9e7378d5f07fab79dc</span><br><span class="line">Status: Downloaded newer image for mysql:5.7</span><br><span class="line">docker.io/library/mysql:5.7</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183232675.png" alt="image-20220405183232675"></p>
<p><strong>docker rmi 删除镜像</strong></p>
<p>rmi=rm 删除 + i 镜像(image),删除镜像</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rmi 镜像id 					# 删除指定镜像</span><br><span class="line">docker rmi 镜像id 镜像id 镜像id	 	 # 删除多个镜像</span><br><span class="line">docker rmi -f $(docker images -aq)	 # 删除全部镜像</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker rmi f26e21ddd20d	# 镜像id</span><br></pre></td></tr></table></figure>







<h2 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h2><p><strong>说明:我们有了镜像才可以创建容器,linux,下载一个centos镜像来测试学习</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull centos</span><br></pre></td></tr></table></figure>

<p><strong>新建容器并启动</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run [可选参数] image</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 参数说明</span></span><br><span class="line">--name=“Nmae” 	容器名字  solr01 solr02 用来区分容器</span><br><span class="line">-d            	后台方式运行，类型nohup，docker里只要加上-d</span><br><span class="line">-it				使用交互方式运行，进入容器查看内容</span><br><span class="line">-p				指定容器的端口 -p 8080:8080</span><br><span class="line">	-p ip:主机端口：容器端口</span><br><span class="line">	-p 主机端口:容器端口（容器）</span><br><span class="line">	-p 容器端口</span><br><span class="line">	容器端口</span><br><span class="line">-P				随机指定端口</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试,启动并进入容器 -it交互模式</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker run -it centos /bin/bash</span><br><span class="line">[root@17e53f625cbc /]# ls	# 查看容器内部的centos,基础版本,很多命令是不完善的</span><br><span class="line">bin  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 从容器中退回主机</span></span><br><span class="line">[root@17e53f625cbc /]# exit</span><br><span class="line">exit</span><br><span class="line">xxx@data:~/solrdata$ ls</span><br><span class="line">data  log4j2.xml  logs</span><br><span class="line">xxx@data:~/solrdata$ </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>列出所有运行中的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker ps 命令</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> 列出当前正在运行的容器</span></span><br><span class="line">-a		# 列出当前正在运行的容器+带出历史运行过的容器</span><br><span class="line">-n=?	# 显示最近创建的容器</span><br><span class="line">-q		# 只显示容器的编号</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS               PORTS               NAMES</span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS               PORTS               NAMES</span><br><span class="line">17e53f625cbc        centos                                                 &quot;/bin/bash&quot;              3 minutes ago       Exited (0) 58 seconds ago                                                            magical_haibt</span><br></pre></td></tr></table></figure>

<p><strong>退出容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">exit	# 直接容器停止并退出</span><br><span class="line">Crtl + P + Q # 容器不停止退出</span><br></pre></td></tr></table></figure>

<p><strong>删除容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rm 容器id					# 删除指定容器,不能删除正在运行的容器,如果要强制删除,rm -f</span><br><span class="line">docker rm -f $(docker ps -aq)	 # 删除所有容器</span><br><span class="line">docker ps -a -q|xargs docker rm	 # 删除所有容器(高级)</span><br></pre></td></tr></table></figure>

<p><strong>启动和停止容器的操作</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker start 容器id		# 启动容器</span><br><span class="line">docker restart 容器id		# 重启容器</span><br><span class="line">docker stop 容器id		# 停止当前正在运行的容器</span><br><span class="line">docker kill 容器id		# 强制停止当前容器</span><br></pre></td></tr></table></figure>

<h2 id="常用其他命令"><a href="#常用其他命令" class="headerlink" title="常用其他命令"></a>常用其他命令</h2><p><strong>后台启动容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker run -d 镜像名</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker run -d centos</span><br><span class="line">7c5bca7cedbe3d4c32aa5273b9068a9d154a2e5f7a665f5861f8369a1a592862</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 问题 docker ps 发现centos停止了</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS               PORTS               NAMES</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 常见的坑,docker 容器使用后台运行,就必须要有一个前台进程,docker发现没有应用,就会自动停止</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> nginx,容器启动后,发现自己没有提供服务,就会立刻停止,就是没有程序了</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>查看日志</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 可选项</span></span><br><span class="line">Options:</span><br><span class="line">  -f, --follow         Follow log output</span><br><span class="line">      --tail string    Number of lines to show from the end of the logs (default &quot;all&quot;)</span><br><span class="line">  -t, --timestamps     Show timestamps</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker logs -f -t 508f4663f65e # 没有日志</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自己编写一段shell脚本</span></span><br><span class="line">&quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line">xxx@data:~/solrdata$ sudo docker run -d centos /bin/bash -c &quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                  PORTS                                                  NAMES</span><br><span class="line">5a96132edf5d        centos                     &quot;/bin/bash -c &#x27;while…&quot;   55 seconds ago      Up 53 seconds                                                                  priceless_elbakyan</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示日志</span></span><br><span class="line"> -tf		   # 显示日志</span><br><span class="line"> --tail number # 要显示的日志条数</span><br></pre></td></tr></table></figure>

<p><strong>查看容器中的进程信息</strong> ps</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker top 容器id</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker top 5a96132edf5d</span><br><span class="line">UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD</span><br><span class="line">root                4109297             4109277             0                   15:33               ?                   00:00:00            /bin/bash -c while true;do echo wjm;sleep 1;done</span><br><span class="line">root                4112148             4109297             0                   15:40               ?                   00:00:00            /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 1</span><br></pre></td></tr></table></figure>

<p><strong>查看镜像的元数据</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker inspect 容器id</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker inspect 5a96132edf5d</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;Id&quot;: &quot;5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67&quot;,</span><br><span class="line">        &quot;Created&quot;: &quot;2022-04-04T07:33:47.224853805Z&quot;,</span><br><span class="line">        &quot;Path&quot;: &quot;/bin/bash&quot;,</span><br><span class="line">        &quot;Args&quot;: [</span><br><span class="line">            &quot;-c&quot;,</span><br><span class="line">            &quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;State&quot;: &#123;</span><br><span class="line">            &quot;Status&quot;: &quot;running&quot;,</span><br><span class="line">            &quot;Running&quot;: true,</span><br><span class="line">            &quot;Paused&quot;: false,</span><br><span class="line">            &quot;Restarting&quot;: false,</span><br><span class="line">            &quot;OOMKilled&quot;: false,</span><br><span class="line">            &quot;Dead&quot;: false,</span><br><span class="line">            &quot;Pid&quot;: 4109297,</span><br><span class="line">            &quot;ExitCode&quot;: 0,</span><br><span class="line">            &quot;Error&quot;: &quot;&quot;,</span><br><span class="line">            &quot;StartedAt&quot;: &quot;2022-04-04T07:33:48.561816958Z&quot;,</span><br><span class="line">            &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Image&quot;: &quot;sha256:5d0da3dc976460b72c77d94c8a1ad043720b0416bfc16c52c45d4847e53fadb6&quot;,</span><br><span class="line">        &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/resolv.conf&quot;,</span><br><span class="line">        &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/hostname&quot;,</span><br><span class="line">        &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/hosts&quot;,</span><br><span class="line">        &quot;LogPath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67-json.log&quot;,</span><br><span class="line">        &quot;Name&quot;: &quot;/priceless_elbakyan&quot;,</span><br><span class="line">        &quot;RestartCount&quot;: 0,</span><br><span class="line">        &quot;Driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">        &quot;Platform&quot;: &quot;linux&quot;,</span><br><span class="line">        &quot;MountLabel&quot;: &quot;&quot;,</span><br><span class="line">        &quot;ProcessLabel&quot;: &quot;&quot;,</span><br><span class="line">        &quot;AppArmorProfile&quot;: &quot;docker-default&quot;,</span><br><span class="line">        &quot;ExecIDs&quot;: null,</span><br><span class="line">        &quot;HostConfig&quot;: &#123;</span><br><span class="line">            &quot;Binds&quot;: null,</span><br><span class="line">            &quot;ContainerIDFile&quot;: &quot;&quot;,</span><br><span class="line">            &quot;LogConfig&quot;: &#123;</span><br><span class="line">                &quot;Type&quot;: &quot;json-file&quot;,</span><br><span class="line">                &quot;Config&quot;: &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;NetworkMode&quot;: &quot;default&quot;,</span><br><span class="line">            &quot;PortBindings&quot;: &#123;&#125;,</span><br><span class="line">            &quot;RestartPolicy&quot;: &#123;</span><br><span class="line">                &quot;Name&quot;: &quot;no&quot;,</span><br><span class="line">                &quot;MaximumRetryCount&quot;: 0</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;AutoRemove&quot;: false,</span><br><span class="line">            &quot;VolumeDriver&quot;: &quot;&quot;,</span><br><span class="line">            &quot;VolumesFrom&quot;: null,</span><br><span class="line">            &quot;CapAdd&quot;: null,</span><br><span class="line">            &quot;CapDrop&quot;: null,</span><br><span class="line">            &quot;Capabilities&quot;: null,</span><br><span class="line">            &quot;Dns&quot;: [],</span><br><span class="line">            &quot;DnsOptions&quot;: [],</span><br><span class="line">            &quot;DnsSearch&quot;: [],</span><br><span class="line">            &quot;ExtraHosts&quot;: null,</span><br><span class="line">            &quot;GroupAdd&quot;: null,</span><br><span class="line">            &quot;IpcMode&quot;: &quot;private&quot;,</span><br><span class="line">            &quot;Cgroup&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Links&quot;: null,</span><br><span class="line">            &quot;OomScoreAdj&quot;: 0,</span><br><span class="line">            &quot;PidMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Privileged&quot;: false,</span><br><span class="line">            &quot;PublishAllPorts&quot;: false,</span><br><span class="line">            &quot;ReadonlyRootfs&quot;: false,</span><br><span class="line">            &quot;SecurityOpt&quot;: null,</span><br><span class="line">            &quot;UTSMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;UsernsMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;ShmSize&quot;: 67108864,</span><br><span class="line">            &quot;Runtime&quot;: &quot;runc&quot;,</span><br><span class="line">            &quot;ConsoleSize&quot;: [</span><br><span class="line">                0,</span><br><span class="line">                0</span><br><span class="line">            ],</span><br><span class="line">            &quot;Isolation&quot;: &quot;&quot;,</span><br><span class="line">            &quot;CpuShares&quot;: 0,</span><br><span class="line">            &quot;Memory&quot;: 0,</span><br><span class="line">            &quot;NanoCpus&quot;: 0,</span><br><span class="line">            &quot;CgroupParent&quot;: &quot;&quot;,</span><br><span class="line">            &quot;BlkioWeight&quot;: 0,</span><br><span class="line">            &quot;BlkioWeightDevice&quot;: [],</span><br><span class="line">            &quot;BlkioDeviceReadBps&quot;: null,</span><br><span class="line">            &quot;BlkioDeviceWriteBps&quot;: null,</span><br><span class="line">            &quot;BlkioDeviceReadIOps&quot;: null,</span><br><span class="line">            &quot;BlkioDeviceWriteIOps&quot;: null,</span><br><span class="line">            &quot;CpuPeriod&quot;: 0,</span><br><span class="line">            &quot;CpuQuota&quot;: 0,</span><br><span class="line">            &quot;CpuRealtimePeriod&quot;: 0,</span><br><span class="line">            &quot;CpuRealtimeRuntime&quot;: 0,</span><br><span class="line">            &quot;CpusetCpus&quot;: &quot;&quot;,</span><br><span class="line">            &quot;CpusetMems&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Devices&quot;: [],</span><br><span class="line">            &quot;DeviceCgroupRules&quot;: null,</span><br><span class="line">            &quot;DeviceRequests&quot;: null,</span><br><span class="line">            &quot;KernelMemory&quot;: 0,</span><br><span class="line">            &quot;KernelMemoryTCP&quot;: 0,</span><br><span class="line">            &quot;MemoryReservation&quot;: 0,</span><br><span class="line">            &quot;MemorySwap&quot;: 0,</span><br><span class="line">            &quot;MemorySwappiness&quot;: null,</span><br><span class="line">            &quot;OomKillDisable&quot;: false,</span><br><span class="line">            &quot;PidsLimit&quot;: null,</span><br><span class="line">            &quot;Ulimits&quot;: null,</span><br><span class="line">            &quot;CpuCount&quot;: 0,</span><br><span class="line">            &quot;CpuPercent&quot;: 0,</span><br><span class="line">            &quot;IOMaximumIOps&quot;: 0,</span><br><span class="line">            &quot;IOMaximumBandwidth&quot;: 0,</span><br><span class="line">            &quot;MaskedPaths&quot;: [</span><br><span class="line">                &quot;/proc/asound&quot;,</span><br><span class="line">                &quot;/proc/acpi&quot;,</span><br><span class="line">                &quot;/proc/kcore&quot;,</span><br><span class="line">                &quot;/proc/keys&quot;,</span><br><span class="line">                &quot;/proc/latency_stats&quot;,</span><br><span class="line">                &quot;/proc/timer_list&quot;,</span><br><span class="line">                &quot;/proc/timer_stats&quot;,</span><br><span class="line">                &quot;/proc/sched_debug&quot;,</span><br><span class="line">                &quot;/proc/scsi&quot;,</span><br><span class="line">                &quot;/sys/firmware&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;ReadonlyPaths&quot;: [</span><br><span class="line">                &quot;/proc/bus&quot;,</span><br><span class="line">                &quot;/proc/fs&quot;,</span><br><span class="line">                &quot;/proc/irq&quot;,</span><br><span class="line">                &quot;/proc/sys&quot;,</span><br><span class="line">                &quot;/proc/sysrq-trigger&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;GraphDriver&quot;: &#123;</span><br><span class="line">            &quot;Data&quot;: &#123;</span><br><span class="line">                &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e-init/diff:/var/lib/docker/overlay2/81761618a33f3926b117dce5b1a9ae7094d898b3d32f20d50da147a2c0c1dfd0/diff&quot;,</span><br><span class="line">                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e/merged&quot;,</span><br><span class="line">                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e/diff&quot;,</span><br><span class="line">                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e/work&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;Name&quot;: &quot;overlay2&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Mounts&quot;: [],</span><br><span class="line">        &quot;Config&quot;: &#123;</span><br><span class="line">            &quot;Hostname&quot;: &quot;5a96132edf5d&quot;,</span><br><span class="line">            &quot;Domainname&quot;: &quot;&quot;,</span><br><span class="line">            &quot;User&quot;: &quot;&quot;,</span><br><span class="line">            &quot;AttachStdin&quot;: false,</span><br><span class="line">            &quot;AttachStdout&quot;: false,</span><br><span class="line">            &quot;AttachStderr&quot;: false,</span><br><span class="line">            &quot;Tty&quot;: false,</span><br><span class="line">            &quot;OpenStdin&quot;: false,</span><br><span class="line">            &quot;StdinOnce&quot;: false,</span><br><span class="line">            &quot;Env&quot;: [</span><br><span class="line">                &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;Cmd&quot;: [</span><br><span class="line">                &quot;/bin/bash&quot;,</span><br><span class="line">                &quot;-c&quot;,</span><br><span class="line">                &quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;Image&quot;: &quot;centos&quot;,</span><br><span class="line">            &quot;Volumes&quot;: null,</span><br><span class="line">            &quot;WorkingDir&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Entrypoint&quot;: null,</span><br><span class="line">            &quot;OnBuild&quot;: null,</span><br><span class="line">            &quot;Labels&quot;: &#123;</span><br><span class="line">                &quot;org.label-schema.build-date&quot;: &quot;20210915&quot;,</span><br><span class="line">                &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;,</span><br><span class="line">                &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;,</span><br><span class="line">                &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;,</span><br><span class="line">                &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;NetworkSettings&quot;: &#123;</span><br><span class="line">            &quot;Bridge&quot;: &quot;&quot;,</span><br><span class="line">            &quot;SandboxID&quot;: &quot;e8757375dc8ce5c604047bace5801859349b35be01c4d0983f6e18aefad6d7c8&quot;,</span><br><span class="line">            &quot;HairpinMode&quot;: false,</span><br><span class="line">            &quot;LinkLocalIPv6Address&quot;: &quot;&quot;,</span><br><span class="line">            &quot;LinkLocalIPv6PrefixLen&quot;: 0,</span><br><span class="line">            &quot;Ports&quot;: &#123;&#125;,</span><br><span class="line">            &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/e8757375dc8c&quot;,</span><br><span class="line">            &quot;SecondaryIPAddresses&quot;: null,</span><br><span class="line">            &quot;SecondaryIPv6Addresses&quot;: null,</span><br><span class="line">            &quot;EndpointID&quot;: &quot;35abfaa04e627a268c7c0a2279bbd919c831c66dcb7e87ca7afa95694b2c6cd0&quot;,</span><br><span class="line">            &quot;Gateway&quot;: &quot;172.17.0.1&quot;,</span><br><span class="line">            &quot;GlobalIPv6Address&quot;: &quot;&quot;,</span><br><span class="line">            &quot;GlobalIPv6PrefixLen&quot;: 0,</span><br><span class="line">            &quot;IPAddress&quot;: &quot;172.17.0.5&quot;,</span><br><span class="line">            &quot;IPPrefixLen&quot;: 16,</span><br><span class="line">            &quot;IPv6Gateway&quot;: &quot;&quot;,</span><br><span class="line">            &quot;MacAddress&quot;: &quot;02:42:ac:11:00:05&quot;,</span><br><span class="line">            &quot;Networks&quot;: &#123;</span><br><span class="line">                &quot;bridge&quot;: &#123;</span><br><span class="line">                    &quot;IPAMConfig&quot;: null,</span><br><span class="line">                    &quot;Links&quot;: null,</span><br><span class="line">                    &quot;Aliases&quot;: null,</span><br><span class="line">                    &quot;NetworkID&quot;: &quot;99b4b446329155dc0d91f3989f0cd78ac1c354467b34c72c1ac8a51085632eb8&quot;,</span><br><span class="line">                    &quot;EndpointID&quot;: &quot;35abfaa04e627a268c7c0a2279bbd919c831c66dcb7e87ca7afa95694b2c6cd0&quot;,</span><br><span class="line">                    &quot;Gateway&quot;: &quot;172.17.0.1&quot;,</span><br><span class="line">                    &quot;IPAddress&quot;: &quot;172.17.0.5&quot;,</span><br><span class="line">                    &quot;IPPrefixLen&quot;: 16,</span><br><span class="line">                    &quot;IPv6Gateway&quot;: &quot;&quot;,</span><br><span class="line">                    &quot;GlobalIPv6Address&quot;: &quot;&quot;,</span><br><span class="line">                    &quot;GlobalIPv6PrefixLen&quot;: 0,</span><br><span class="line">                    &quot;MacAddress&quot;: &quot;02:42:ac:11:00:05&quot;,</span><br><span class="line">                    &quot;DriverOpts&quot;: null</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>进入当前正在运行的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 我们通常容器都是使用后台方式运行的,需要进入容器修改一些配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 命令1 docker <span class="built_in">exec</span> -it 容器id bashShell</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE        COMMAND        CREATED        STATUS        PORTS        NAMES</span><br><span class="line">5a96132edf5d        centos        &quot;/bin/bash -c &#x27;while…&quot;        12 minutes ago        Up 12 minutes        priceless_elbakyan</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker exec -it 5a96132edf5d /bin/bash</span><br><span class="line">[root@5a96132edf5d /]# ls</span><br><span class="line">bin  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">[root@5a96132edf5d /]# ps -ef</span><br><span class="line">UID          PID    PPID  C STIME TTY          TIME CMD</span><br><span class="line">root           1       0  0 07:33 ?        00:00:00 /bin/bash -c while true;do echo wjm;sleep 1;done</span><br><span class="line">root         750       0  0 07:46 pts/0    00:00:00 /bin/bash</span><br><span class="line">root         771       1  0 07:46 ?        00:00:00 /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 1</span><br><span class="line">root         772     750  0 07:46 pts/0    00:00:00 ps -ef</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 命令2 docker attach 容器id</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker attach 5a96132edf5d </span><br><span class="line">正在执行当前代码...</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span>		<span class="comment"># 进入容器后开启一个新的终端,可以在里面操作(常用)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker attach		<span class="comment"># 进入容器正在执行的终端,不会启动新的进程!,while true的话就死循环了</span></span></span><br></pre></td></tr></table></figure>

<p><strong>从容器内拷贝文件到主机上</strong></p>
<p>容器内和容器外是隔离的,如何拷贝?</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404155319337.png" alt="image-20220404155319337"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker cp 容器id:容器内路径 目的的主机路径</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看帮助 看到是可以双向拷贝的</span></span><br><span class="line">xxx@data:~$ sudo docker cp --help</span><br><span class="line">Usage:	docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-</span><br><span class="line">	docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH</span><br><span class="line"></span><br><span class="line">Copy files/folders between a container and the local filesystem</span><br><span class="line"></span><br><span class="line">Use &#x27;-&#x27; as the source to read a tar archive from stdin</span><br><span class="line">and extract it to a directory destination in a container.</span><br><span class="line">Use &#x27;-&#x27; as the destination to stream a tar archive of a</span><br><span class="line">container source to stdout.</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -a, --archive       Archive mode (copy all uid/gid information)</span><br><span class="line">  -L, --follow-link   Always follow symbol link in SRC_PATH</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看当前主机目录下的文件</span></span><br><span class="line">xxx@data:~$ ls</span><br><span class="line">solrdata  total.csv</span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入docker容器内部</span></span><br><span class="line">xxx@data:~$ sudo docker attach 825f1e107b64</span><br><span class="line">[root@825f1e107b64 /]# ls</span><br><span class="line">bin  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">[root@825f1e107b64 /]# cd home/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在容器内新建一个文件</span></span><br><span class="line">[root@825f1e107b64 home]# touch test.java</span><br><span class="line">[root@825f1e107b64 home]# ls</span><br><span class="line">test.java</span><br><span class="line">[root@825f1e107b64 home]# exit</span><br><span class="line">exit</span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器内/home/test.java拷贝到主机上的默认目录</span></span><br><span class="line">xxx@data:~$ sudo docker cp 825f1e107b64:/home/test.java .	</span><br><span class="line">xxx@data:~$ ls</span><br><span class="line">solrdata  test.java  total.csv</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 拷贝是一个手动过程,未来我们使用 -v 卷的技术可以实现自动同步 /home /home</span></span><br></pre></td></tr></table></figure>

<h2 id="小节"><a href="#小节" class="headerlink" title="小节"></a>小节</h2><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404163030861.png" alt="image-20220404163030861"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Commands:</span><br><span class="line">  attach      Attach to a running container						# 当前 shell 下 attach 连接指定运行镜像</span><br><span class="line">  build       Build an image from a Dockerfile					# 通过 Dockerfile 定制镜像</span><br><span class="line">  commit      Create a new image from a container&#x27;s changes		# 提交当前容器为新镜像</span><br><span class="line">  cp          Copy files/folders between a container and the local filesystem	# 从容器中拷贝文件到宿主机中</span><br><span class="line">  create      Create a new container							# 创建一个新的容器,同 run ,但不启动容器</span><br><span class="line">  diff        Inspect changes  on a container&#x27;s filesystem		# 查看 docker 容器的变化</span><br><span class="line">  events      Get real time events from the server				# 从 docker 服务获取容器实时事件</span><br><span class="line">  exec        Run a command in a running container				# 再已存在的容器中运行命令</span><br><span class="line">  export      Export a container&#x27;s filesystem as a tar archive	# 导出容器的内容流作为一个 tar 归档文件[对应 import ]</span><br><span class="line">  history     Show the history of an image						# 展示一个镜像形成历史</span><br><span class="line">  images      List images										# 列出系统当前镜像</span><br><span class="line">  import      Import the contents from a tarball to create a filesystem image	# 从 tar 包中的内容创建一个新的文件系统映像[对应 import ]</span><br><span class="line">  info        Display system-wide information					# 展示系统相关信息</span><br><span class="line">  inspect     Return low-level information on Docker objects	# 查看容器详细信息</span><br><span class="line">  kill        Kill one or more running containers				# kill 指定容器</span><br><span class="line">  load        Load an image from a tar archive or STDIN			# 从 tar 包中加载一个镜像[对应 sava ]</span><br><span class="line">  login       Log in to a Docker registry						# 注册或者登录一个 docker 源服务器</span><br><span class="line">  logout      Log out from a Docker registry					# 退出登录</span><br><span class="line">  logs        Fetch the logs of a container						# 输出容器的日志</span><br><span class="line">  pause       Pause all processes within one or more containers	# 暂停容器</span><br><span class="line">  port        List port mappings or a specific mapping for the container	# 查看映射端口对应的容器内部源端口</span><br><span class="line">  ps          List containers									# 列出容器列表</span><br><span class="line">  pull        Pull an image or a repository from a registry		# 从 docker 镜像源服务器拉取指定镜像</span><br><span class="line">  push        Push an image or a repository to a registry		# 推送指定镜像至 docker 镜像源服务器</span><br><span class="line">  rename      Rename a container								# 重命名容器</span><br><span class="line">  restart     Restart one or more containers					# 重构其容器</span><br><span class="line">  rm          Remove one or more containers						# 删除一个或多个容器</span><br><span class="line">  rmi         Remove one or more images							# 删除一个或多个镜像[无容器使用该镜像时才能删除,否则需要删除相关容器才能继续,或者 -f 强制删除]</span><br><span class="line">  run         Run a command in a new container					# 创建一个新的容器并运行命令</span><br><span class="line">  save        Save one or more images to a tar archive (streamed to STDOUT by default)	# 保存镜像为 tar 包[对应 load ]</span><br><span class="line">  search      Search the Docker Hub for images					# 在 docker hub 中搜索镜像</span><br><span class="line">  start       Start one or more stopped containers				# 启动容器</span><br><span class="line">  stats       Display a live stream of container(s) resource usage statistics	# 展示容器的实时资源占用情况</span><br><span class="line">  stop        Stop one or more running containers				# 停止容器</span><br><span class="line">  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE	# 给源中镜像打标签</span><br><span class="line">  top         Display the running processes of a container		# 查看容器中运行的进程信息</span><br><span class="line">  unpause     Unpause all processes within one or more containers	# 取消暂停容器</span><br><span class="line">  update      Update configuration of one or more containers	# 更新容器的配置</span><br><span class="line">  version     Show the Docker version information				# 查看 docker 版本号</span><br><span class="line">  wait        Block until one or more containers stop, then print their exit codes	# 截取容器停止时的退出状态值</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>docker命令很多,上面都是最常用的.</p>
<h2 id="作业练习"><a href="#作业练习" class="headerlink" title="作业练习"></a>作业练习</h2><h3 id="Docker-安装-Nginx"><a href="#Docker-安装-Nginx" class="headerlink" title="Docker 安装 Nginx"></a>Docker 安装 Nginx</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1. 搜索镜像 search ,建议到 docker hub 上搜索查看详细文档</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 下载镜像 pull</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. 运行测试</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -d 后台运行</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --name 给容器命名</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -p 宿主机(linux服务器)端口,容器(docker)内部端口</span></span><br><span class="line">xxx@data:~$ sudo docker run -d --name nginx01 -p 3344:80 nginx</span><br><span class="line">659cd3e2b3825a85c7e4b02ad2fe52be267fa2fd5425391162b859fec5f3c9c4</span><br><span class="line">xxx@data:~$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE             COMMAND             CREATED             STATUS             PORTS             NAMES</span><br><span class="line">659cd3e2b382        nginx             &quot;/docker-entrypoint.…&quot;             33 seconds ago             Up 31 seconds             0.0.0.0:3344-&gt;80/tcp             nginx01</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试访问成功</span></span><br><span class="line">xxx@data:~$ curl localhost:3344</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">.......</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入容器</span></span><br><span class="line">xxx@data:~$ sudo docker exec -it nginx01 /bin/bash</span><br><span class="line">root@659cd3e2b382:/# whereis nginx</span><br><span class="line">nginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx</span><br><span class="line">root@659cd3e2b382:/# cd /etc/nginx</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>端口暴露的概念</strong></p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404172634182.png" alt="image-20220404172634182" style="zoom:50%;" />





<p>思考问题: 我们每次改动nginx配置文件,都需要进入容器内部 ? 十分麻烦,要是可以在容器外部提供一个映射路径,达到在容器外部修改文件,容器内部就可以自动修改?   <strong>-v 数据卷</strong></p>
<h3 id="Docker-安装-tomcat"><a href="#Docker-安装-tomcat" class="headerlink" title="Docker 安装 tomcat"></a>Docker 安装 tomcat</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 官方的使用</span></span><br><span class="line">docker run -it --rm tomcat:9.0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 我们之前的使用都是 -d 后台,停止容器后,容器还是可以查到.--rm 表示用完就删.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 所以 docker run -it --rm tomcat:9.0 一般用来测试,用完即删</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载再启动</span></span><br><span class="line">docker pull tomcat:9.0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动运行</span></span><br><span class="line">docker run -d -p 3344:8080 --name tomcat01 tomcat</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试访问没有问题</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入容器</span></span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker exec -it tomcat01 /bin/bash</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# ls</span><br><span class="line">BUILDING.txt	 LICENSE  README.md	 RUNNING.txt  conf  logs	    temp     webapps.dist</span><br><span class="line">CONTRIBUTING.md  NOTICE   RELEASE-NOTES  bin	      lib   native-jni-lib  webapps  work</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# ls -al</span><br><span class="line">total 176</span><br><span class="line">drwxr-xr-x 1 root root  4096 Apr  1 19:34 .</span><br><span class="line">drwxr-xr-x 1 root root  4096 Mar 30 05:14 ..</span><br><span class="line">-rw-r--r-- 1 root root 19004 Mar 31 14:24 BUILDING.txt</span><br><span class="line">-rw-r--r-- 1 root root  6210 Mar 31 14:24 CONTRIBUTING.md</span><br><span class="line">-rw-r--r-- 1 root root 60269 Mar 31 14:24 LICENSE</span><br><span class="line">-rw-r--r-- 1 root root  2333 Mar 31 14:24 NOTICE</span><br><span class="line">-rw-r--r-- 1 root root  3378 Mar 31 14:24 README.md</span><br><span class="line">-rw-r--r-- 1 root root  6905 Mar 31 14:24 RELEASE-NOTES</span><br><span class="line">-rw-r--r-- 1 root root 16507 Mar 31 14:24 RUNNING.txt</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 bin</span><br><span class="line">drwxr-xr-x 1 root root  4096 Apr  4 09:43 conf</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 lib</span><br><span class="line">drwxrwxrwx 1 root root  4096 Apr  4 09:43 logs</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 native-jni-lib</span><br><span class="line">drwxrwxrwx 2 root root  4096 Apr  1 19:34 temp</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 webapps</span><br><span class="line">drwxr-xr-x 7 root root  4096 Mar 31 14:24 webapps.dist</span><br><span class="line">drwxrwxrwx 2 root root  4096 Mar 31 14:24 work</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# cd webapps</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat/webapps# ls</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat/webapps# </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 发现问题 1.linux命令少了  2. 404 not found --&gt;没有webapps    是镜像的原因,默认是最小的镜像,所有不必要的都剔除了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 保证最小可运行环境</span></span><br></pre></td></tr></table></figure>



<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404174410059.png" alt="image-20220404174410059" style="zoom:50%;" />



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 更改配置再访问</span></span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# cp -r webapps.dist/* webapps</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# cd webapps</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat/webapps# ls</span><br><span class="line">ROOT  docs  examples  host-manager  manager</span><br></pre></td></tr></table></figure>

<p>404 变成了完整页面, 全程都是在docker里修改,同样可以部署</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404175010930.png" alt="image-20220404175010930" style="zoom:50%;" />



<p>思考问题: 我们以后要部署项目,如果每次都要进入容器是不是十分麻烦?要是可以在容器外部提供一个映射路径,webapps,我们在外部放置项目,就自动同步到内部就好了!</p>
<p>现在 docker 容器: tomcat + 网站内容 ; 如果将来 docker 容器放 mysql + 数据库数据 , 再如果把容器删了 , 就相当于删库跑路了…. 非常不科学 . (期待 -v 数据卷 如何解决)</p>
<h3 id="Docker-部署-es-kibana-没有实际部署"><a href="#Docker-部署-es-kibana-没有实际部署" class="headerlink" title="Docker 部署 es + kibana(没有实际部署)"></a>Docker 部署 es + kibana(没有实际部署)</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> es 暴露的端口很多</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> es 十分耗内存</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> es 的数据一般需要放置到安全目录! 挂载</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --net somenetwork ? 网络配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载 启动</span></span><br><span class="line">docker run -d --name elasticsearch01 --net somenetwork -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch:7.6.2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动了 Linux就卡住了   因为非常占内存</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker stats 查看cpu的状态</span></span><br></pre></td></tr></table></figure>



<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404181625831.png" alt="image-20220404181625831" style="zoom:67%;" />



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 赶紧关闭,增加内存限制,修改配置文件 -e 环境配置修改 限制最小最大占用内存</span></span><br><span class="line">docker run -d --name elasticsearch02 --net somenetwork -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot; elasticsearch:7.6.2</span><br><span class="line"><span class="meta">#</span><span class="bash"> docker stats 查看cpu的状态</span></span><br></pre></td></tr></table></figure>



<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404181645136.png" alt="image-20220404181645136" style="zoom: 67%;" />



<p>使用 kibana 连接 es ? 思考网络如何连接.</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404182053887.png" alt="image-20220404182053887" style="zoom:50%;" />

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以上是看 b 站教学视频记的笔记。</p>
<p>教程地址：<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=9">【狂神说Java】Docker最新超详细版教程通俗易懂_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker镜像原理</title>
    <url>/2022/04/05/Docker%E9%95%9C%E5%83%8F%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404192342184.png" alt="image-20220404192342184" style="zoom:67%;" />

<h2 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h2><p>镜像：一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，包含运行某个软件所需的所有内容，包括代码、库、环境变量和配置文件。</p>
<p>每个文件叠加过后就是我们的应用，虽然是叠加而来，但是对外却是一个整体的系统文件</p>
<h2 id="镜像加载原理"><a href="#镜像加载原理" class="headerlink" title="镜像加载原理"></a>镜像加载原理</h2><blockquote>
<p>UFS文件系统</p>
</blockquote>
<p>下载时看到一层层的就是这个。</p>
<p><strong>联合文件系统（UnionFS）</strong>是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。另外，不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的改动层，大大提高了存储的效率。</p>
<p><strong>特性</strong>：一次性同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。</p>
<blockquote>
<p>Docker镜像加载原理</p>
</blockquote>
<span id="more"></span>

<ul>
<li>bootfs(boot file system) 主要包含bootloader和kernel, bootloader 主要是 **引导加载 **kernel,当我们加载镜像的时候，会通过bootloader加载kernal，Docker镜像最底层是bootfs，当boot加载完成后整个kernal内核都在内存中了，bootfs也就可以卸载，值得注意的是，bootfs是被所有镜像共用的，许多镜像images都是在base image(rootfs)基础上叠加的 .</li>
</ul>
<ul>
<li>rootfs (root file system)，在bootfs之 上.包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是<strong>各种不同的操作系统发行版</strong>，比如Ubuntu, Centos等等 。所以说每个 docker 就是一个小的虚拟机环境。</li>
</ul>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404192342184.png" alt="image-20220404192342184" style="zoom:67%;" />

<p>Linux 不同发行版之间，bootfs 都是一样的，只有 rootfs 不一样 。 所以当 Docker 镜像加载时， bootfs 直接使用宿主机的内核，只需要提供 rootfs ，这部分十分精简，所以 centos 镜像可以很小（才不到300M），并且加载速度很快（虚拟机启动分钟级，容器启动秒级）。</p>
<h2 id="分层理解"><a href="#分层理解" class="headerlink" title="分层理解"></a>分层理解</h2><blockquote>
<p> 镜像分层</p>
</blockquote>
<p>用 <code>docker inspect</code> 观察下载下来的镜像：</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404194829940.png" alt="image-20220404194829940" style="zoom: 50%;" />

<p>这里的 Layers 就是每一层的文件，UFS叠加以后成为整个镜像。</p>
<blockquote>
<p>特性</p>
</blockquote>
<p>以 tomcat 镜像为例，它是一个有6个层级的镜像，pull到本地，再创建一个新的容器，此时整个 tomcat 会作为一整个镜像层，而你做的所有操作都会记录在容器层。如果想保存新的镜像，镜像层和容器层会再次打包，形成一个新的镜像。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404194637448.png" alt="image-20220404194637448"></p>
<h2 id="commit镜像"><a href="#commit镜像" class="headerlink" title="commit镜像"></a>commit镜像</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker commit 提交热熔器成为一个新的副本</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 命令和git类似</span></span><br><span class="line">docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器id 目标镜像名：[TAG]</span><br></pre></td></tr></table></figure>

<p>测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.启动一个默认的tomcat</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.发现这个默认的tomcat 是没有webapps应用，这是镜像的原因，官方的镜像默认 webapps下面是没有文件的</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.自己拷贝了一些基本文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.将修改过的容器通过commit提交为一个镜像！我们以后就是用修改过的镜像就可以，这就是我们自己的一个修改过的镜像</span></span><br></pre></td></tr></table></figure>



<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183351927.png" alt="image-20220405183351927" style="zoom:80%;" />

<p>入门Docker！接下来，容器数据卷！DokcerFile！Docker网络！</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以上是看 b 站教学视频记的笔记。</p>
<p>教程地址：<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=9">【狂神说Java】Docker最新超详细版教程通俗易懂_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《GAG：Global Attributed Graph Neural Network for Streaming Session-based Recommendation》</title>
    <url>/2022/03/13/GAG/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220309191028621.png" alt="image-20220309191028621"></p>
<hr>
<p>原paper：<a href="https://doi.org/10.1145/3397271.3401109"><em>GAG: Global Attributed Graph Neural Network for Streaming Session-Based Recommendation</em></a></p>
<p>源码解读：（近期发布）</p>
<hr>
<p>中译：基于流会话推荐的全局属性图神经网络</p>
<p>总结：将SSRM的encoder部分换成了图神经网络模型，并且沿用了NARM、SRGNN等采用的注意力机制，将用户信息作为全局信息融入GNN模型中，解决了保存用户长期兴趣的问题；改进了reservior的采样策略：计算推荐结果和真实交互的Wasserstein距离作为信息量指标，从而计算采样概率，改进采样策略。</p>
<p>展望：如何引入跨会话信息到SSR问题中，十分值得研究。</p>
<hr>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>question作者想解决什么问题？ </p>
<p>1）SR任务中，用户信息常常被忽略，所以难以抓住用户的长期兴趣。SSR任务中，流数据常常是单个交互而不是会话数据。</p>
<p>2）如何设计适合SSR的通用的reservoir。</p>
</li>
<li><p>method作者通过什么理论/模型来解决这个问题？</p>
<p>针对1），作者提出 <strong>G</strong>lobal <strong>A</strong>ttributed <strong>G</strong>raph （GAG）neural network，全局属性的图神经网络。每当新数据到达时，GAG可以同时考虑全局属性和当前场景，以获得会话和用户的更全面的表示。</p>
<p>针对2）作者提出了 Wasserstein 存储库，帮助保存历史数据的<strong>代表性</strong>画像。</p>
</li>
<li><p>answer作者给出的答案是什么？</p>
<p>GAG + Wasserstein reservoir，取得了SOTA。</p>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>why作者为什么研究这个课题？    </p>
<p>会话推荐的发展现状：大部分会话推荐模型都专注与静态场景，“流会话”(Streaming session)的设定更贴近实际，但却很少研究。由于用户的喜好在随时间变化，所以对新数据做预测，仍然使用在原来数据上训练的静态模型是不合理的，为了更准确地捕捉用户兴趣，模型应该用最新的数据在线更新。</p>
<p>流推荐的发展现状：一些方法利用了存储技术解决流任务，但是它们的缺点是，交互数据都已相同概率存储到存储库中，这是一种离散的方式存储会话，有信息损失，捕捉不了连续的会话序列模式。还有一些在线学习的方法，每当新数据到来，模型就相应地更新，这会导致模型对新数据过拟合并且无法有效保留用户的长期兴趣。</p>
<p>只有SSRM提出了一个结合两者的解决方案，但有不足之处。</p>
</li>
<li><p>how当前研究到了哪一阶段？</p>
<p>SSRM。SSRM有两方面不足：1）计算信息量时，需要预先获得所有物品的隐含表示（MF方法得到的），别的方法不适用。2）模型将会话推荐的方法（GRU4Rec）和矩阵分解（MF）直接结合，这里原文是用MF计算的权重，对GRU4Rec的隐藏状态加权求和。很难学到用户和物品间更复杂的关联。</p>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 </li>
</ul>
<p>LastFM：<a href="http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz">http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz</a></p>
<p>Gowalla：<a href="https://snap.stanford.edu/data/loc-gowalla.html">https://snap.stanford.edu/data/loc-gowalla.html</a></p>
<ul>
<li>数据划分</li>
</ul>
<p>给数据集 $D$ 中的会话按时间排序，分成前60%作为训练集，和后40%作为候选集。为了模拟线上的流数据输入，将候选集再划分成5个等长切片作为测试机。第一个测试机和10%的训练集作为验证集。实验中，若要预测第 $i$ 个测试集的序列行为，那么 $i$ 之前的测试集切片都用作在线训练。</p>
<ul>
<li>重要指标 </li>
</ul>
<p>MRR@20、Recall@20</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>会话推荐Session-based Recommendation<ul>
<li>属于序列推荐</li>
</ul>
</li>
<li>流推荐Streaming Recommendation<ul>
<li>属于在线学习Online Learning，大多数在线学习更关注新数据，往往不能记忆历史交互</li>
<li>随机采样Random Sampling 方法是为了解决 “历史遗忘“问题而提出的，它通过引入一个储存库来保存用户的长期交互。</li>
</ul>
</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>Item embdding： $ x_i =Embed_v(v_i)$ ；User embdding： $ p_j =Embed_u(u_j)$  ；</p>
<p>User u 在时间步 t 的会话序列：$ S_{u,t}=[v_1,v_2,…,v_l]$ </p>
<p>任务目标，根据用户 $u$ 的历史会话 ${S_{u,0},S_{u,1},…,S_{u,t}}$ 预测下一个可能交互的物品 $ v_{t+1}$ </p>
<p>与会话推荐不同的是，这里假设所有会话 $S$ 都以很快的速度达到，所以受限于算力，必须选取高效的方式处理历史会话信息和当前会话信息。而会话推荐，没有流数据这一设定，可以同时处理用户所有序列，不必考虑效率。</p>
<h3 id="GAG模型框架"><a href="#GAG模型框架" class="headerlink" title="GAG模型框架"></a>GAG模型框架</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220309205703438.png" alt="image-20220309205703438"></p>
<p>GAG的主要工作由两部分构成，1）GAG model：将用户信息转化为全局属性并将其融入到会话图中；2）Wasserstein reservoir存储库策略用来学习流数据。</p>
<h3 id="Global-Attributed-Graph-GAG"><a href="#Global-Attributed-Graph-GAG" class="headerlink" title="Global Attributed Graph (GAG)"></a>Global Attributed Graph (GAG)</h3><h4 id="全局属性的会话图"><a href="#全局属性的会话图" class="headerlink" title="全局属性的会话图"></a>全局属性的会话图</h4><p>建图方式和SRGNN一样，建成有向图，不同的是加入了 <strong>全局属性（用户属性）</strong> $u$  变成三元组 $G_s = (u,V_s,E_s)$ ，图的边定义为：$E_s=(w_{s,(n-1)n},v_{n-1},v_n)$ ，也是三元组， $w$ 是权重，和SRGNN计算方式一样，基于该边出现的频率。</p>
<h4 id="全局属性的图神经网络"><a href="#全局属性的图神经网络" class="headerlink" title="全局属性的图神经网络"></a>全局属性的图神经网络</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220312132405693.png" alt="image-20220312132405693"></p>
<p>会话图作为输入进到GAG模型，模型的计算从边、节点到全局属性。</p>
<ol>
<li>逐边更新 per-edge update</li>
</ol>
<p>边特征在这里指的是边的权值，是固定值，不是dense vector，不会更新。所以边信息只用来更新节点特征和全局特征。因为是有向图，所以对于一条边来说，需要双向更新，一个节点既作为sender，也作为receiver。更新公式：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>e_{k,in}’ &amp;= \phi ^e_{in}(e_k,v_{r_k},v_{s_k},u)<br>\&amp;= w_k \cdot MLP(v_{s_k}||u),<br>\<br>e_{k,out}’ &amp;= \phi ^e_{out}(e_k,v_{r_k},v_{s_k},u)<br>\&amp;= w_k \cdot MLP(v_{s_k}||u),<br>\end{aligned}<br>\end{equation}<br>$$<br>其中两个MLP是不共享权重的，因为含义不同，一个是计算sender方向的特征，一个是计算receiver方向的特征。</p>
<ol start="2">
<li>逐点更新 per-node update</li>
</ol>
<p>逐点更新是基于逐边更新的结果的。逐点更新的结果是包含所有入or出的邻居信息的标准化后的加和。以第一个公式为例， $s_j$ 是所有指向 $r_i$（也即 $i$ ） 的邻居。节点 $i$ 的<strong>节点</strong>的 in-coming feature 是所有指向 $i$ 的<strong>边的</strong>标准化后的 out-going feature 的加和。 标准化是将 $j$ 指向 $i$ 的这条边的 out-going feature 除以 $\sqrt{i的入度 \cdot j的出度}$ ，可以看出， $j$ 的出度越大（从 $j$ 发出的边越多），$i$ 的入度越大（指向 $i$ 的边越多），都会导致 $i$ 来自节点 $j$ 的 in-coming feature 值越小。这是符合直觉的。<br>$$<br>\begin{equation}<br>\begin{aligned}<br>v_{i,in}’ =\sum_{j \in { v_{s_j} = v_{r_i}}} \frac{e_{j,out’}}{\sqrt{N_{in}(i)N_{out}(j)}}<br>\<br>v_{i,out}’ =\sum_{j \in { v_{r_j} = v_{s_i}}} \frac{e_{j,in’}}{\sqrt{N_{out}(i)N_{in}(j)}} </p>
<p>\end{aligned}<br>\end{equation}<br>$$</p>
<ol start="3">
<li>节点的最终表示</li>
</ol>
<p>最后节点 $i$ 表示融合了自己的 in-coming feature 和 out-going feature，节点最后的表示实际上包含了 1）自身的节点信息；2）邻居信息（通过边传播）；3）连接的边的权重；4）全局属性：<br>$$<br>v_i’= MLP (v_{i,in}’ || v_{i,out}’)<br>$$</p>
<ol start="4">
<li>会话表示</li>
</ol>
<p>与NARM、STAMP、SRGNN工作类似，也用序列中的最后一个节点对其它节点做 self-attention 。有的工作用内积计算最后一个节点与其他节点的相似度作为权重，如NARM；也有用MLP获得权重的，如STAMP、SRGNN。这里用MLP。<br>$$<br>\begin{equation}<br>\begin{aligned}<br>\<br>u’ &amp;= \phi ^ u (V’,u)<br>\<br>&amp;= Self-Atten(v_l’,v_i’,u) + u</p>
<p>\end{aligned}<br>\end{equation}<br>$$<br>Self-Atten 分为以下两个部分：<br>$$<br>\begin{equation}<br>\begin{aligned}</p>
<p>\alpha <em>i &amp;= MLP(v_l’||v_i’||u)<br>\<br>u</em>{s,g} &amp;= \sum ^n _{i=1} \alpha_i v_i’<br>\end{aligned}<br>\end{equation}<br>$$</p>
<p>这样得到的 $u_{s,g}$ 可以看作short-term兴趣增强的会话表示，再融合全局属性 $u$ ，得到长短期兴趣结合的会话表示： $u’ = u_{s,g} + u$ 。这种residual connection残差连接的方式，还可以减轻直接学习全局属性 $u$ 的负担。</p>
<h3 id="推荐-预测"><a href="#推荐-预测" class="headerlink" title="推荐/预测"></a>推荐/预测</h3><p>Session embedding 和 item embedding 做内积，再经过softmax得到概率分布： $\hat y = Softmax(u’^T X)$ 。</p>
<h3 id="Wasserstein-Reservoir"><a href="#Wasserstein-Reservoir" class="headerlink" title="Wasserstein Reservoir"></a>Wasserstein Reservoir</h3><p>将离线模型拓展到流设定下，提出Wasserstein reservoir方法。提出该方法的目标是：用新来的数据更新模型，并且保持从历史交互中学到的知识。</p>
<p>传统的在线学习方法通常只用新数据更新模型，所以导致模型会忘记过去的知识。为了避免这一点，本文利用reservoir来保持对历史数据的长期记忆，reservoir技术在流数据库管理系统中非常常见。</p>
<p>如何选择reservoir中的数据？之前的方法是：使用随机采样方法。每个新数据都以 $\frac{|C|}{t}$ 的概率随机替换掉已经在 $C$ 中的数据。这种方法被证明是从当前数据集中随机采样，并且可以保持模型的long-term memory。</p>
<p>作者认为，使用以上的随机采样方法得到 $C$ ，把它当作训练数据来训练模型的方式不好，原因如下：随机采样难以更关心新数据（time descent probability），但是最近的数据又是非常重要的。所以应该用新来的数据和reservoir $C$ 中的老数据一起更新预训练的模型，而不光光是reservoir $C$ 中的老数据。</p>
<p>但是，即便用新老数据一起更新模型，由于随机采样策略没变，训练数据中的大部分数据都是long-term数据，模型早就学得很好了，所以用它来训练对模型更新帮助不大。</p>
<p>如果当前模型在最新会话上预测结果不好，可能意味着用户兴趣转移or当前模型无法捕捉一些转换模式。这样的数据称之为”有信息量的数据“，对模型更新意义更大。</p>
<p>在本文中，一个会话的信息量被定义为模型预测的分布 $\hat y$ 和真实交互 $y$ 的距离。下面是三种计算距离的算法：</p>
<ol>
<li>Wasserstein 距离（EMD 距离）：</li>
</ol>
<p>$$<br>d_{W}\left(\mathbb{P}<em>{r}, \mathbb{P}</em>{g}\right)=\inf <em>{\gamma \in \Pi\left(\mathbb{P}</em>{r}, \mathbb{P}<em>{g}\right)} \mathbb{E}</em>{(x, y) \sim \gamma}[|x-y|]<br>$$</p>
<ol start="2">
<li>Kullback-Leibler（KL）散度：</li>
</ol>
<p>$$<br>d_{K L}\left(\mathbb{P}<em>{r} | \mathbb{P}</em>{g}\right)=\sum_{i=1}^{n} P_{r}(x) \log \frac{P_{r}(x)}{P_{g}(x)}<br>$$</p>
<ol start="3">
<li>Total Variation（全变分）距离：</li>
</ol>
<p>$$<br>d_{T V}\left(\mathbb{P}<em>{r}, \mathbb{P}</em>{g}\right)=\sup <em>{A \in \Sigma}\left|\mathbb{P}</em>{r}(A)-\mathbb{P}_{g}(A)\right|<br>$$</p>
<p>在推荐任务中，真实分布是one-hot向量，只有真实标签处为1。</p>
<p>KL散度，也称交叉熵、相对熵。不选择KL散度的原因是：在推荐任务中，公式简化为：$d_{K L}(\mathbf{y} | \hat{\mathbf{y}})=-\log P_{g}\left(v_{i}\right)$ ，实际上只衡量了真实标签处的差异，没有考虑整个分布之间的差异。而且KL散度本身就是非对称性函数：$D(p | q) \neq D(q | p)$ ，用它作为一个真正的距离度量可能不是很合适。</p>
<p>不选择全变分距离的原因是：在推荐任务中，公式简化为： $d_{T V}(\mathbf{y}, \hat{\mathbf{y}})=\max <em>{j \neq i}\left(1-P</em>{g}\left(v_{i}\right), P_{g}\left(v_{j}\right)\right)$  ，这个结果要么只衡量了真实标签以外的差异，要么只衡量了真实标签。</p>
<h4 id="在线训练算法描述"><a href="#在线训练算法描述" class="headerlink" title="在线训练算法描述"></a>在线训练算法描述</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220313210103872.png" alt="image-20220313210103872"></p>
<p>$S$ 是用来更新模型的，与 $C$ 无关。</p>
<p>因为流数据中会有新用户和新物品出现，为了防止模型忽略这些新的会话，它们会被直接加入 $S$ 当作训练数据。</p>
<p>$C \cup C^{n e w} - S$ 即其余的会话数据，分别计算它们的Wasserstein距离，再根据以下公式计算各自的采样概率：<br>$$<br>p_{\text {sample }}\left(s_{i}\right)=\frac{d_{i}}{\sum_{s_{j} \in C \cup C^{n e w}-S} d_{j}},<br>$$<br>采样完以后就得到训练数据 $S$ ，这个 $S$ 是对当前模型来说信息量最大的数据集，用它来更新模型最有效。</p>
<p>最后还要更新 reservoir $C$ ，用随机采样算法来更新，以保持模型的 long-term 记忆。</p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>$ Cross \ Entropy \ Loss=-\sum_{i=1}^{l} \mathrm{y}<em>{i} \log \left(\hat{\mathrm{y}}</em>{i}\right) $</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><h3 id="对比实验结果"><a href="#对比实验结果" class="headerlink" title="对比实验结果"></a>对比实验结果</h3><p>S-POP居然比GRU4Rec结果好，可能因为S-POP能抽取出会话间的信息。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314084551972.png" alt="image-20220314084551972"></p>
<h3 id="细化评价指标的top-k"><a href="#细化评价指标的top-k" class="headerlink" title="细化评价指标的top k"></a>细化评价指标的top k</h3><p>SSRM方法相比于另外三个基于图的方法，效果下降得幅度更大，说明图结构更适合做会话表示任务，也说明图结构有一定的泛化能力。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314085304068.png" alt="image-20220314085304068"></p>
<h3 id="全局属性的影响"><a href="#全局属性的影响" class="headerlink" title="全局属性的影响"></a>全局属性的影响</h3><p>三个消融实验的对比模型如下：</p>
<ul>
<li><p>FGNN：节点更新层和输出层都不加入用户信息。</p>
</li>
<li><p>GAG-FGNN：将节点更新函数换成FGNN的节点更新层，但是保留全局属性更新  $u’ = u_{s,g} + u$ 。</p>
</li>
<li><p>GAG-NoGA：节点更新层不变，但是去掉全局属性更新函数 $u’ = u_{s,g} + u$ 。</p>
</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314091613738.png" alt="image-20220314091613738"></p>
<p>结果如下。GAG-FGNN和GAG-NoGA都在模型中融入了全局信息，前者在用户信息更新部分，后者在GNN的节点更新部分。相比于没有全局信息的FGNN，两者都有提升，说明融入全局信息对推荐是有用的。GAG-NoGA比GAG-FGNN提升更大，说明在节点更新阶段融入全局信息更有效。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314092750543.png" alt="image-20220314092750543"></p>
<h3 id="Wasserstein-Reservoir的影响"><a href="#Wasserstein-Reservoir的影响" class="headerlink" title="Wasserstein Reservoir的影响"></a>Wasserstein Reservoir的影响</h3><p>消融实验的对比模型如下：</p>
<ul>
<li>GAG-Static：直接去掉在线训练部分</li>
<li>GAG-RanUni：从原reservoir和新数据的并集里，随机采样。这也是最普遍的设计。</li>
<li>GAG-FixNew：直接保留新数据，剩下的数据随机采样。</li>
<li>GAG-WassUni：对所有原reservoir和新数据计算Wasserstein距离，然后根据这个距离采样。（原模型是先全部保存所有带有新用户和新物品的会话，再根据Wass距离采样）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314100934020.png" alt="image-20220314100934020"></p>
<p>结果如下：</p>
<p>Static结果最差，因为：1）兴趣漂移；2）新用户、新物品出现。</p>
<p>纯随机采样的GAG-RanUni，在在线模型中结果最差，不如有策略地选择。</p>
<p>GAG-WassUni优于大部分策略，说明采用Wass距离的有效性。</p>
<p>GAG和GAG-WassUni相比，GAG结果更好，说明保留新用户和新物品的重要性。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314100603124.png" alt="image-20220314100603124"></p>
<h3 id="Reservoir-效率分析"><a href="#Reservoir-效率分析" class="headerlink" title="Reservoir 效率分析"></a>Reservoir 效率分析</h3><p>有两个参数reservoir的设计影响很大：reservoir大小（ $C$ ）和窗口大小（ $S$ ）。一方面，reservoir的大小表示reservoir的容量，这决定了推荐系统在线更新的存储要求。另一方面，窗口大小限制了多少数据实例将被抽样用于在线训练，这代表了推荐系统在线更新的工作负荷。</p>
<h4 id="Reservoir-size-的影响"><a href="#Reservoir-size-的影响" class="headerlink" title="Reservoir size 的影响"></a>Reservoir size 的影响</h4><p>Reservoir容量越大，新数据保存的概率就越低，模型就更注重历史数据。但是在流设定下，新数据更能代表用户最近的兴趣。SOTA模型SSRM在$\frac{|D|}{20}$ 时表现最好，而GAG是 $\frac{|D|}{100}$ ，所以GAG效率更高。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314103059323.png" alt="image-20220314103059323"></p>
<h4 id="Window-size-的影响"><a href="#Window-size-的影响" class="headerlink" title="Window size 的影响"></a>Window size 的影响</h4><p>很明显，窗口大小越大，模型表现越好。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314104151741.png" alt="image-20220314104151741"></p>
<h3 id="超参数的影响"><a href="#超参数的影响" class="headerlink" title="超参数的影响"></a>超参数的影响</h3><h4 id="Embedding-size影响"><a href="#Embedding-size影响" class="headerlink" title="Embedding size影响"></a>Embedding size影响</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314104403575.png" alt="image-20220314104403575"></p>
<h4 id="GNN-layer层数影响"><a href="#GNN-layer层数影响" class="headerlink" title="GNN layer层数影响"></a>GNN layer层数影响</h4><p>一般来说，由于梯度爆炸，GNN模型总是受到模型深度增加的影响。在我们的实验中，GAG模型的性能随着GNN 层数增加而下降，这与常见的观察是一致的。此外，会话的连通性比传统的图数据要小，这也限制了更深的GNN模型的能力。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314104558903.png" alt="image-20220314104558903"></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>流会话推荐</tag>
        <tag>GAG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Graph-Enhanced Multi-Task Learning of Multi-Level Transition Dynamics for Session-based Recommendation》</title>
    <url>/2021/10/20/MTD/</url>
    <content><![CDATA[<hr>
<p>原paper：<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16534">https://ojs.aaai.org/index.php/AAAI/article/view/16534</a></p>
<p>开源代码：<a href="https://github.com/sessionRec/MTD">https://github.com/sessionRec/MTD</a></p>
<hr>
<h3 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h3><p>大多数现有的基于会话的推荐技术并没有很好地设计来捕捉复杂的转换动态(complex transition dynamics)，这些动态表现为时间有序和多层次相互依赖的关系结构。</p>
<p>complex transition dynamics 的”complex”体现在：multi-level relation(intra- and inter-session item relation) . 会话内：short-term and long-term item transition，会话间：long-range cross-session dependencies。复杂依赖的例子见Figure 1.</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/MTD-1.png" alt="MTD-1"></p>
<span id="more"></span>

<h3 id="主要贡献："><a href="#主要贡献：" class="headerlink" title="主要贡献："></a>主要贡献：</h3><p>1.开发了一种新的会话推荐框架，可以捕获会话内和会话间的物品转换模式（多层次转换动态）</p>
<p>2.开发了一种位置感知的注意力机，用于学习会话内的序列行为和session-specific knowledge。此外，在图神经网络范例的基础上，建立了全局上下文增强的会话间关系编码器，赋予MTD来捕获会话间项目依赖关系。</p>
<p>3.在三个数据集上取得了SOTA，Yoochoose、Diginetica、Retailrocket。</p>
<h3 id="网络图："><a href="#网络图：" class="headerlink" title="网络图："></a>网络图：</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/MTD-2.png" alt="MTD-2"></p>
<h3 id="方法论："><a href="#方法论：" class="headerlink" title="方法论："></a>方法论：</h3><h4 id="1-学习会话内的物品关系"><a href="#1-学习会话内的物品关系" class="headerlink" title="1.学习会话内的物品关系"></a>1.学习会话内的物品关系</h4><p>1）Self-attention层：</p>
<p>QKV self-attention + feed forward network</p>
<p>2）位置感知的物品级的注意力聚合模块：<br>$$<br>\alpha _ i = \delta(\mathbf{g}^T \cdot \sigma(\mathbf{W} _ 3\cdot\mathbf{x} _ {s,I} + \mathbf{W} _ 4\cdot\mathbf{x} _ {s,i}))<br>$$</p>
<p>$$<br>\mathbf{x}^* _ s = \sum_{i=1}^I\alpha _ i\cdot\mathbf{x _ \mathit{s,i}}<br>$$</p>
<p>$$<br>\mathbf{q} _ s = \mathbf{W} _ c[\mathbf{x} _ {s,I},\mathbf{x}^* _ s,\mathbf{p} _ s]<br>$$</p>
<p>$\mathbf{x}_{s,I}$表示最后一次点击，$\mathbf{x^*_s}$表示聚合后的会话表示，$\mathbf{p}_s$表示加入物品相对位置信息的会话表示，$\mathbf{q}_s $是最后的会话表示。</p>
<p>3）loss = $\mathit{L_in}$</p>
<h4 id="2-对全局转换动态建模"><a href="#2-对全局转换动态建模" class="headerlink" title="2.对全局转换动态建模"></a>2.对全局转换动态建模</h4><p>1）用图神经网络结构和GCN对inter-session的依赖建模</p>
<p>2）用<strong>互信息学习</strong>来增强跨会话的建模物品间关系的encoder</p>
<p>3）loss = $\mathit{L_cr}$</p>
<h4 id="3-Model-Inference"><a href="#3-Model-Inference" class="headerlink" title="3.Model Inference"></a>3.Model Inference</h4><p>定义loss：<br>$$<br>\mathit{L}=\mathit{L} _ {cr} + \lambda_1\mathit{L} _ {in}+\lambda_2||\Theta||^2 _ 2<br>$$</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch Geometric 学习笔记</title>
    <url>/2021/11/14/Pytorch%20Geometric/</url>
    <content><![CDATA[<hr>
<p>官网永远是最好的学习资料：<a href="https://pytorch-geometric.readthedocs.io/en/latest/">https://pytorch-geometric.readthedocs.io/en/latest/</a></p>
<p>跟着配套colaboratory的教程走，大概一天能学完五个教程，学完也算基本入门pytroch-geometric了。</p>
<hr>
<p><a href="https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=gUFSrDPxuQ23">1. Introduction.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li>This concludes the first introduction into the world of GNNs and PyTorch Geometric. In the follow-up sessions, you will learn how to achieve state-of-the-art classification results on a number of real-world graph datasets.</li>
<li>概要：介绍图的基本结构，GCN怎么用。</li>
</ul>
<p><a href="https://colab.research.google.com/drive/14OvFnAXggxB8vM4e8vSURUp1TaKnovzX">2. Node Classification.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li>In this chapter, you have seen how to apply GNNs to real-world problems, and, in particular, how they can effectively be used for boosting a model’s performance. In the next section, we will look into how GNNs can be used for the task of graph classification.</li>
<li>概要：用GNN实现某些真实的节点分类任务，与MLP效果更好。</li>
</ul>
<span id="more"></span>

<p><a href="https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=CN3sRVuaQ88l">3. Graph Classification.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li><p>In this chapter, you have learned how to apply GNNs to the task of graph classification. You have learned how graphs can be batched together for better GPU utilization, and how to apply readout layers for obtaining graph embeddings rather than node embeddings.</p>
<p>In the next session, you will learn how you can utilize PyTorch Geometric to let Graph Neural Networks scale to single large graphs.</p>
</li>
<li><p>概要：学习了应用GNN实现图分类。学习了GNN上的mini-batch是如何构造以更好利用GPU。学习了如何用readout层获取图的表示。</p>
<ul>
<li>和图像一样用padding和rescaling让图大小相同太浪费空间，所以用对角矩阵相连的方法处理。在torch里是用稀疏矩阵存储的，所以开销不大。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211118204147577.png" alt="image-20211118204147577"></p>
<ul>
<li>Dataloader和torch里差不多 <code>train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)</code>。</li>
<li><code>DataBatch(edge_index=[2, 2636], x=[1188, 7], edge_attr=[2636, 4], y=[64], batch=[1188], ptr=[65])</code> 里的batch记录的是每个节点在哪个图里，batch = [0,…,0,1,…,1,2,…,2]表示一个batch里有三张图</li>
<li>nn.GraphConv() 有residual connection。</li>
<li>图的表示可以写成所有节点的均值<code>x = global_mean_pool(x, batch)</code> </li>
</ul>
</li>
</ul>
<p><a href="https://colab.research.google.com/drive/1XAjcjRHrSR_ypCk_feIWFbcBKyT4Lirs#scrollTo=SDOmdUe0C3U1">4. Scaling GNNs.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li><p>In this chapter, you have been presented a method to scale GNNs to large graphs, which otherwise would not fit into GPU memory.</p>
<p>This also concludes the hands-on tutorial on <strong>deep graph learning with PyTorch Geometric</strong>. If you want to learn more about GNNs or PyTorch Geometric, feel free to check out <strong><a href="https://pytorch-geometric.readthedocs.io/en/latest/?badge=latest">PyG’s documentation</a></strong>, <strong><a href="https://github.com/rusty1s/pytorch_geometric">its list of implemented methods</a></strong> as well as <strong><a href="https://github.com/rusty1s/pytorch_geometric/tree/master/examples">its provided examples</a></strong>, which cover additional topics such as <strong>link prediction</strong>, <strong>graph attention</strong>, <strong>mesh or point cloud convolutions</strong> and <strong>other methods for scaling up GNNs</strong>.</p>
<p><em>Happy hacking!</em></p>
</li>
<li><p>概要：介绍了了降低显存的方法，cluster-gnn，使得训练超大图成为可能。</p>
</li>
<li><p>不再在整个图上划分mini-batch，先分成sub-graph再分mini-batch，解决了邻居爆炸（。。邻居数量）问题</p>
</li>
<li><p>分太开也不好，所以cluster以后随机对sub-graph再连接</p>
<ul>
<li><code>ClusterData</code> converts a <code>Data</code> object into a dataset of subgraphs containing <code>num_parts</code> partitions.</li>
<li>Given a user-defined <code>batch_size</code>, <code>ClusterLoader</code> implements the stochastic partitioning scheme in order to create mini-batches.</li>
</ul>
</li>
<li><p>这种采样方法，只用改划分数据的代码，训练过程不变。</p>
</li>
</ul>
<p><a href="https://colab.research.google.com/drive/1D45E5bUK3gQ40YpZo65ozs7hg5l-eo_U?usp=sharing#scrollTo=iWRxB3JYFXNF">5. Point Cloud Classification.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li>概要：介绍了点云分类任务的三大步骤。又在PointNet++和PPFNet的实践中，介绍了如何自定义MessagePassing以及采样策略。</li>
<li>PointNet++<ul>
<li><p>Grouping阶段，用knn graph或者半径图</p>
<ul>
<li><p>```python<br>from torch_cluster import knn_graph<br>根据点的坐标计算最近的k个点，连起来</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 邻居聚合阶段。聚合邻居信息</span><br><span class="line"></span><br><span class="line">  - 从</span><br><span class="line"></span><br><span class="line">    ```python</span><br><span class="line">    class PointNetLayer(torch_geometric.nn.MessagePassing)：</span><br><span class="line">    	def __init__(self, in_channels, out_channels):</span><br><span class="line">            pass</span><br><span class="line">        def forward(self, h, pos, edge_index):</span><br><span class="line">            pass</span><br><span class="line">        def message(self, h_j, pos_j, pos_i):</span><br><span class="line">            pass</span><br></pre></td></tr></table></figure>

<p>继承，并定义出一个与<code>GraphConv()</code> 、<code>GCNConv()</code> 同一级别的类，例如一种新的卷积层。</p>
</li>
<li><p>MessagePassing接口通过自动处理消息传播，来帮助我们创建<strong>消息传递图神经网络</strong>。只需要定义 message 功能即可。</p>
</li>
<li><p><code>def message()</code>  定义如何构建一个可学习的message给每条边（每个边对应一个邻居，所以也可以看成定义message给每个邻居），以及传播的规则</p>
</li>
<li><p><code>def forward()</code>  调用propagate()，开始传播</p>
</li>
<li><p>PPFNet，解决旋转不变性</p>
</li>
</ul>
</li>
<li><p>downsampling（下采样）阶段</p>
<ul>
<li><p><strong>Farthest Point Sampling</strong> (FPS) 最远点采样。使得每次采点都和已经采样的点距离最远。这种方式证明比随机采样更能覆盖整个点集。</p>
</li>
<li><p>不同batch中fps是独立的，所以要传入batch向量</p>
<ul>
<li><pre><code class="python">index = fps(pos, batch, ratio=0.5)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[6. GNN Explanation.ipynb - Colaboratory (google.com)](https://colab.research.google.com/drive/1fLJbFPz0yMCQg81DdCP5I8jXw9LoggKO?usp=sharing#scrollTo=F1op-CbyLuN4)</span><br><span class="line"></span><br><span class="line">- 占坑</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line"></span><br><span class="line">建个新环境</span><br><span class="line"></span><br></pre></td></tr></table></figure>
conda create -n pyg python==3.8.0
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">安装torch</span><br><span class="line"></span><br></pre></td></tr></table></figure>
pip install torch==1.10.0
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">安装依赖包</span><br><span class="line"></span><br><span class="line">## 方法1：</span><br><span class="line"></span><br><span class="line">![image-20211115163907936](C:\Users\pc\OneDrive\Typora文档\images\image-20211115163907936.png)</span><br><span class="line"></span><br><span class="line">官网安装比较省事，但是可以看到只有最新的几个版本，如果你的pytorch版本比较旧（旧也是为了稳定...）可以尝试方法2。</span><br><span class="line"></span><br><span class="line">## 方法2：</span><br><span class="line"></span><br><span class="line">https://data.pyg.org/whl/</span><br><span class="line"></span><br><span class="line">根据pytorch版本和cuda版本，在这个网站选择对应版本进入，例如我是torch-1.10和cuda-10.2，所以进入https://data.pyg.org/whl/torch-1.10.0+cu102.html</span><br><span class="line"></span><br><span class="line">然后根据系统类型和python版本，下好安装包，如下</span><br><span class="line"></span><br><span class="line">torch_**scatter**-2.0.9-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">torch_**sparse**-0.6.12-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">torch_**cluster**-1.5.9-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">torch_spline_conv-1.2.1-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">最后用pip离线离线安装</span><br><span class="line"></span><br></pre></td></tr></table></figure>
pip install xxx.whl
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意要切换到安装目录，且按顺序安装 scatter–&gt;sparse–&gt;cluster–&gt;spline</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Recbole避坑手册</title>
    <url>/2021/12/05/Recbole%E9%81%BF%E5%9D%91%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[<p>RecBole是个非常好的开源库，这几天做评测的时候用上了，奈何本人能力有限，遇到了非常多bug（可能是自己行为造成的），简单记录一下。可以参考这个：<a href="https://blog.csdn.net/turinger_2000/category_10624007.html">RecBole小白入门系列_Turinger_2000的博客-CSDN博客</a></p>
<p><strong>使用方法</strong>就是：<a href="https://github.com/RUCAIBox/RecBole">RUCAIBox/RecBole (github.com)</a>，下载下来unzip或者clone到设备上。然后再RecBole主目录下编写一个test.yaml文件记录一些配置，再运行run_recbole.py就可以。test.yaml大概要设置4类东西：dateset setting, model setting, train setting, evaluate setting.</p>
<p>整个项目文件如下，几个比较重要的文件夹和文件标出来了，后面会说到。</p>
<span id="more"></span>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205143321872.png" alt="image-20211205143321872"></p>
<p>接下来以一个<strong>用自己数据集跑SASRec模型</strong>的例子说明如何使用RecBole。</p>
<h2 id="配置test-yaml文件"><a href="#配置test-yaml文件" class="headerlink" title="配置test.yaml文件"></a>配置test.yaml文件</h2><h3 id="构造数据集——data-setting"><a href="#构造数据集——data-setting" class="headerlink" title="构造数据集——data setting"></a>构造数据集——data setting</h3><p>如果想跑自己的实验，那么很重要的一件事就是构造自己的数据集，recbole要求个人首先构建可以处理的原子文件，然后就可以传给模型处理了。详细见：<a href="https://recbole.io/cn/data_flow.html">数据流 | 伯乐 (recbole.io)</a></p>
<p>根据<a href="https://recbole.io/cn/atomic_files.html">原子文件 | 伯乐 (recbole.io)</a>，Sequential模型只需要.inter的原子文件，如下图：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205143831872.png" alt="image-20211205143831872"></p>
<p>虽然不知道.inter 是什么文件，但是可以看以下模型本身给的数据集以及处理好的原子文件模仿着构造。数据集保存在RecBole/dataset/ml-100k下（以ml-100k数据集为例），找到ml-100k.inter，用记事本打开格式如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205144208594.png" alt="image-20211205144208594"></p>
<p>BPR和CF等general model可能会用到rating（用户评分），timestamp。但是Sequential model一般只需要timestamp把点击行为构成对应用户的序列就行，想跑的SASRec论文附的代码里，对数据集的处理是：只保留user_id和item_id两列，按点击顺序存储。我的数据集book长这个样子：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205144708967.png" alt="image-20211205144708967"></p>
<p>只有两列特征user_id和item_id，看起来好像很接近，但是踩坑穿越回来的我可以告诉你，这里必须得有timestamp一列，recbole就是这么处理sequential model的数据集的，没有办法。那添加什么样的timestamp呢？book数据集是按点击顺序存储的（user_id已经重新从0开始标号），所以其实只要加个递增的timestamp就行了，这里用pandas简单处理下多加一列就行。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205145228872.png" alt="image-20211205145228872"></p>
<p>注意，这里用pandas处理的时候，顺便把列名改了。user_id和item_id后面加上”:token”，timestamp后面加上”:float”。RecBole要求这么做，后面也会说到。这样处理好文件后，pandas输出的一般是csv，重命名的时候要改成.inter后缀。然后在dataset下新建一个文件夹，起名为你的dataset名称xxx（可以自己起，这个很重要），然后.inter文件也要命名为xxx.inter，如图所示：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205145659369.png" alt="image-20211205145659369"></p>
<p>然后我们在RecBole主目录下新建一个test.yaml文件，在里面输入：（暂时不明白没事，抄下来就行）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dataset config</span></span><br><span class="line">field_separator: <span class="string">&quot;,&quot;</span>  <span class="comment">#指定数据集field的分隔符</span></span><br><span class="line">seq_separator: <span class="string">&quot; &quot;</span>   <span class="comment">#指定数据集中token_seq或者float_seq域里的分隔符</span></span><br><span class="line">USER_ID_FIELD: user_id <span class="comment">#指定用户id域</span></span><br><span class="line">ITEM_ID_FIELD: item_id <span class="comment">#指定物品id域</span></span><br><span class="line">TIME_FIELD: timestamp  <span class="comment">#指定时间域</span></span><br><span class="line">NEG_PREFIX: neg_   <span class="comment">#指定负采样前缀</span></span><br><span class="line"><span class="comment">#指定从什么文件里读什么列，这里就是从book.inter里面读取user_id, item_id,timestamp这四列</span></span><br><span class="line">load_col:</span><br><span class="line">  inter: [user_id, item_id, timestamp]</span><br></pre></td></tr></table></figure>

<p>需要注意前两条separator，csv文件的话默认分隔符是”,”，还有最后一行local:这里按照数据集的列指定就行，到此数据集基本构造好了。</p>
<h3 id="用Sequential-model类跑模型——model-setting"><a href="#用Sequential-model类跑模型——model-setting" class="headerlink" title="用Sequential model类跑模型——model setting"></a>用Sequential model类跑模型——model setting</h3><p>以SASRec为例，想跑一个模型，如何看这个模型需要的参数？到 RecBole/recbole/properties/model 底下找到对应模型的yaml文件，打开以后大概长这样。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205150832821.png" alt="image-20211205150832821"></p>
<p>这里包含了模型需要的参数，每次调用都到这里改很麻烦，所以recbole可以实现用test.yaml的设置覆盖具体模型的设置，所以只要在test.yaml（主目录下的那个配置文件）里改我们添加，并做一点修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_layers: <span class="number">2</span></span><br><span class="line">n_heads: <span class="number">2</span></span><br><span class="line">hidden_size: <span class="number">100</span></span><br><span class="line">inner_size: <span class="number">256</span></span><br><span class="line">hidden_dropout_prob: <span class="number">0.5</span></span><br><span class="line">attn_dropout_prob: <span class="number">0.5</span></span><br><span class="line">hidden_act: <span class="string">&#x27;gelu&#x27;</span></span><br><span class="line">layer_norm_eps: <span class="number">1e-12</span></span><br><span class="line">initializer_range: <span class="number">0.02</span></span><br><span class="line">loss_type: <span class="string">&#x27;CE&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="训练设置——train-setting"><a href="#训练设置——train-setting" class="headerlink" title="训练设置——train setting"></a>训练设置——train setting</h3><p>通用的训练设置也要写到test.yaml中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># training settings</span><br><span class="line">epochs: 500  				#训练的最大轮数</span><br><span class="line">train_batch_size: 20 #2048 	#训练的batch_size</span><br><span class="line">learner: adam 				#使用的pytorch内置优化器</span><br><span class="line">learning_rate: 0.001 		#学习率</span><br><span class="line">training_neg_sample_num: 0 	#负采样数目</span><br></pre></td></tr></table></figure>



<h3 id="评估设置——evaluate-setting"><a href="#评估设置——evaluate-setting" class="headerlink" title="评估设置——evaluate setting"></a>评估设置——evaluate setting</h3><p>通用的评估设置也要写道test.yaml中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># evalution settings</span><br><span class="line">eval_step: 1 				#每次训练后做evalaution的次数</span><br><span class="line">eval_setting: RO_RS,full 	#对数据随机重排，设置按比例划分数据集，且使用全排序</span><br><span class="line">group_by_user: True 		#是否将一个user的记录划到一个组里，当eval_setting使用RO_RS的时候该项必须是True</span><br><span class="line">split_ratio: [0.8,0.1,0.1] 	#切分比例</span><br><span class="line">metrics: [&quot;Recall&quot;, &quot;MRR&quot;,&quot;NDCG&quot;,&quot;Hit&quot;,&quot;Precision&quot;] #评测标准</span><br><span class="line">topk: [10] #评测标准使用topk，设置成10评测标准就是[&quot;Recall@10&quot;, &quot;MRR@10&quot;, &quot;NDCG@10&quot;, &quot;Hit@10&quot;, &quot;Precision@10&quot;]</span><br><span class="line">valid_metric: MRR@10 		#选取哪个评测标准作为作为提前停止训练的标准</span><br><span class="line">stopping_step: 10 			#控制训练收敛的步骤数，在该步骤数内若选取的评测标准没有什么变化，就可以提前停止了</span><br><span class="line">eval_batch_size: 4096 		#评测的batch_size</span><br></pre></td></tr></table></figure>

<p>recbole实现了earlystopping早停策略，可以设置控制收敛的步骤数。</p>
<p>eval_setting，可以设置不同的数据切分方式，具体可见<a href="https://recbole.io/cn/evaluation.html">评测 | 伯乐 (recbole.io)</a>，大致如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205163312199.png" alt="image-20211205163312199"></p>
<h2 id="run！跑起来吧，baseline！"><a href="#run！跑起来吧，baseline！" class="headerlink" title="run！跑起来吧，baseline！"></a>run！跑起来吧，baseline！</h2><p>我们cd到RecBole目录下，此时数据集已经准备好，test.yaml文件也已经写好，可以开始跑实验了，用以下指令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python run_recbole.py --model=SASRec --dataset=book ----config_files=test.yaml</span><br></pre></td></tr></table></figure>

<p>如果看不懂参数的话，点进run_recbole.py看一下就明白了！</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>RecBole是个集成度很高，也比较方便用于复现一些基本推荐模型的开源库（在此致敬中国人民大学AI BOX小组！），需要用recbole跑自己的baseline时，只需要三步：</p>
<ol>
<li>构造满足原子文件的数据集</li>
<li>写好.yaml格式的配置文件</li>
<li>run！</li>
</ol>
<h2 id="记录一些遇到的坑"><a href="#记录一些遇到的坑" class="headerlink" title="记录一些遇到的坑"></a>记录一些遇到的坑</h2><h3 id="1-neg-sampling-不能使用-‘CE’-loss"><a href="#1-neg-sampling-不能使用-‘CE’-loss" class="headerlink" title="1. neg_sampling 不能使用 ‘CE’ loss"></a>1. neg_sampling 不能使用 ‘CE’ loss</h3><p>在配置文件中加上一行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">neg_sampling: (注意这里冒号后面有个空格)</span><br></pre></td></tr></table></figure>



<h3 id="2-使用不上gpu"><a href="#2-使用不上gpu" class="headerlink" title="2.使用不上gpu"></a>2.使用不上gpu</h3><p><strong>问题描述：</strong>程序可以跑起来，但是nvtop看不到它在gpu上运行。并且无论如何修改配置文件都没有用。非常奇怪的问题。检查torch.cuda.is_available()的时候发现，居然输出False，所以原因是用不了CUDA。</p>
<p><strong>问题原因</strong>：用不了CUDA。</p>
<p>用conda list检查安装的库时发现，默认安装的torch是cpu的</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205140943532.png" alt="image-20211205140943532"></p>
<blockquote>
<p>左边是默认安装的，想了一宿也没想明白为什么默认安装cpu版本</p>
</blockquote>
<p>卸载cpu版本再安装cuda版本有点麻烦，所以我直接新建了一个环境，并且自带torch=1.10，然后再根据依赖安装，使用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install -r requirement.txt</span><br></pre></td></tr></table></figure>

<p>这里不用担心torch会被覆盖，因为torch版本大于1.17就会自动跳过了。</p>
<p>然后总可以跑了吧，运行下面指令（用RecBole需要这样输）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python run_recbole.py --model=BERT4Rec --dataset=book ----config_files=bert4rec.yaml</span><br></pre></td></tr></table></figure>

<p>按理说此时torch是建环境时我安装的cuda版本，一定不是cpu版本，然而每次运行仍然是device=cpu。实在让人崩溃！</p>
<p><strong>解决方案</strong>：</p>
<p>最后debug多轮，寻找到的解决方案是：</p>
<ul>
<li>在run_recbole.py里import torch，并且打印torch.cuda.is_available()</li>
</ul>
<p>这样做合理的<strong>可能的原因</strong>：</p>
<p>可能项目某个地方import torch，import进来的torch是cpu版本的，所以提前import可以解决。</p>
]]></content>
      <categories>
        <category>代码阅读</category>
      </categories>
      <tags>
        <tag>RecBole</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Self-Attentive Sequential Recommendation》</title>
    <url>/2021/11/03/SASRec/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117230533153.png" alt="image-20211117230533153"></p>
<hr>
<p>原paper：<a href="https://ieeexplore.ieee.org/document/8594844">https://ieeexplore.ieee.org/document/8594844</a></p>
<p>源码解读：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec</a></p>
<hr>
<p>中译：自注意序列推荐</p>
<p>总结：比较早使用self-attention的序列推荐模型</p>
<hr>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>  question作者想解决什么问题？  </li>
</ul>
<p>序列动态是许多当代推荐系统的一个重要特征，它试图根据用户最近执行的操作来捕捉用户活动的“上下文“。RNN模型可以在稠密数据集上捕捉长期语义。（马尔科夫链）MC模型可以在稀疏数据集上仅根据最近几次action做出预测。本文想平衡这两个目标：在稀疏和稠密数据集上做到捕捉长期语义、依赖较少的action做预测。</p>
<ul>
<li>  method作者通过什么理论/模型来解决这个问题？</li>
</ul>
<p>本文提出了一个基于self-attention的序列模型（SASRec），在每个时间步寻找与用户历史最相关的物品作为next item的预测。</p>
<ul>
<li>  answer作者给出的答案是什么？</li>
</ul>
<p>在稀疏和稠密数据集上，与MC/CNN/RNN方法相比都取得了SOTA效果。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>  why作者为什么研究这个课题？    </li>
</ul>
<p>MC方法模型简单，但因为它的强假设（当前预测仅取决于最近n次）使得它在稀疏数据上表现好，但是不能捕捉更复杂的动态转换。RNN方法需要稠密数据，并且计算复杂。最近出现新的序列模型Transformer，它是基于self-attention的，效率高并且可以捕获句子中单词的句法和语义模式。受self-attention方法启发，应用到序列推荐上。</p>
<ul>
<li>  how当前研究到了哪一阶段？ </li>
</ul>
<p>第一个将transformer里的self-attention应用到了序列推荐上。</p>
<ul>
<li>  what作者基于什么样的假设（看不懂最后去查）？</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>优点 <ul>
<li>  SASRec模型建模了整个序列，自适应地考虑items来预测</li>
<li>  在dense和sparse的数据集上效果都很好</li>
<li>  比CNN/RNN方法快了一个数量级</li>
</ul>
</li>
<li>  缺点</li>
<li>展望<ul>
<li>  引进更多上下文信息，比如等待时间、行为类型、位置、设备等。</li>
<li>  探索处理超长序列（如clicks）的方法</li>
</ul>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 （都开源）<ul>
<li>  Amazon</li>
<li>  Steam 作者爬的，开源了</li>
<li>  Movielens</li>
</ul>
</li>
<li>重要指标 <ul>
<li>  Hit@10</li>
<li>  NDCG@10</li>
</ul>
</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>这部分主要参考了知乎[[1]](<a href="https://zhuanlan.zhihu.com/p/277660092?utm_source=qq">推荐算法炼丹笔记：序列化推荐算法SASRec - 知乎 (zhihu.com)</a>)</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211115114043810.png" alt="image-20211115114043810"></p>
<p><strong>1.Embedding层</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-6127cd6bfcdc00f007ba287f11c1f55f_720w.jpg" alt="img"></p>
<p><strong>A. Positional Embedding</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-558fcc53330d91271fc2850a3998e704_720w.jpg" alt="img"></p>
<p><strong>2.Self-Attention Block</strong></p>
<p><strong>A.Self-Attention Layer</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-4ad7a98ce285113021eade4349199c5f_720w.jpg" alt="img"></p>
<p><strong>C.Point-Wise Feed-Forward Network</strong>: 尽管self-attention能将之前item的emebdding使用自适应的权重进行集成，但仍然是一个先线性模型,为了加入非线性能力, 我们使用两层的DDN,</p>
<p><img src="https://pic2.zhimg.com/80/v2-bdfa1cac41b3f4aa676e81d54a72671d_720w.jpg" alt="img"></p>
<p><strong>3.Stacking Self-Attention Blocks</strong></p>
<p>在第一个self-attention block之后,学习item的迁移可以学习更加复杂的item迁移,所以我们对self-attention block进行stacking,第b(b&gt;1)的block可以用下面的形式进行定义：</p>
<p><img src="https://pic3.zhimg.com/80/v2-cdc40ee5705587460d39e19649625942_720w.jpg" alt="img"></p>
<p><strong>4.Prediction Layer</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-873157dd4336dcbbd818227c7ced3f25_720w.jpg" alt="img"></p>
<p>使用同质(homogeneous)商品embedding的一个潜在问题是，它们的内部产品不能代表不对称的商品转换。然而，我们的模型没有这个问题，因为它学习了一个非线性变换。例如，前馈网络可以很容易地实现同项嵌入的不对称性,<strong>经验上使用共享的商品embedding也可以大大提升模型的效果;</strong></p>
<p><strong>显示的用户建模</strong>：为了提供个性化的推荐,现有的方法常用两种方法,(1).学习显示的用户embedding表示用户的喜好;(2).考虑之前的行为并且引入隐式的用户embedding。此处使用并没有带来提升。</p>
<p><strong>5.网络训练</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-684099a2a86837c0b3ad701ea2169710_720w.jpg" alt="img"></p>
<p><strong>6.方案复杂度分析</strong></p>
<p><strong>a. 空间复杂度</strong></p>
<p>模型中学习的参数来自于self-attention.ffn以及layer normalization的参数,总的参数为:</p>
<p><img src="https://pic4.zhimg.com/80/v2-3d4d8db1c48964728a0c6830ecc4a71b_720w.jpg" alt="img"></p>
<p><strong>b. 时间复杂度</strong></p>
<p>我们模型的计算复杂度主要在于self-attention layer和FFN网络,</p>
<p><img src="https://pic1.zhimg.com/80/v2-1cd0b2b09e9bc3fba57281ab76f2d478_720w.jpg" alt="img"></p>
<p>里面最耗时间的还是self-attention layer, 不过这个可以进行并行化。</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><p>该次实验主要为了验证下面的四个问题：</p>
<ol>
<li>是否SASRec比现有最好的模型(CNN/RNN)要好？</li>
<li>在SASRec框架中不同的成份的影响怎么样？</li>
<li>SASRec的训练效率和可扩展性怎么样？</li>
<li>attention的权重是否可以学习得到关于位置和商品属性的有意义的模式?</li>
</ol>
<p><strong>1. 推荐效果比较</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-e789c62c7c2e998f0713341ebc43155f_720w.jpg" alt="img"></p>
<ul>
<li>SASRec在稀疏的和dense的数据集合熵比所有的baseline都要好, 获得了6.9%的Hit Rate提升以及9.6%的NDCG提升；</li>
</ul>
<p><strong>2. SASRec框架中不同成份的影响</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-155ea54d12922a3d1aafcece005b5731_720w.jpg" alt="img"></p>
<ul>
<li>删除PE: 删除位置embedding ,在稀疏的数据集上,删除PE效果变好,但是在稠密的数据集上,删除PE的效果变差了。</li>
<li>不共享IE(Item Embedding): 使用共享的item embedding比不使用要好很多;</li>
<li>删除RC(Residual Connection):不实用残差连接,性能会变差非常多;</li>
<li>删除Dropout: dropout可以帮助模型,尤其是在稀疏的数据集上,Dropout的作用更加明显;</li>
<li>blocks的个数：没有block的时候,效果最差,在dense数据集上,相比稀疏数据多一些block的效果好一些;</li>
<li>Multi-head:在我们数据集上,single-head效果最好.</li>
</ul>
<p><strong>3. SASRec的训练效率和可扩展性</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-b4a0692c6cf9b0a335dae79eba2ed723_720w.jpg" alt="img"></p>
<p><img src="https://pic1.zhimg.com/80/v2-1a7f5f0f47c7ada0e2ccd22b23078584_720w.jpg" alt="img"></p>
<ul>
<li>SASRec是最快的;</li>
<li>序列长度可以扩展至500左右.</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/277660092?utm_source=qq">[1]推荐算法炼丹笔记：序列化推荐算法SASRec</a></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>序列推荐</tag>
        <tag>SASRec</tag>
        <tag>自注意力</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Streaming Session-based Recommendation》</title>
    <url>/2022/03/07/SSRM/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220307185213298.png" alt="image-20220307185213298"></p>
<hr>
<p>原paper：<a href="https://dl.acm.org/doi/10.1145/3292500.3330839">Streaming Session-based Recommendation | Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</a></p>
<p>源码解读：未开源</p>
<hr>
<p>中译：流会话推荐</p>
<p>总结：第一篇结合了流推荐和会话推荐的论文（准备开坑）。主要解决两个问题，MF attention + GRU  解决用户行为的不确定性；存储技术+主动采样策略 解决了更贴近实时场景的“高速、海量、连续的流数据”的需求。个人认为可以进一步做的地方：session encoder部分，用新模型；储存技术；采样技术。</p>
<hr>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>question作者想解决什么问题？ </p>
<p>1）用户行为的不确定性。</p>
<p>2）会话推荐在实际场景中会以会话流的形式出现，即连续不断的、海量的、高速的会话数据，但现有会话推荐模型都是离线模型，没有模型可以解决这个问题。</p>
</li>
<li><p>method作者通过什么理论/模型来解决这个问题？</p>
<p>作者提出SSRM(Streaming Session-based Recommendation Machine)模型，其中</p>
<p>1）为了解决用户行为的不确定性，作者利用历史交互，提出基于矩阵分解的注意力模型。</p>
<p>2）为了解决流会话数据“海量”、“高速”的挑战，作者基于存储提出使用主动采样策略的流模型。</p>
</li>
<li><p>answer作者给出的答案是什么？</p>
<p>在LastFM和Gowalla数据集上证明了SOTA。</p>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>第一篇提出流会话推荐模型的论文。</p>
<ul>
<li><p>why作者为什么研究这个课题？    </p>
<p>“流会话”(Streaming session)的设定更贴近实际。</p>
</li>
<li><p>how当前研究到了哪一阶段？</p>
<p>第一篇提出流会话推荐模型的论文。</p>
</li>
<li><p>what作者基于什么样的假设（看不懂最后去查）？</p>
<p>1）用户的历史交互信息是可获得的</p>
<p>2）背景是流会话的设定，即会话数据海量、连续不断、迅速地迭代。</p>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 </li>
</ul>
<p>LastFM：<a href="http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz">http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz</a></p>
<p>Gowalla：<a href="https://snap.stanford.edu/data/loc-gowalla.html">https://snap.stanford.edu/data/loc-gowalla.html</a></p>
<ul>
<li>数据划分</li>
</ul>
<p>给数据集 $D$ 中的会话按时间排序，分成前60%作为训练集，和后40%作为候选集。为了模拟线上的流数据输入，将候选集再划分成5个等长切片作为测试机。第一个测试机和10%的训练集作为验证集。实验中，若要预测第 $i$ 个测试集的序列行为，那么 $i$ 之前的测试集切片都用作在线训练。</p>
<ul>
<li>重要指标 </li>
</ul>
<p>MRR@20、Recall@20</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li><h3 id="SSRM模型框架"><a href="#SSRM模型框架" class="headerlink" title="SSRM模型框架"></a>SSRM模型框架</h3></li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220307191209899.png" alt="image-20220307191209899"></p>
<p>SSRM的主要工作是，建立一个基于注意力的会话推荐系统（离线模型），再将其拓展到流会话的设定。</p>
<ul>
<li><h3 id="离线模型：基于历史行为的注意力编码器"><a href="#离线模型：基于历史行为的注意力编码器" class="headerlink" title="离线模型：基于历史行为的注意力编码器"></a>离线模型：基于历史行为的注意力编码器</h3></li>
</ul>
<p>离线模型 = 建模序列 + 矩阵分解</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220307193225545.png" alt="image-20220307193225545"></p>
<p>其中，建模“序列” = 基础会话编码器 + 基于矩阵分解的注意力会话编码器。这两部分如下1.2.：</p>
<ol>
<li><strong>基础会话编码器(Basic Session Encoder)</strong></li>
</ol>
<p>主要为了建模当前会话的表示。采用基本的GRU模型（重置门、更新门、候选状态），将最后一个隐藏状态作为当前会话 $i$ 的基本表示： $c_i = h_t$ 。</p>
<ol start="2">
<li><strong>基于矩阵分解的注意力会话编码器(MF-based Attentive Session Encoder)</strong></li>
</ol>
<p>MF分解得到每个用户的隐含表示 $p_u \in \R ^ {1 \times D} $ 和每个物品的隐含表示 $q_t \in \R ^ {1 \times D} $ 。MF既用来单独作为一个模块，还通过引入MF（利用了历史交互信息）来加强上面的基础模型。</p>
<p>$p_u $ 和 $q_t$ 的内积  $\hat y_{u,t}^R = &lt;p_u,q_t&gt;=p_u \cdot q_t$ 用来表示用户 $u$ 有多“喜欢”物品 $i$ 。这里的 $q_t$ 和上面的 $h_t$ 是物品 $t$ 的两种表示，作用、含义不同： $h_t$ ，即GRU在物品 $t$ 处的隐含表示，是总结了序列行为(1,2,…,t)的物品表示；而 $q_t$ 作为实际的物品表示，可用于和用户隐含表示做内积，表明用户是否喜欢这个物品。</p>
<p>为了降低随机性、捕捉用户 $u$ 的主要意图，这里使用了注意力机制编码会话表示 $c_i$：<br>$$<br>c_i = [\ \sum^t_{j=1}\alpha_{u,j}\cdot h_j \ ; h_t \ ]<br>$$<br>其中权重 $\alpha_u = softmax(\hat y_u^R)$ ，即对用户 $u$ 对序列中物品的“喜欢”程度的打分进行softmax，得到和为1的权重。求到加权的表示后，再与GRU得到的会话表示 $h_t$ 拼接，作为最终的会话表示（这里 $h_t$ 用到了两次，一次作为最后一个物品表示，一次作为会话表示）。</p>
<ol start="3">
<li><strong>混合注意力推荐系统</strong></li>
</ol>
<p>SSRM再进一步将注意力编码器的输出和MF的输出结合，使得模型不仅考虑当前会话的序列行为，还考虑了用户的长期兴趣和共现行为。</p>
<p>会话编码器最后输出，每个会话对每个物品的打分： $\hat y_{i,t}^S = q_t B c_i^T $  ，表示第 $i$ 个会话对物品 $t$ 的i打分，其中 $q_t \in \R ^ {1 \times D} $ ， $c_i \in \R ^ {1 \times H} $ ，变换矩阵 $B \in \R ^ {D \times H} $ 。</p>
<p>由MF得到的 $p_u$ 和 $q_t$  的内积  $\hat y_{u,t}^R =p_u \cdot q_t$ 。</p>
<p>两者使用一个可调参数 $w$ 加权求和  $\hat y_{i,t} = w \hat y_{i,t}^R + (1-w)\hat y_{i,t}^S $ 。</p>
<p>最后，把物品排序任务当作分类任务，使用CE loss损失函数训练模型。 $r_u$ 是物品的真实标签分布， $\hat y_u$ 是预测的概率分布。<br>$$<br>L(r_u,\hat y_u) = - \sum^n_{i=1} r_{u,i} \cdot log(\hat y_{u,i})<br>$$</p>
<ul>
<li><h3 id="在线训练：基于存储的、使用主动采样策略的流模型"><a href="#在线训练：基于存储的、使用主动采样策略的流模型" class="headerlink" title="在线训练：基于存储的、使用主动采样策略的流模型"></a>在线训练：基于存储的、使用主动采样策略的流模型</h3></li>
</ul>
<p>建立完基于注意力的会话推荐系统（离线模型）后，再将其拓展到流会话的设定。建立存储的目的是准确地概括总结历史行为。</p>
<p><strong>1.传统模型的更新方法</strong></p>
<p>随机采样技术。令 $C$ 为保存会话序列的存储库，令 $t$ 为下一个时刻即将到达的数据实例。当 $t&gt;|C|$ 时，存储库会以 $\frac{|C|}{t} $ 的概率存储这个数据实例，同时随机替换掉存储库 $C$ 里的数据实例。这样得到的存储库可以证明相当于当前数据集的随机采样结果，同时也证明可以保留模型的长期记忆[1]。但是 $\frac{|C|}{t} $ 是随时间递减的，这样模型就会倾向于忽略最近的数据，因为越近的数据被选入存储库的概率越低。而实际场景中是存在用户兴趣漂移的现象的，换句话说，使用这种随机采样技术难以实现“捕捉最新产生的数据中的行为模式”。</p>
<p><strong>2.主动采样策略</strong></p>
<p>虽然更新时同时使用整个存储库和所有新到达数据能产生更好结果，但是由于高速的流数据和有限的计算资源，这种方式通常会导致可利用的数据非常少（这个问题也被称为系统过载）。所以需要一个明智的样本选择策略。</p>
<p>本文提出的主动采样策略的思想和一些主动学习（Active learning）类似，即“选择对系统贡献最大的样本的最小集合”，供用户评估。具体来说，为了使模型在有限时间窗口内尽可能地多学习，采样策略每次应该选择 $C^{new}\cup C$ 中<strong>信息量最大</strong>的会话实例。</p>
<p>计算会话的信息量，使用会话中每个物品得分的均值，其中物品得分 $r_{u,k}$ 表示用户 $u$ 在当前模型下对物品 $k$ 的预测能力，使用用户隐含表示和物品隐含表示做内积得到 $r_{u,k}=p_u q_k$。$r_{u,k}$ 值越小， $r_{s_i}$ 值越小，说明模型越难以预测该会话中的物品，说明该会话信息量越大，越能修正模型，越该对它进行采样。<br>$$<br>r_{s_i}=\frac{\sum^t_{k=1}r_{u,k}}{t}<br>$$<br>以 $r_{s_i}$ 值对这些会话降序排列，再根据排名计算每个会话的权重因子 $w_{s_i}$，最后得到每个会话的采样概率 $p(s_i)$ ：<br>$$<br>w_{s_i}=exp(\frac{rank_{s_i}}{C\cup C^{new}})<br>;\<br>p(s_i)=\frac{w_{s_i}}{\sum_{s_i \in C \cup C^{new}}w_{s_i}}<br>$$<br>其中，信息量越大的会话排名越靠后，权重因子越高，采样概率也越大。</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><ul>
<li><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3></li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308193608578.png" alt="image-20220308193608578"></p>
<ul>
<li><h3 id="和SBR的经典方法比较"><a href="#和SBR的经典方法比较" class="headerlink" title="和SBR的经典方法比较"></a>和SBR的经典方法比较</h3></li>
</ul>
<p>SOTA。LastFM的提升率比Gowalla高的主要原因是，Gowalla更系数，每个用户的点击很少，并且会话数据只有寥寥几个，从而导致训练不充分。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308193302601.png" alt="image-20220308193302601"></p>
<ul>
<li><h3 id="验证矩阵分解注意力机制的有效性（MF-Attention）"><a href="#验证矩阵分解注意力机制的有效性（MF-Attention）" class="headerlink" title="验证矩阵分解注意力机制的有效性（MF-Attention）"></a>验证矩阵分解注意力机制的有效性（MF-Attention）</h3></li>
</ul>
<p>Baseline：只有基础会话编码器（Basic session encoder），好像就是GRU4Rec。三个模型采用相同的streaming技术，以消除线上更新方法带来的影响。SSRM效果最好。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308193902198.png" alt="image-20220308193902198"></p>
<ul>
<li><h3 id="不同流策略的影响"><a href="#不同流策略的影响" class="headerlink" title="不同流策略的影响"></a>不同流策略的影响</h3></li>
</ul>
<p>S1：没有存储库，且仅用新来的数据更新模型。S2：有存储库，采用从 $C$ 中随机采样训练的传统方法。S3：S2的基础上，从 $C\cup C^{new}$ 中采样。S1、S3优于S2，说明相比于历史long-term memory，模型更能从用户最近行为中受益。而SSRM模型优于其它所有，说明本文提出的主动采样策略是有效的。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308215603572.png" alt="image-20220308215603572"></p>
<ul>
<li><h3 id="不同过载设定的影响"><a href="#不同过载设定的影响" class="headerlink" title="不同过载设定的影响"></a>不同过载设定的影响</h3></li>
</ul>
<p>$W$ ：固定时间窗口，即一次能从 $C\cup C^{new}$ 中采样的样本数。 $W$ 越小说明工作负载越重，只训练到有限的序列。横轴看，窗口越大，过载越轻，模型表现越好；反之窗口越大，过载越重，模型表现越差。纵向看不同测试集，因为这些测试集是按时间顺序分的，越往后新用户、新物品越多。但是可以发现，从test2到test5，它们之间的gap越来越小。SSRM能够快速减小这种gap，证明其处理新数据的能力。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308215631714.png" alt="image-20220308215631714"></p>
<ul>
<li><h3 id="存储库大小的影响"><a href="#存储库大小的影响" class="headerlink" title="存储库大小的影响"></a>存储库大小的影响</h3></li>
</ul>
<p> $|C|$ 的大小决定了保留多少历史信息，它保留的观测行为越多，就越可能从历史行为中采样，就会有更多的样本距离current behavior越久远，故模型就会用更少的当前会话信息来更新。换句话说，对于过去没有出现的用户和项目，模型会学习得更少，这将导致性能相对较差。Figure 6 里也能得出这个结论，最近的行为更重要。但是因为存在long-memory problem，如果只用当前session来训练模型的话，结果会变低，总的来说，历史行为和当前行为两者得结合起来。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308194605932.png" alt="image-20220308194605932"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1]Ernesto Diaz-Aviles, Lucas Drumond, Lars Schmidt-Thieme, and Wolfgang Nejdl. 2012. Real-time top-n recommendation in social streams. RecSys (2012)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>流会话推荐</tag>
        <tag>SSRM</tag>
      </tags>
  </entry>
  <entry>
    <title>SASRec代码笔记</title>
    <url>/2021/11/06/SASRec%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<hr>
<p>完整的代码注释：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec</a></p>
<p>论文笔记：<a href="https://guadzilla.github.io/2021/11/03/SASRec/">https://guadzilla.github.io/2021/11/03/SASRec/</a></p>
<hr>
<h2 id="collections-defaultdict-list"><a href="#collections-defaultdict-list" class="headerlink" title="collections.defaultdict(list)"></a>collections.defaultdict(list)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">collections</span>.<span class="title">defaultdict</span>(<span class="params">default_factory=<span class="literal">None</span>, /[, ...]</span>)</span></span><br></pre></td></tr></table></figure>

<p>返回一个新的类似字典的对象。<code>defaultdict</code> 是内置 <code>dict </code>类的子类。 它重载了一个方法并添加了一个可写的实例变量。</p>
<p>本对象包含一个名为 <code>default_factory</code> 的属性，构造时，第一个参数用于为该属性提供初始值，默认为 None。所有其他参数（包括关键字参数）都相当于传递给 dict 的构造函数。</p>
<p>使用<code> defulydict(list)</code>实例化对象时， <code>default_factory=list</code>，可以很轻松地<strong>将（键-值对组成的）序列转换为（键-列表组成的）字典</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = [(<span class="string">&#x27;yellow&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;blue&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;yellow&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;blue&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;red&#x27;</span>, <span class="number">1</span>)]</span><br><span class="line">d = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> s:</span><br><span class="line">    d[k].append(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>(d.items())</span><br><span class="line"><span class="comment"># 输出：[(&#x27;blue&#x27;, [2, 4]), (&#x27;red&#x27;, [1]), (&#x27;yellow&#x27;, [1, 3])]</span></span><br></pre></td></tr></table></figure>

<p>当字典中没有的键第一次出现时，python自动为其返回一个空列表，list.append()会将值添加进新列表；再次遇到相同的键时，list.append()将其它值再添加进该列表。</p>
<span id="more"></span>

<h2 id="Python自定义多线程"><a href="#Python自定义多线程" class="headerlink" title="Python自定义多线程"></a>Python自定义多线程</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_neq</span>(<span class="params">l, r, s</span>):</span></span><br><span class="line">    t = np.random.randint(l, r)</span><br><span class="line">    <span class="keyword">while</span> t <span class="keyword">in</span> s:</span><br><span class="line">        t = np.random.randint(l, r)</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_function</span>(<span class="params">user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>():</span></span><br><span class="line"></span><br><span class="line">        user = np.random.randint(<span class="number">1</span>, usernum + <span class="number">1</span>)    <span class="comment"># 随机采样user id，注意是从1开始的</span></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(user_train[user]) &lt;= <span class="number">1</span>: user = np.random.randint(<span class="number">1</span>, usernum + <span class="number">1</span>)  <span class="comment"># 长度小于1的训练集不要</span></span><br><span class="line"></span><br><span class="line">        seq = np.zeros([maxlen], dtype=np.int32)    <span class="comment"># seq序列，长度固定为maxlen，用0在前面padding补上长度，例：[0,0,...,0,23,15,2,6]</span></span><br><span class="line">        pos = np.zeros([maxlen], dtype=np.int32)</span><br><span class="line">        neg = np.zeros([maxlen], dtype=np.int32)</span><br><span class="line">        nxt = user_train[user][-<span class="number">1</span>]  <span class="comment"># user_train的最后一个item取为nxt</span></span><br><span class="line">        idx = maxlen - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        ts = <span class="built_in">set</span>(user_train[user])  <span class="comment"># ts为序列的item集合</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(user_train[user][:-<span class="number">1</span>]):   <span class="comment"># 从后往前遍历user_train,idx为当前要填充的下标</span></span><br><span class="line">            seq[idx] = i</span><br><span class="line">            pos[idx] = nxt</span><br><span class="line">            <span class="keyword">if</span> nxt != <span class="number">0</span>: neg[idx] = random_neq(<span class="number">1</span>, itemnum + <span class="number">1</span>, ts)  <span class="comment"># 生成的负样本不能取该序列item集合里的item</span></span><br><span class="line">            nxt = i</span><br><span class="line">            idx -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> idx == -<span class="number">1</span>: <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (user, seq, pos, neg)    <span class="comment"># 返回一次采样，(用户id,训练序列，label序列，负样本序列)</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(SEED)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:     <span class="comment"># 采样一个batch_size大小的数据样本，打包成一个batch，放到线程队列里</span></span><br><span class="line">        one_batch = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            one_batch.append(sample())</span><br><span class="line"></span><br><span class="line">        result_queue.put(<span class="built_in">zip</span>(*one_batch))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WarpSampler</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, User, usernum, itemnum, batch_size=<span class="number">64</span>, maxlen=<span class="number">10</span>, n_workers=<span class="number">1</span></span>):</span></span><br><span class="line">        self.result_queue = Queue(maxsize=n_workers * <span class="number">10</span>)   <span class="comment"># 长度为10的线程队列</span></span><br><span class="line">        self.processors = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_workers):</span><br><span class="line">            self.processors.append(     <span class="comment"># Process()进程的类, target：要调用的对象即sampler_function，args：调用该对象要接受的参数</span></span><br><span class="line">                Process(target=sample_function, args=(User,</span><br><span class="line">                                                      usernum,</span><br><span class="line">                                                      itemnum,</span><br><span class="line">                                                      batch_size,</span><br><span class="line">                                                      maxlen,</span><br><span class="line">                                                      self.result_queue,</span><br><span class="line">                                                      np.random.randint(<span class="number">2e9</span>)</span><br><span class="line">                                                      )))</span><br><span class="line">            self.processors[-<span class="number">1</span>].daemon = <span class="literal">True</span></span><br><span class="line">            self.processors[-<span class="number">1</span>].start()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_batch</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.result_queue.get()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.processors:</span><br><span class="line">            p.terminate()</span><br><span class="line">            p.join()</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line"><span class="comment"># sampler是WarpSampler对象的实例，每次调用sampler.next_batch(),就返回一个batch的样本。</span></span><br><span class="line"><span class="comment"># 进一步解释：每次调用sampler.next_batch()就call其线程队列里的一个线程，每个线程用于返回一个batch的数据。</span></span><br><span class="line">sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen, n_workers=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>



<h2 id="torch-tril"><a href="#torch-tril" class="headerlink" title="torch.tril()"></a>torch.tril()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.tril(<span class="built_in">input</span>, diagonal=<span class="number">0</span>, *, out=<span class="literal">None</span>) → Tensor</span><br><span class="line"><span class="comment"># 功能：返回下三角矩阵其余部分用out填充（默认为0）</span></span><br><span class="line"><span class="comment"># input：输入矩阵，二维tensor</span></span><br><span class="line"><span class="comment"># diagonal：表示对角线位置，diagonal=0为主对角线，diagonal=-1为主对角线往下1格，diagonal=1为主对角线往上1格</span></span><br><span class="line"><span class="comment"># out：表示填充，默认用out=None即0填充</span></span><br></pre></td></tr></table></figure>

<p>例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[-<span class="number">1.0813</span>, -<span class="number">0.8619</span>,  <span class="number">0.7105</span>],</span><br><span class="line">        [ <span class="number">0.0935</span>,  <span class="number">0.1380</span>,  <span class="number">2.2112</span>],</span><br><span class="line">        [-<span class="number">0.3409</span>, -<span class="number">0.9828</span>,  <span class="number">0.0289</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tril(a)</span><br><span class="line">tensor([[-<span class="number">1.0813</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0935</span>,  <span class="number">0.1380</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [-<span class="number">0.3409</span>, -<span class="number">0.9828</span>,  <span class="number">0.0289</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.randn(<span class="number">4</span>, <span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">tensor([[ <span class="number">1.2219</span>,  <span class="number">0.5653</span>, -<span class="number">0.2521</span>, -<span class="number">0.2345</span>,  <span class="number">1.2544</span>,  <span class="number">0.3461</span>],</span><br><span class="line">        [ <span class="number">0.4785</span>, -<span class="number">0.4477</span>,  <span class="number">0.6049</span>,  <span class="number">0.6368</span>,  <span class="number">0.8775</span>,  <span class="number">0.7145</span>],</span><br><span class="line">        [ <span class="number">1.1502</span>,  <span class="number">3.2716</span>, -<span class="number">1.1243</span>, -<span class="number">0.5413</span>,  <span class="number">0.3615</span>,  <span class="number">0.6864</span>],</span><br><span class="line">        [-<span class="number">0.0614</span>, -<span class="number">0.7344</span>, -<span class="number">1.3164</span>, -<span class="number">0.7648</span>, -<span class="number">1.4024</span>,  <span class="number">0.0978</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tril(b, diagonal=<span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">1.2219</span>,  <span class="number">0.5653</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.4785</span>, -<span class="number">0.4477</span>,  <span class="number">0.6049</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">1.1502</span>,  <span class="number">3.2716</span>, -<span class="number">1.1243</span>, -<span class="number">0.5413</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [-<span class="number">0.0614</span>, -<span class="number">0.7344</span>, -<span class="number">1.3164</span>, -<span class="number">0.7648</span>, -<span class="number">1.4024</span>,  <span class="number">0.0000</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tril(b, diagonal=-<span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.4785</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">1.1502</span>,  <span class="number">3.2716</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [-<span class="number">0.0614</span>, -<span class="number">0.7344</span>, -<span class="number">1.3164</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>]])</span><br></pre></td></tr></table></figure>



<h2 id="Python中的-波浪线运算符"><a href="#Python中的-波浪线运算符" class="headerlink" title="Python中的 ~ 波浪线运算符"></a>Python中的 ~ 波浪线运算符</h2><p>~，用法只有一个那就是按位取反</p>
<p><a href="https://blog.csdn.net/lanchunhui/article/details/51746477"> Python 波浪线与补码_https://space.bilibili.com/59807853-CSDN博客_python 波浪线</a></p>
<h2 id="torch-nn-MultiAttention"><a href="#torch-nn-MultiAttention" class="headerlink" title="torch.nn.MultiAttention"></a>torch.nn.MultiAttention</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=<span class="number">0.0</span>, bias=<span class="literal">True</span>, add_bias_kv=<span class="literal">False</span>, add_zero_attn=<span class="literal">False</span>, kdim=<span class="literal">None</span>, vdim=<span class="literal">None</span>, batch_first=<span class="literal">False</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>):</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对应公式：<br>$$<br>Multihead(Q,K,V) = Concat(head_1,…,head_h)W^O    \<br>where \quad head_i= Attention(QW^Q_i,KW^K_i,VW^V_i)<br>$$</p>
<p>计算公式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">forward(query, key, value, key_padding_mask=<span class="literal">None</span>, need_weights=<span class="literal">True</span>, attn_mask=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>QKV比较常规，需要注意的是</p>
<ol>
<li>key_padding_mask参数，大小为（N，S），指定key中的哪些元素不做attention计算，即看作padding。注意，为True的位置不计算attention（是padding的地方不计算）</li>
<li>attn_mask参数，</li>
</ol>
<h2 id="torch-nn-BCEWithLogitsLoss"><a href="#torch-nn-BCEWithLogitsLoss" class="headerlink" title="torch.nn.BCEWithLogitsLoss()"></a>torch.nn.BCEWithLogitsLoss()</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">forward(self, input: Tensor, target: Tensor) -&gt; Tensor</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li>input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits).</li>
<li>target: Tensor of the same shape as input with values between 0 and 1</li>
</ul>
<p>input：$x$        output：$y$</p>
<p>$$<br>ℓ(x,y)=L={l_1,…,l_N}^T<br>$$<br>$$<br>l_n=−w_n[y_n·log\sigma(x_n)+(1−y_n)·log(1−\sigma(x_n))]<br>$$</p>
<p>当 $y=1$ 时，$l_n=−log\sigma(x_n)$  ；当 $y=0$ 时，$l_n=−log(1-\sigma(x_n))$     。</p>
<p>论文里使用了一个全1的矩阵pos_labels，和一个全0的矩阵neg_labels。正例标签值都为1（正确的item，ground truth应该是概率为1），负例标签值都为0（错误的item，ground truth应该是概率为0）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), \</span><br><span class="line">torch.zeros(neg_logits.shape, device=args.device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(&quot;\neye ball check raw_logits:&quot;); print(pos_logits); print(neg_logits)</span></span><br><span class="line"><span class="comment"># check pos_logits &gt; 0, neg_logits &lt; 0</span></span><br><span class="line">adam_optimizer.zero_grad()</span><br><span class="line">indices = np.where(pos != <span class="number">0</span>)    <span class="comment"># 返回一个二维数组array， array[0]=[横坐标], array[1]=[纵坐标]</span></span><br><span class="line">loss = bce_criterion(pos_logits[indices], pos_labels[indices])  <span class="comment"># 使正例的得分尽量</span></span><br><span class="line">loss += bce_criterion(neg_logits[indices], neg_labels[indices])</span><br></pre></td></tr></table></figure>



<h2 id="torch-argsort"><a href="#torch-argsort" class="headerlink" title="torch.argsort()"></a>torch.argsort()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.argsort(<span class="built_in">input</span>, dim=-<span class="number">1</span>, descending=<span class="literal">False</span>) → LongTensor</span><br></pre></td></tr></table></figure>

<p>沿着指定dim从小到大（默认）排序元素，然后返回这些元素原来的下标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;t = torch.randint(<span class="number">1</span>,<span class="number">10</span>,(<span class="number">1</span>,<span class="number">5</span>))</span><br><span class="line">&gt;&gt;&gt;t</span><br><span class="line">tensor([[<span class="number">7</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">3</span>]])</span><br><span class="line">&gt;&gt;&gt;t.argsort()</span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">&gt;&gt;&gt;t.argsort().argsort()</span><br><span class="line">tensor([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两次argsort()可以返回每个元素的rank排名</span></span><br><span class="line"><span class="comment"># 解释：</span></span><br><span class="line"><span class="comment"># 把商品0,1,2,3,4按顺序摆好，他们的得分分别为[7,9,5,6,3]</span></span><br><span class="line"><span class="comment"># 对所有商品的得分从小到大排序（argsort()操作）</span></span><br><span class="line"><span class="comment"># 得到积分排名是[3,5,6,7,9]，积分排名对应的商品id是[4,2,3,0,1]（第一次argsort()的结果），每个商品id对应的下标就是他们的得分名次</span></span><br><span class="line"><span class="comment"># 例如商品4得分最高排在第一位，商品1得分最低排最后一位</span></span><br><span class="line"><span class="comment"># 然后我们想得到0,1,2,3,4顺序下的结果</span></span><br><span class="line"><span class="comment"># 所以对商品id排序，使得商品摆放顺序由[4,2,3,0,1]变为[0,1,2,3,4]，这里也是argsort()操作，因为0~4天然有顺序关系</span></span><br><span class="line"><span class="comment"># [4,2,3,0,1]变为[0,1,2,3,4]的同时，排名情况[0,1,2,3,4]也变成了[3,4,1,2,0]（第二次argsort()的结果）</span></span><br><span class="line"><span class="comment"># 即求得每个商品在原来顺序下的得分名次</span></span><br></pre></td></tr></table></figure>

<p><a href="https://www.cnblogs.com/traditional/p/13702904.html">numpy中的argmax、argmin、argwhere、argsort、argpartition函数 - 古明地盆 - 博客园 (cnblogs.com)</a></p>
<h2 id="评价指标Hit-Ratio、NDCG-1"><a href="#评价指标Hit-Ratio、NDCG-1" class="headerlink" title="评价指标Hit Ratio、NDCG[1]"></a>评价指标Hit Ratio、NDCG<a href="https://dl.acm.org/doi/10.1145/2806416.2806504">[1]</a></h2><h3 id="Hit-Ratio"><a href="#Hit-Ratio" class="headerlink" title="Hit Ratio"></a>Hit Ratio</h3><p>Evaluation Metrics. Given a user, each algorithm produces a ranked list of items. To assess the ranked list with the ground-truth item set (GT), we adopt Hit Ratio (HR), which has been commonly used in top-N evaluation . If a test item appears in the recommended list, it is deemed a hit. HR is calculated as:<br>$$<br>HR@K=\frac{Number\ of \  Hits@K}{|GT|}<br>$$</p>
<h3 id="NDCG"><a href="#NDCG" class="headerlink" title="NDCG"></a>NDCG</h3><p>As the HR is recall-based metric, it does not reflect the accuracy of getting top ranks correct, which is crucial in many real-world applications. To address this, we also adopt Normalized Discounted Cumulative Gain (NDCG), which assigns higher importance to results at top ranks, scoring successively lower ranks with marginal fractional utility:<br>$$<br>NDCG@K=Z_K\sum^K_{i=1}\frac{2^{r_i}-1}{log_2{(i+1)}}<br>$$<br>where ZK is the normalizer to ensure the perfect ranking has a value of 1; ri is the graded relevance of item at position i. We use the simple binary relevance for our work: ri = 1 if the item is in the test set, and 0 otherwise. For both metrics, larger values indicate better performance. In the evaluation, we calculate both metrics for each user in the test set, and report the average score.</p>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># evaluate on test set</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">model, dataset, args</span>):</span></span><br><span class="line">    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)  <span class="comment"># deepcopy一份用于valid和test</span></span><br><span class="line"></span><br><span class="line">    NDCG = <span class="number">0.0</span></span><br><span class="line">    HT = <span class="number">0.0</span></span><br><span class="line">    valid_user = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> usernum &gt; <span class="number">10000</span>:  <span class="comment"># 用户数量大于10000就随机采10000</span></span><br><span class="line">        users = random.sample(<span class="built_in">range</span>(<span class="number">1</span>, usernum + <span class="number">1</span>), <span class="number">10000</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        users = <span class="built_in">range</span>(<span class="number">1</span>, usernum + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> users:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(train[u]) &lt; <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">len</span>(test[u]) &lt; <span class="number">1</span>: <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        seq = np.zeros([args.maxlen], dtype=np.int32)</span><br><span class="line">        idx = args.maxlen - <span class="number">1</span></span><br><span class="line">        <span class="comment"># 假设原始序列为[1,2,3,4,5,6,7]    6用于valid；7用于test</span></span><br><span class="line">        seq[idx] = valid[u][<span class="number">0</span>]  <span class="comment"># seq: [0,0,0,...,0,0,0,6]</span></span><br><span class="line">        idx -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(train[u]):  <span class="comment"># seq: [0,0,0,...,0,1,2,3,4,5,6]  只剩test里的[7]用于预测</span></span><br><span class="line">            seq[idx] = i</span><br><span class="line">            idx -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> idx == -<span class="number">1</span>: <span class="keyword">break</span></span><br><span class="line">        rated = <span class="built_in">set</span>(train[u])  <span class="comment"># 序列物品集合</span></span><br><span class="line">        rated.add(<span class="number">0</span>)</span><br><span class="line">        item_idx = [test[u][<span class="number">0</span>]]  <span class="comment"># 取出ground truth label</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># item_idx: [label,random,random,...,random] 1+100个随机物品，看得分在top10的情况</span></span><br><span class="line">            t = np.random.randint(<span class="number">1</span>, itemnum + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">while</span> t <span class="keyword">in</span> rated: t = np.random.randint(<span class="number">1</span>, itemnum + <span class="number">1</span>)</span><br><span class="line">            item_idx.append(t)</span><br><span class="line"></span><br><span class="line">        predictions = -model.predict(*[np.array(l) <span class="keyword">for</span> l <span class="keyword">in</span> [[u], [seq], item_idx]])</span><br><span class="line">        predictions = predictions[<span class="number">0</span>]  <span class="comment"># (1,101) -&gt; 101 (squeeze)</span></span><br><span class="line"></span><br><span class="line">        rank = predictions.argsort().argsort()[<span class="number">0</span>].item()  <span class="comment"># 做两次argsort()，可以得到每个位置的rank排名</span></span><br><span class="line"></span><br><span class="line">        valid_user += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rank &lt; <span class="number">10</span>:  <span class="comment"># TOP10才记录，这里真实rank = rank + 1 ，因为argsort()索引包含0</span></span><br><span class="line">            NDCG += <span class="number">1</span> / np.log2(rank + <span class="number">2</span>)</span><br><span class="line">            HT += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> valid_user % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;.&#x27;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">            <span class="comment"># sys.stdout.flush()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> NDCG / valid_user, HT / valid_user</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p><a href="https://dl.acm.org/doi/10.1145/2806416.2806504">[1]He X, Chen T, Kan M Y, et al. Trirank: Review-aware explainable recommendation by modeling aspects</a></p>
]]></content>
      <categories>
        <category>代码阅读</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>SASRec</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Time Interval Aware Self-Attention for Sequential Recommendation》</title>
    <url>/2021/11/18/TiSASRec/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117164643993.png"></p>
<hr>
<p>原paper：<a href="https://dl.acm.org/doi/10.1145/3336191.3371786">https://dl.acm.org/doi/10.1145/3336191.3371786</a></p>
<p>源码解读：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec</a></p>
<hr>
<p>中译：时间间隔感知的自注意力序列推荐</p>
<p>总结：是SASRec工作的延续，在self-attention的基础上加了绝对位置信息和相对时间间隔信息（加在Q和K里）取得了更好的performamce。发现Beauty数据集序列模式不明显。</p>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>  <strong>question作者想解决什么问题？</strong></li>
</ul>
<p>MC模型和RNN模型都只将用户交互作为有序序列（一种强假设），却没有考虑交互与交互之间的时间间隔。</p>
<ul>
<li>  <strong>method作者通过什么理论/模型来解决这个问题？</strong></li>
</ul>
<p>在序列模型的结构中显式建模交互的时间戳（timestamps），并且探索不同时间间隔对next item推荐的影响。提出TiSASRec模型，模型建模了item在序列中的绝对位置以及交互之间的时间间隔。</p>
<ul>
<li>  <strong>answer作者给出的答案是什么？</strong></li>
</ul>
<p>展示了不同设定下TiSASRec的特点，比较了不同位置编码下自注意力模块的表现。在dense和sparse数据集都取得了SOTA。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>  <strong>why作者为什么研究这个课题？</strong></li>
</ul>
<p>Temporal recommendation（实时推荐）主要建模“绝对时间”来捕获用户与物品的实时动态，即挖掘实时模式、依据时间建模。Sequential recommendation（序列推荐）主要依据交互的顺序挖掘序列模式。序列推荐只用timestamps来决定item顺序，其实假设了所有交互之间是等间隔的。但一天之内产生的序列和一个月内产生的序列显然对next item的影响区别很大。</p>
<ul>
<li>  <strong>how当前研究到了哪一阶段</strong></li>
</ul>
<p>目前的序列推荐只挖掘序列模式，即假设交互之间是等间隔的，不合理。有模型使用自注意力+相对位置编码[1]，受到启发。</p>
<ul>
<li>  <strong>what作者基于什么样的假设（看不懂最后去查）</strong></li>
</ul>
<p>交互序列应该被建模为包含时间间隔的序列。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li><p><strong>优点</strong></p>
<ul>
<li>  结合了<strong>绝对位置编码</strong>和<strong>相对时间间隔</strong>编码的优点。</li>
<li>  证明了使用相对时间间隔的有效性。</li>
</ul>
</li>
<li><p>  <strong>缺点</strong></p>
</li>
<li><p>  <strong>展望</strong></p>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li><p><strong>数据来源</strong></p>
<ul>
<li>  MovieLens-1m</li>
<li>  Amazon CDs&amp;Vinyl/ Movies&amp;TV/ Beauty/ Games</li>
<li>  Steam</li>
</ul>
</li>
<li><p><strong>重要指标</strong></p>
<ul>
<li>  Hit@10、NDCG@10</li>
</ul>
</li>
</ul>
<h2 id="Method-amp-Table"><a href="#Method-amp-Table" class="headerlink" title="Method &amp; Table"></a>Method &amp; Table</h2><ul>
<li>模型架构</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211116223143274.png"></p>
<ul>
<li>参数说明</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211116223530787.png" alt="image-20211116223530787"></p>
<p><strong>1.个性化的时间间隔（time interval）</strong></p>
<p>规定了序列$S$的maxlen（n），长度小于n的序列用一个特殊标记的padding item来padding。时间序列$T$用第一个item的timestamps来padding（到这里还只是时间戳）。</p>
<p>对每个用户制定个性化的时间间隔，个性化指的其实就是下面介绍的缩放操作。对用户$u$来说，时间戳序列$t=(t_1,t_2,…,t_n)$，用任意两个物品的时间戳之差表示物品之间的时间间隔，作为任意两个物品之间的关系（relation）$r_{ij}$，于是得到时间间隔集合$R^u$。规定一个缩放系数$r^u_{min}=min(R^u)$，即序列里的最小时间间隔，再对所有时间间隔缩放$r^u_{ij}=\lfloor\frac{|r_i-r_j|}{r^u_{min}}\rfloor$，得到时间间隔矩阵$M^u\in N^{n\times n}$。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211116225152019.png" alt="image-20211116225152019"></p>
<p>另外，论文还规定了每个$r^u_{ij}$的阈值，对大于阈值的做了一个clip操作得到$M^u_{clipped}$。</p>
<p><strong>2.Embedding层</strong></p>
<ul>
<li>item的表示：padding item用$\vec0$表示，其它每个item用d维向量表示，构成$M^I\in R^{|I|\times d}$的item embedding矩阵，则前n个item的表示为$E^I\in R^{n\times d}$。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117222616679.png" alt="image-20211117222616679"></p>
<ul>
<li>position 的表示：即位置编码，用两个K、V矩阵$M^P_K\in R^{n\times d}$和$M^P_V\in R^{n\times d}$，表示每个位置（序列最大长度为n）的Key和Value向量，分别为$E^P_K\in R^{n\times d}$和$E^P_V\in R^{n\times d}$。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117081419839.png" alt="image-20211117081419839"></p>
<ul>
<li>relative time interval的表示：和positional embedding相似，用两个K、V矩阵$E^P_K\in R^{k\times d}$和$E^P_V\in R^{k\times d}$，表示每个位置（序列最大长度为n）的Key和Value向量，其中k表示一共有k种相对时间间隔。于是clipped后的$M^u_{clipped}$，把对应的$r_{ij}$替换成对应的K、V向量，就得到了$E^R_K\in R^{n\times n\times d}$和$E^R_V\in R^{n\times n\times d}$。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117081413089.png" alt="image-20211117081413089"></p>
<p><strong>3.时间间隔感知的自注意力机制</strong></p>
<p>仅有item和对应的时间戳也不能把序列确定下来，还要加入item在序列中的位置。</p>
<ul>
<li><strong>时间间隔感知的自注意力层Time Interval-Aware Self-attention Layer</strong></li>
</ul>
<p>传统的自注意力层为QKV模式，可以定义成$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$。用item embedding乘以$W^Q、W^K、W^V$投影到其对应的$Querry、Key、Value$空间上。</p>
<p>这里本质上也是这么做的，但对K和V做了一点改变。</p>
<p>作者首先将$E^I=(m_{s_1},m_{s_2},…,m_{s_n})$表示的item序列变换新序列$Z=(z_1,z_2,…,z_n)$，$z_i\in R^d$。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117222727853.png" alt="image-20211117222727853"></p>
<p>其中$(m_{s_j}W^V+r_{ij}^v+p_j^v)$**对应QKV模式里的$Value$**，其中$m_{s_j}W^Q$是将item embedding投影到$Value$空间，不过在此基础上还加上了realation embedding（相对时间间隔）和position embedding（位置编码）的value表示。</p>
<p>找到$Value$以后，公式就可以写成：$z_i=\sum^n_{j=1}\alpha_{ij}\ Value_j$。</p>
<p>系数$\alpha_{ij}$是其实就是$softmax(\frac{QK^T}{\sqrt{d_k}})$部分。$softmax()$在论文中体现在$\alpha_{ij}=\frac{exp\ e_{ij}}{\sum^n_{k=1}exp\ e_{ik}}$，那么可以猜测$\frac{QK^T}{\sqrt{d_k}}$就对应论文中的$e_{ij}$了。事实正如此，$e_{ij}$被定义为：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117090739720.png" alt="image-20211117090739720"></p>
<p>其中$m_{s_j}W^Q$是将item embedding投影到$Querry$空间，**对应QKV模式里的$Querry$**。</p>
<p>$(m_{s_j}W^K+r^k_{ij}+p^k_j)$，**对应QKV模式里的$Key$**，其中$m_{s_j}W^K$是将item embedding投影到$Key$空间，不过在此基础上还加上了realation embedding（相对时间间隔）和position embedding（位置编码）的key表示，另外除以的$\sqrt{d}$是缩放系数。</p>
<ul>
<li><strong>因果关系Causality</strong></li>
</ul>
<p>序列本身就有因果关系，因为我们在预测第t+1个物品时，只知道前t个物品的信息。但是在做self-attention时，每个物品都能感知到所有物品（因为Q对所有K做了查询），破坏了因果关系。所以我们必须规定，在做self-attention时，规定每个$Q_i$只能查询$K_j$，其中$j&lt;i$，即每个Q只能查询在其之前（previous）的K，满足了因果关系，代码里可以用mask实现。</p>
<ul>
<li><strong>前馈层Point-wise Feef-Forward Network</strong></li>
</ul>
<p>FFN为模型加入非线性性。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117093429227.png" alt="image-20211117093429227"></p>
<p>Residual connection和dropout正则化。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117093655881.png" alt="image-20211117093655881"></p>
<p>Layer Norm正则化。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117093809122.png" alt="image-20211117093809122"></p>
<p><strong>4.预测层</strong></p>
<p>常规的点积计算每个物品的得分。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117094112160.png" alt="image-20211117094112160"></p>
<p><strong>5.模型训练</strong></p>
<p>取物品序列$\widetilde{S_{|S^u|}}=(S^u_1,S^u_2,…,S^u_{|S^u|})$和对应的时间序列$\widetilde{T_{|T^u|}}=(T^u_1,T^u_2,…,T^u_{|T^u|})$的前$|S^u|-1$项，即$\widetilde{S_{|S^u|-1}}=(S^=u_1,S^u_2,…,S^u_{|S^u|-1})$和$\widetilde{T_{|T^u|-1}}=(T^u_1,T^u_2,…,T^u_{|T^u-1|})$。通过裁剪和补长各自化成成相同长度n的两个序列$s=(s_1,s_2,…,s_n)$，和$t=(t_1,t_2,…,t_n)$。给定这两个序列，再规定对应的输出序列$o=(o_1,o_2,…,o_n)$，其中$o_i$定义为：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117095415283.png" alt="image-20211117095415283"></p>
<p>简而言之，padding项的输出为&lt;pad&gt;；$s$最后一项之前的输出（预测）为下一项；$s$最后一项的输出（预测）为$S^u_{|S^u|}$，注意$S^u_{|S^u|}$不在$s$里，因为一开始就把最后一项拿出来了。</p>
<p>loss采用进行负采样的binary cross entropy，加入了F正则项：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117100703091.png" alt="image-20211117100703091"></p>
<p>padding项也计算了loss，但是没有意义，所以实际计算时把padding项的loss mask掉。</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><p><strong>1.模型表现</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117101056812.png" alt="image-20211117101056812"></p>
<ul>
<li>TiSASRec在6个数据集上达到了SOTA</li>
</ul>
<p><strong>2.Ablation Study</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117103132511.png" alt="image-20211117103132511"></p>
<ul>
<li>TiSASRec-R去掉了在K和V里去掉了position embedding（绝对位置）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117103534252.png" alt="image-20211117103534252"></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117103553274.png" alt="image-20211117103553274"></p>
<ul>
<li>SASRec去掉了relative time interval（relation，相对时间间隔）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117114708312.png" alt="image-20211117114708312"></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117114729528.png" alt="image-20211117114729528"></p>
<ul>
<li>结果表明保留绝对位置和相对时间间隔时model performance最好</li>
</ul>
<p><strong>3.超参数实验</strong></p>
<p><strong>A.隐向量维度d</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117104027269.png" alt="image-20211117104027269"></p>
<ul>
<li>不同模型在不同数据集（<em>除了Games和Steam，why？</em>）上选择d={10，20，30，40，50}</li>
<li>基本上所给模型在所给数据集上都是d越大越好</li>
<li>在Beauty数据集比较特殊，MARank、Caser、TransRec的表现随着d增大在变差</li>
<li>所以最后选d=50</li>
</ul>
<p><strong>B. 序列最大长度n</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117104705615.png" alt="image-20211117104705615"></p>
<ul>
<li>n越大效果越好，并且在这两个数据集上SASRec表现比TiSASRec差且更快收敛。</li>
<li>所以最后选n=50</li>
<li><em>疑问：只选了MovieLens和Amazon CD&amp;Vinyl做实验，why？SASRec论文里选MovieLens-1m做实验的时候maxlen选的可是200，且performance比TiSASRec选50时好….这篇论文maxlen选的最大才50，why？</em></li>
</ul>
<p><strong>C.最大时间间隔k</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117105425551.png" alt="image-20211117105425551"></p>
<ul>
<li>k越大意味着要训练的参数越多</li>
<li>TiSASRec整体上更稳定，TiSASRec-R当k取合适时表现最好，但当k更大时表现变差。</li>
<li><em>疑问：ml-1m上比较稳定且permformance在提升，到最大值2048。但CD&amp;Vinyl上最好表现是k=256，但论文最后选的k=512</em></li>
</ul>
<p><strong>4.个性化时间间隔实验</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117110513024.png" alt="image-20211117110513024"></p>
<ul>
<li>Method（1）直接用时间戳作为特征，Method（2）使用没缩放的时间间隔，Method（3）使用个性化（根据每个用户最小时间间隔缩放后的）的时间间隔，即论文方法。</li>
<li>注意前两个方法没有使用时间戳裁剪 timestamps clip</li>
<li>Method（3）的performence最好</li>
</ul>
<p><strong>5.可视化</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117111955529.png" alt="image-20211117111955529"></p>
<ul>
<li>Figure 7 表明预测时使用时间间隔产生的推荐不一样，而且好像更准确</li>
<li>Figure 8 是不同时间间隔的权重可视化<ul>
<li>小时间间隔的权重更大，说明更短期交互的物品对预测结果影响更大</li>
<li>(a)MovieLens是dense数据集，(b) CDs&amp;Vinyl是sparse数据集。左边绿的区域更大，说明dense数据集上预测需要更大范围的物品。</li>
<li>Amazon Beauty数据集没有明显的黄绿区域，说明这个数据集没有明显的序列模式，这也说明了为什么有些序列模型在该数据集上效果不是很好。</li>
</ul>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://aclanthology.org/N18-2074/">[1]Self-Attention with Relative Position Representations</a>)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>序列推荐</tag>
        <tag>自注意力</tag>
        <tag>TiSASRec</tag>
      </tags>
  </entry>
  <entry>
    <title>TiSASRec代码笔记</title>
    <url>/2021/11/22/TiSASRec%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<hr>
<p>完整的代码注释：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec</a></p>
<p>论文笔记：<a href="https://guadzilla.github.io/2021/11/18/TiSASRec/">https://guadzilla.github.io/2021/11/18/TiSASRec/</a></p>
<hr>
<h2 id="squeeze-unsqueeze-repeat-expand"><a href="#squeeze-unsqueeze-repeat-expand" class="headerlink" title="squeeze, unsqueeze, repeat ,expand"></a>squeeze, unsqueeze, repeat ,expand</h2><p><strong>torch.squeeze(input,dim,*,out) —&gt;Tensor</strong></p>
<blockquote>
<p><em>squeeze：挤压，捏</em></p>
</blockquote>
<p>与unsqueeze操作相反，在指定dim处加入一维，如果dim未指定，则所有为1的维度去掉。</p>
<p><strong>torch.unsqueeze(input,dim) —&gt; Tensor</strong></p>
<blockquote>
<p><em>unsqueeze：挤压的反义词，膨胀</em></p>
</blockquote>
<p>与squeeze操作相反，返回一个新张量，在原来张量的指定dim处加入一维。</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],		<span class="comment"># x.shape=(2,4) ,有三处可以插入维度 _,1,_,4,_</span></span><br><span class="line">                  [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]]) 	</span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">0</span>).shape			<span class="comment"># (_,2,_,4,_),在第0维度（最左边）插入1维 = (1,2,4)</span></span><br><span class="line"><span class="comment"># torch.Size([1, 2, 4])</span></span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">1</span>).shape			<span class="comment"># (_,2,_,4,_),在第1维度（中间的）插入1维 = (2,1,4)</span></span><br><span class="line"><span class="comment"># torch.Size([2, 1, 4])</span></span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">2</span>).shape			<span class="comment"># (_,2,_,4,_),在第2维度（最右边）插入1维 = (2,1,4)</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 1])</span></span><br><span class="line"></span><br><span class="line">y = torch.unsqueeze(x, -<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)		<span class="comment"># 在最后填两个为1的维度</span></span><br><span class="line">y.shape</span><br><span class="line"><span class="comment"># torch.Size([2, 4, 1, 1])		</span></span><br><span class="line">y.squeeze().shape					<span class="comment"># squeeze不指定dim，会去掉所有size=1的维度</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4])</span></span><br></pre></td></tr></table></figure>

<p>*<em>torch.repeat(<em>size)</em></em></p>
<p>沿着指定的维度重复这个张量。类似numpy.tile()，地板铺（把tensor当成一块地板，按形状铺）。</p>
<p>*<em>torch.expand(<em>sizes)</em></em></p>
<p>将单个维度<strong>拓展</strong>成更大维度，和repeat不一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">x</span><br><span class="line"><span class="comment"># tensor([1, 2, 3])</span></span><br><span class="line">x.repeat(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],		# x作为地板，被重复铺了(2,3)次</span></span><br><span class="line"><span class="comment">#         [1, 2, 3, 1, 2, 3, 1, 2, 3]])</span></span><br><span class="line">x.expand(<span class="number">2</span>,<span class="number">3</span>)		</span><br><span class="line"><span class="comment"># tensor([[1, 2, 3],						# x被拓展成(2,3)</span></span><br><span class="line"><span class="comment">#         [1, 2, 3]])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>实际代码：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time_mask = time_mask.unsqueeze(-<span class="number">1</span>).repeat(self.head_num, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 1.unsqueeze()：time_mask.shape=(batch_size,maxlen) ——&gt; (batch_size,maxlen,1)，最后一个维度填1</span></span><br><span class="line"><span class="comment"># 2.repeat():(batch_size,maxlen,1) ——&gt; (self.head_num*batch_size,maxlen,1),第一个维度乘倍数</span></span><br><span class="line">time_mask = time_mask.expand(-<span class="number">1</span>, -<span class="number">1</span>, attn_weights.shape[-<span class="number">1</span>])	<span class="comment"># 这里attn_weights.shape[-1]=maxlen</span></span><br><span class="line"><span class="comment"># 3.(self.head_num*batch_size,maxlen,1) ——&gt;(self.head_num*batch_size,maxlen,maxlen)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">attn_mask = attn_mask.unsqueeze(<span class="number">0</span>).expand(attn_weights.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"><span class="comment"># (maxlen,maxlen) ——&gt; (1,maxlen,maxlen) ——&gt; (batch_size,maxlen,maxlen)</span></span><br></pre></td></tr></table></figure>



<h2 id="手动多头注意力"><a href="#手动多头注意力" class="headerlink" title="手动多头注意力"></a>手动多头注意力</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeAwareMultiHeadAttention</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># required homebrewed mha layer for Ti/SASRec experiments</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size, head_num, dropout_rate, dev</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TimeAwareMultiHeadAttention, self).__init__()</span><br><span class="line">        self.Q_w = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.K_w = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.V_w = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = torch.nn.Dropout(p=dropout_rate)</span><br><span class="line">        self.softmax = torch.nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.head_num = head_num</span><br><span class="line">        self.head_size = hidden_size // head_num</span><br><span class="line">        self.dropout_rate = dropout_rate</span><br><span class="line">        self.dev = dev</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, time_mask, attn_mask, time_matrix_K, time_matrix_V, abs_pos_K, abs_pos_V</span>):</span></span><br><span class="line">        <span class="comment"># time_mask: padding item的mask,     attn_mask: 为了causality的mask,下三角</span></span><br><span class="line">        Q, K, V = self.Q_w(queries), self.K_w(keys), self.V_w(keys)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># head dim * batch dim for parallelization (h*N, T, C/h)</span></span><br><span class="line">        <span class="comment"># 即(batch_size, maxlen, hidden_units) ----&gt; (batch_size*3, maxlen, hidden_units/3)</span></span><br><span class="line">        <span class="comment">#   (batch_size, maxlen, maxlen, hidden_units) ----&gt; (batch_size*3, maxlen, maxlen, hidden_units/3)</span></span><br><span class="line">        Q_ = torch.cat(torch.split(Q, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">        K_ = torch.cat(torch.split(K, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">        V_ = torch.cat(torch.split(V, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        time_matrix_K_ = torch.cat(torch.split(time_matrix_K, self.head_size, dim=<span class="number">3</span>), dim=<span class="number">0</span>)</span><br><span class="line">        time_matrix_V_ = torch.cat(torch.split(time_matrix_V, self.head_size, dim=<span class="number">3</span>), dim=<span class="number">0</span>)</span><br><span class="line">        abs_pos_K_ = torch.cat(torch.split(abs_pos_K, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">        abs_pos_V_ = torch.cat(torch.split(abs_pos_V, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># batched channel wise matmul to gen attention weights  ---公式（8）</span></span><br><span class="line">        attn_weights = Q_.matmul(torch.transpose(K_, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        attn_weights += Q_.matmul(torch.transpose(abs_pos_K_, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        attn_weights += time_matrix_K_.matmul(Q_.unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># seq length adaptive scaling   ---公式（8）</span></span><br><span class="line">        attn_weights = attn_weights / (K_.shape[-<span class="number">1</span>] ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># key masking, -2^32 lead to leaking, inf lead to nan</span></span><br><span class="line">        <span class="comment"># 0 * inf = nan, then reduce_sum([nan,...]) = nan</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># time_mask = time_mask.unsqueeze(-1).expand(attn_weights.shape[0], -1, attn_weights.shape[-1])</span></span><br><span class="line">        <span class="comment"># 会报错，必须按下面的1.2.3.</span></span><br><span class="line">        time_mask = time_mask.unsqueeze(-<span class="number">1</span>).repeat(self.head_num, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1.unsqueeze()：time_mask.shape=(batch_size,maxlen) ——&gt; (batch_size,maxlen,1),最后一个维度填1</span></span><br><span class="line">        <span class="comment"># 2.repeat():(batch_size,maxlen,1) ——&gt; (self.head_num*batch_size,maxlen,1),第一个维度乘倍数</span></span><br><span class="line">        time_mask = time_mask.expand(-<span class="number">1</span>, -<span class="number">1</span>, attn_weights.shape[-<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 3.(self.head_num*batch_size,maxlen,1) ——&gt;(self.head_num*batch_size,maxlen,maxlen)</span></span><br><span class="line">        <span class="comment"># tips：attn_weights= (B,maxlen,maxlen),每个batch中size=(maxlen,maxlen)，每行表示某个item对其它所有item的atten矩阵</span></span><br><span class="line">        <span class="comment">#      time_mask是对padding的item做mask,本来是(B,maxlen,1),每个batch中size=(maxlen,1)</span></span><br><span class="line">        <span class="comment">#      expand成(B,maxlen,maxlen)才能把attn里padding的物品，即对应行都mask掉</span></span><br><span class="line"></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">0</span>).expand(attn_weights.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (maxlen,maxlen) ——&gt; (1,maxlen,maxlen) ——&gt; (batch_size,maxlen,maxlen)</span></span><br><span class="line">        <span class="comment"># padding取负无穷是因为底下要用softmax，以e为底的负无穷接近0</span></span><br><span class="line">        paddings = torch.ones(attn_weights.shape) *  (-<span class="number">2</span>**<span class="number">32</span>+<span class="number">1</span>) <span class="comment"># -1e23 # float(&#x27;-inf&#x27;),</span></span><br><span class="line">        paddings = paddings.to(self.dev)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这两步一起为了mask掉不用的attention计算，第一步是mask掉padding的items，第二是为了因果关系mask掉afterwards的items</span></span><br><span class="line">        attn_weights = torch.where(time_mask, paddings, attn_weights) <span class="comment"># True:pick padding</span></span><br><span class="line">        attn_weights = torch.where(attn_mask, paddings, attn_weights) <span class="comment"># enforcing causality</span></span><br><span class="line"></span><br><span class="line">        attn_weights = self.softmax(attn_weights)   <span class="comment"># ---公式（7）</span></span><br><span class="line">        attn_weights = self.dropout(attn_weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---公式（6），把alpha放进去乘了</span></span><br><span class="line">        outputs = attn_weights.matmul(V_)</span><br><span class="line">        outputs += attn_weights.matmul(abs_pos_V_)</span><br><span class="line">        outputs += attn_weights.unsqueeze(<span class="number">2</span>).matmul(time_matrix_V_).reshape(outputs.shape)<span class="comment">#.squeeze(2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (num_head * N, T, C / num_head) -&gt; (N, T, C)</span></span><br><span class="line">        outputs = torch.cat(torch.split(outputs, Q.shape[<span class="number">0</span>], dim=<span class="number">0</span>), dim=<span class="number">2</span>) <span class="comment"># div batch_size</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>代码阅读</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>TiSASRec</tag>
      </tags>
  </entry>
  <entry>
    <title>（待更新）推荐系统：经典算法——协同过滤（Collebrative Filtering）</title>
    <url>/2021/11/23/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/</url>
    <content><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>经典Movielens数据集</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">All ratings are contained in the file &quot;ratings.dat&quot; and are in the</span><br><span class="line">following format:</span><br><span class="line"></span><br><span class="line">UserID::MovieID::Rating::Timestamp</span><br><span class="line"></span><br><span class="line">- UserIDs range between 1 and 6040 </span><br><span class="line">- MovieIDs range between 1 and 3952</span><br><span class="line">- Ratings are made on a 5-star scale (whole-star ratings only)</span><br><span class="line">- Timestamp is represented in seconds since the epoch as returned by time(2)</span><br><span class="line">- Each user has at least 20 ratings</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><p>采用K-fold交叉验证，将用户行为数据均匀分成K份，其中一份作为测试集，K-1份作为训练集。协同过滤算法只考虑物品/用户的共现关系，所以用户序列都用集合表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span>(<span class="params">data, M, k, seed</span>):</span></span><br><span class="line">    test = []</span><br><span class="line">    train = []</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">0</span>, M) == k:</span><br><span class="line">            test.append([user, item])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train.append([user, item])</span><br><span class="line">    train_ = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">    test_ = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> train:</span><br><span class="line">        train_[user].add(item)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> test:</span><br><span class="line">        test_[user].add(item)</span><br><span class="line">    <span class="keyword">return</span> train_, test_</span><br></pre></td></tr></table></figure>

<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>召回率Recall，准确率Precision，覆盖率Coverage，新颖度Popularity。</p>
<span id="more"></span>

<p>召回率Recall：正确推荐的商品占所有应该推荐的商品的比例，即应该推荐的推荐了多少。公式描述：对用户u推荐N个物品（$R(u)$），令用户在测试集上喜欢的物品集合为$T(u)$，则<br>$$<br>Recall=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |T(u)|}<br>$$<br>准确率Precision：正确推荐的商品占推荐的商品列表的比例，即有多少推荐对了。公式描述：<br>$$<br>Precision=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |R(u)|}<br>$$<br>覆盖率Coverage：推荐的商品占所有商品的比例，即推荐的商品覆盖了多少所有商品。反映发掘长尾的能力。<br>$$<br>Coverage = \frac{\bigcup_u R(u)}{|I|} \ \  , \ \bigcup:并集<br>$$<br>新颖度Popularity：刻画推荐物品的平均流行度，平均流行度（Popularity）越高，新颖度越低。$Popularity(x)$定义为$x$在所有用户序列中出现的次数，出现次数越多，流行度越高。<br>$$<br>Popularity= \sum _u \sum _ { i \in R(u) } \log (Popularity(i)+1)<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评价指标:召回率、准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Metric</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    hit = <span class="number">0</span></span><br><span class="line">    recall_all = <span class="number">0</span>      <span class="comment"># recall 的分母</span></span><br><span class="line">    precision_all = <span class="number">0</span>   <span class="comment"># precision 的分母</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        tu = test[user]</span><br><span class="line">        rank = all_recommend_list[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> tu:</span><br><span class="line">                hit += <span class="number">1</span></span><br><span class="line">        recall_all += <span class="built_in">len</span>(tu)</span><br><span class="line">        precision_all += N</span><br><span class="line">    recall = hit / (recall_all * <span class="number">1.0</span>)</span><br><span class="line">    precision = hit / (precision_all * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> recall, precision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：覆盖率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Coverage</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    recommend_items = <span class="built_in">set</span>()</span><br><span class="line">    all_items = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> train[user]:</span><br><span class="line">            all_items.add(item)</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            recommend_items.add(item)</span><br><span class="line">    coverage = <span class="built_in">len</span>(recommend_items) / (<span class="built_in">len</span>(all_items) * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> coverage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：新颖度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Popularity</span>(<span class="params">train, test, N, recommend_res</span>):</span>	<span class="comment"># N:推荐N个物品</span></span><br><span class="line">    item_popularity = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_popularity:</span><br><span class="line">                item_popularity[item] = <span class="number">0</span></span><br><span class="line">            item_popularity[item] += <span class="number">1</span></span><br><span class="line">    popularity = <span class="number">0</span></span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        rank = recommend_res[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            popularity += math.log(<span class="number">1</span> + item_popularity[item])</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    popularity /= n * <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> popularity</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>推荐系统实战</category>
      </categories>
  </entry>
  <entry>
    <title>模板</title>
    <url>/2021/10/17/%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[<hr>
<p>原paper：</p>
<p>源码解读：</p>
<hr>
<p>中译：</p>
<p>总结：</p>
<hr>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>question作者想解决什么问题？ </li>
</ul>
<ul>
<li>method作者通过什么理论/模型来解决这个问题？</li>
</ul>
<ul>
<li>answer作者给出的答案是什么？</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>why作者为什么研究这个课题？    </li>
</ul>
<ul>
<li>how当前研究到了哪一阶段？</li>
</ul>
<ul>
<li>what作者基于什么样的假设（看不懂最后去查）？</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>优点 </li>
</ul>
<ul>
<li>缺点</li>
</ul>
<ul>
<li>展望</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 </li>
</ul>
<ul>
<li>重要指标 </li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li></li>
<li></li>
</ul>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><ul>
<li></li>
<li></li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><h2 id="一些备注"><a href="#一些备注" class="headerlink" title="一些备注"></a>一些备注</h2><p>数学公式不能正确显示的情况：<br>1.<br>$$<br>\mathit{L}=\mathit{L}<em>{cr} + \lambda_1\mathit{L}</em>{in}+\lambda_2||\Theta||^2_2<br>$$</p>
<p>在每个下划线 “ _ ” 前后各加一个空格就好了</p>
<p>$$<br>\mathit{L}=\mathit{L} _ {cr} + \lambda_1\mathit{L} _ {in}+\lambda_2||\Theta||^2 _ 2<br>$$</p>
<p>2.</p>
<p><img src="https://gitee.com/Guadzilla/img-hosting/raw/master/image-20211118185724771.png" alt="image-20211118185724771"></p>
]]></content>
  </entry>
  <entry>
    <title>推荐系统基础1</title>
    <url>/2022/04/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%801/</url>
    <content><![CDATA[<hr>
<h1 id="任务1：推荐系统基础"><a href="#任务1：推荐系统基础" class="headerlink" title="任务1：推荐系统基础"></a>任务1：推荐系统基础</h1><ul>
<li>阅读推荐系统在工业落地的链接：<ul>
<li><a href="https://mp.weixin.qq.com/s/WXcfdzz7vts9UYBVxWs3AA">推荐系统整体架构及算法流程详解</a></li>
<li><a href="https://tech.meituan.com/2017/03/24/travel-recsys.html">美团旅游推荐系统的演进</a></li>
<li><a href="https://www.alibabacloud.com/zh/product/airec">阿里智能推荐AIRec</a></li>
</ul>
</li>
<li>思考 &amp; 回答以下问题，并将回答记录到博客<ul>
<li>推荐系统与常见的结构化问题的区别是什么？</li>
<li>如何评价推荐系统「推荐」的准不准？</li>
<li>推荐系统一般分为召回 &amp; 排序，为什么这样划分？</li>
</ul>
</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/recpre">https://github.com/Guadzilla/recpre</a></p>
<span id="more"></span>

<h2 id="推荐系统与常见的结构化问题的区别是什么？"><a href="#推荐系统与常见的结构化问题的区别是什么？" class="headerlink" title="推荐系统与常见的结构化问题的区别是什么？"></a>推荐系统与常见的结构化问题的区别是什么？</h2><p>结构化数据即表格数据（tabular data），绝大多数数据都是表格数据。</p>
<p>推荐系统有user信息，item信息，这些数据虽然都是结构化的，但是推荐系统涉及到user和item的交互，所以不仅仅是一条结构化数据预测一个分类or一个数值那么简单，还需要从交互中抽象出用户兴趣，例如将用户交互建模为序列，这就不是结构化问题了。</p>
<h2 id="如何评价推荐系统「推荐」的准不准？"><a href="#如何评价推荐系统「推荐」的准不准？" class="headerlink" title="如何评价推荐系统「推荐」的准不准？"></a>如何评价推荐系统「推荐」的准不准？</h2><p>常用的评价指标有：召回率Recall，准确率Precision，覆盖率Coverage，新颖度Popularity。</p>
<p>召回率Recall：正确推荐的商品占所有应该推荐的商品的比例，即应该推荐的推荐了多少。公式描述：对用户u推荐N个物品（$R(u)$），令用户在测试集上喜欢的物品集合为$T(u)$，则<br>$$<br>Recall=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |T(u)|}<br>$$<br>准确率Precision：正确推荐的商品占推荐的商品列表的比例，即有多少推荐对了。公式描述：<br>$$<br>Precision=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |R(u)|}<br>$$<br>覆盖率Coverage：推荐的商品占所有商品的比例，即推荐的商品覆盖了多少所有商品。反映发掘长尾的能力。<br>$$<br>Coverage = \frac{\bigcup_u R(u)}{|I|} \ \  , \ \bigcup:并集<br>$$<br>新颖度Popularity：刻画推荐物品的平均流行度，平均流行度（Popularity）越高，新颖度越低。$Popularity(x)$定义为$x$在所有用户序列中出现的次数，出现次数越多，流行度越高。<br>$$<br>Popularity= \sum _u \sum _ { i \in R(u) } \log (Popularity(i)+1)<br>$$</p>
<p>AUC曲线：AUC（Area Under Curve），ROC曲线下与坐标轴围成的面积。在讲AUC前需要理解混淆矩阵，召回率，精确率，ROC曲线等概念。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/v2-a253b01cf7f141b9ad11eefdf3cf58d3_1440w.jpg" alt="img"></p>
<p>根据混淆矩阵的定义，以另一种形式定义召回率和精确率：<br>$$<br>Recall = \frac{TP}{TP+FN} \<br>Precision = \frac{TP}{TP+FP}<br>$$<br>ROC曲线的横坐标为假阳性率（False Positive Rate, FPR），$FPR=\frac{FP}{FP+TN}$，N是真实负样本的个数， FP是N个负样本中被分类器预测为正样本的个数。<strong>FPRate的意义是所有真实类别为0的样本中，预测类别为1的比例。</strong></p>
<p>纵坐标为真阳性率（True Positive Rate, TPR），$TPR=\frac{TP}{TP+FN}$，P是真实正样本的个数，TP是P个正样本中被分类器预测为正样本的个数。<strong>TPRate的意义是所有真实类别为1的样本中，预测类别为1的比例。</strong></p>
<p><img src="https://camo.githubusercontent.com/07a924ef2229334f903f1ba3e5cd17115a16159dcb1756cda93232b3cf998c0d/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f4a6176616175632e706e67" alt="img"></p>
<p>AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。对ROC更细致的解释：<a href="https://www.zhihu.com/question/39840928/answer/241440370">如何理解机器学习和统计中的AUC？ - 知乎 (zhihu.com)</a></p>
<p>下面是部分评价指标的代码实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评价指标:召回率、准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Metric</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    hit = <span class="number">0</span></span><br><span class="line">    recall_all = <span class="number">0</span>      <span class="comment"># recall 的分母</span></span><br><span class="line">    precision_all = <span class="number">0</span>   <span class="comment"># precision 的分母</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        tu = test[user]</span><br><span class="line">        rank = all_recommend_list[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> tu:</span><br><span class="line">                hit += <span class="number">1</span></span><br><span class="line">        recall_all += <span class="built_in">len</span>(tu)</span><br><span class="line">        precision_all += N</span><br><span class="line">    recall = hit / (recall_all * <span class="number">1.0</span>)</span><br><span class="line">    precision = hit / (precision_all * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> recall, precision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：覆盖率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Coverage</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    recommend_items = <span class="built_in">set</span>()</span><br><span class="line">    all_items = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> train[user]:</span><br><span class="line">            all_items.add(item)</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            recommend_items.add(item)</span><br><span class="line">    coverage = <span class="built_in">len</span>(recommend_items) / (<span class="built_in">len</span>(all_items) * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> coverage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：新颖度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Popularity</span>(<span class="params">train, test, N, recommend_res</span>):</span>	<span class="comment"># N:推荐N个物品</span></span><br><span class="line">    item_popularity = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_popularity:</span><br><span class="line">                item_popularity[item] = <span class="number">0</span></span><br><span class="line">            item_popularity[item] += <span class="number">1</span></span><br><span class="line">    popularity = <span class="number">0</span></span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        rank = recommend_res[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            popularity += math.log(<span class="number">1</span> + item_popularity[item])</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    popularity /= n * <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> popularity</span><br></pre></td></tr></table></figure>





<h2 id="推荐系统一般分为召回-amp-精排，为什么这样划分？"><a href="#推荐系统一般分为召回-amp-精排，为什么这样划分？" class="headerlink" title="推荐系统一般分为召回 &amp; 精排，为什么这样划分？"></a>推荐系统一般分为召回 &amp; 精排，为什么这样划分？</h2><blockquote>
<p>物品集量级非常大，召回先选出一部分候选物品，再对这一部分候选物品做精排，计算开销相比于对所有物品做精排大大降低了。</p>
</blockquote>
<p>相关知识点：召回、排序（精排）。</p>
<h3 id="召回"><a href="#召回" class="headerlink" title="召回"></a>召回</h3><p>从海量的物品中，先筛选出一小部分物品作为推荐的候选集。</p>
<h4 id="召回的目的"><a href="#召回的目的" class="headerlink" title="召回的目的"></a>召回的目的</h4><p>当用户和物品量比较大时，如果直接精排（计算预测得分）复杂度会非常高。计算预测得分通常是计算用户和物品的向量内积，假设 user 和 item 的 embedding 维度都是 D ，用户数为 M ，物品数为 N ，那么计算这个得分的复杂度就是 $O(D^2) *O(MN)$。当 M 和 N 都是百万量级、亿量级时，计算开销会非常大。</p>
<p>如果可以先从海量的物品中，先筛选出一小部分用户最可能喜欢的物品（召回），例如先选出 N/100 的物品，那么复杂度就是 $\frac{O(D^2) *O(MN)}{100}$ ，降低为原来的一百分之一，计算效率更高了。实际场景中，做热销召回的量级可能是百级，这样一来从百万量级的物品数降低到百量级的物品数，计算开销大大降低！另一方面，大量内容中真正的精品只是少数，对所有内容都计算将非常的低效，会浪费大量资源和时间。</p>
<h4 id="召回的重要性"><a href="#召回的重要性" class="headerlink" title="召回的重要性"></a>召回的重要性</h4><p>虽然精排模型一直是优化的重点，但召回模型也非常的重要，因为如果召回的内容不对，怎么精排都是错误的。</p>
<h4 id="召回的方法"><a href="#召回的方法" class="headerlink" title="召回的方法"></a>召回的方法</h4><ol>
<li>热销召回：将一段时间内的热门内容召回。</li>
<li>协同召回：基于用户与用户行为的相似性推荐，可以很好的突破一定的限制，发现用户潜在的兴趣偏好。</li>
<li>标签召回：根据每个用户的行为，构建标签，并根据标签召回内容。</li>
<li>时间召回：将一段时间内最新的内容召回，在新闻视频等有时效性的领域常用。是常见的几种召回方法。</li>
</ol>
<h4 id="多路召回"><a href="#多路召回" class="headerlink" title="多路召回"></a>多路召回</h4><p>一开始我们可能有成千上万的物品，首先要由召回（也叫触发，recall）来挖掘出原则上任何用户有可能感兴趣的东西。这个环节是入口。有时候，单独的召回可能难以做到照顾所有方面，这个时候就需要多路召回。所谓的“多路召回”策略，就是指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用。下图只是一个多路召回的例子，也就是说可以使用多种不同的策略来获取用户排序的候选商品集合，<strong>而具体使用哪些召回策略其实是与业务强相关的</strong>，针对不同的任务就会有对于该业务真实场景下需要考虑的召回规则。例如视频推荐，召回规则可以是“热门视频”、“导演召回”、“演员召回”、“最近上映“、”流行趋势“、”类型召回“等等。</p>
<img src="https://camo.githubusercontent.com/5194f61aac70bfec14ede3fa6b27aed0670f2cd59b5e9ad688f1f526a4c5f658/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731373230313431313336372e706e67237069635f63656e746572" alt="img" style="zoom:67%;" />

<h4 id="Embedding召回"><a href="#Embedding召回" class="headerlink" title="Embedding召回"></a>Embedding召回</h4><h3 id="精排"><a href="#精排" class="headerlink" title="精排"></a>精排</h3><p>排序负责将多个召回策略的结果进行个性化排序。</p>
<h4 id="精排的重要性"><a href="#精排的重要性" class="headerlink" title="精排的重要性"></a>精排的重要性</h4><p>精排是最纯粹的排序，也是最纯粹的机器学习模块。它的目标只有一个，就是<strong>根据手头所有的信息输出最准</strong>的预测。精排一直是优化的重点。召回的物品中，筛选出用户最感兴趣的物品，进一步做出个性化排序，才最终达到推荐的目的。</p>
<h4 id="精排模型"><a href="#精排模型" class="headerlink" title="精排模型"></a>精排模型</h4><img src="https://mmbiz.qpic.cn/mmbiz_jpg/icTMNdGHpfJYqcAFSwiaWKjeqTweM9aJrNKqZVvMn2GZvoDTnPHjYMVywvGicII8P9d4nMjib5Jia8kGlDbicibTGSPlQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" />

]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础2</title>
    <url>/2022/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%802/</url>
    <content><![CDATA[<hr>
<h1 id="任务2：Movielens介绍"><a href="#任务2：Movielens介绍" class="headerlink" title="任务2：Movielens介绍"></a>任务2：Movielens介绍</h1><ul>
<li>下载并读取Movielens 1M数据集（用户、电影、评分）</li>
<li>统计如下指标：<ul>
<li>总共包含多少用户？</li>
<li>总共包含多个电影？</li>
<li>平均每个用户对多少个电影进行了评分？</li>
<li>每部电影 &amp; 每个用户的平均评分是？</li>
</ul>
</li>
<li>如果你来进行划分数据集为训练和验证，你会如何划分？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/recpre">https://github.com/Guadzilla/recpre</a></p>
<span id="more"></span>

<h2 id="统计指标"><a href="#统计指标" class="headerlink" title="统计指标"></a>统计指标</h2><ul>
<li>总共包含 6040 个用户</li>
<li>总共包含 3883 部电影</li>
<li>平均每个用户对 165.6 部电影进行评分</li>
</ul>
<p>其余细节见 notebook 。</p>
<h2 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>参考项亮《推荐系统实践》，将用户行为数据集按照均匀分布随机分成 K 份，挑选一份作为测试集，剩下的 K-1 份作为训练集，进行 K 次实验，然后将 K 次评测指标的平均值作为最终评测指标 （即 K-fold 交叉验证）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span>(<span class="params">data, K, i, seed</span>):</span></span><br><span class="line">    test = []</span><br><span class="line">    train = []</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">0</span>,K)==i:</span><br><span class="line">            test.append([user,item])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train.append([user,item])</span><br><span class="line">    <span class="keyword">return</span> train,test</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础3</title>
    <url>/2022/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%803/</url>
    <content><![CDATA[<hr>
<h1 id="任务3：协同过滤基础"><a href="#任务3：协同过滤基础" class="headerlink" title="任务3：协同过滤基础"></a>任务3：协同过滤基础</h1><ul>
<li>[阅读协同过滤教程](<a href="https://github.com/datawhalechina/fun-rec/blob/master/docs/%E7%AC%AC%E4%B8%80%E7%AB%A0">https://github.com/datawhalechina/fun-rec/blob/master/docs/第一章</a> 推荐系统基础/1.1 基础推荐算法/1.1.2 协同过滤.md)</li>
<li>编写代码计算两个用户的相似度</li>
<li>编写代码计算两个物品的相似度</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/recpre">https://github.com/Guadzilla/recpre</a></p>
<span id="more"></span>

<h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>协同过滤（Collaborative Filtering）推荐算法是最经典、最常用的推荐算法。</p>
<p>所谓协同过滤， 基本思想是<strong>根据用户之前的喜好</strong>以及<strong>其他兴趣相近的用户的选择</strong>来给用户推荐物品(基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐)，<strong>一般是仅仅基于用户的行为数据（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）</strong>。目前应用比较广泛的协同过滤算法是基于邻域的方法， 而这种方法主要有下面两种算法：</p>
<ul>
<li><strong>基于用户的协同过滤算法(UserCF)</strong>: 给用户推荐和他兴趣相似的其他用户喜欢的产品</li>
<li><strong>基于物品的协同过滤算法(ItemCF)</strong>: 给用户推荐和他之前喜欢的物品相似的物品</li>
</ul>
<p>不管是UserCF还是ItemCF算法， 非常重要的步骤之一就是计算用户和用户或者物品和物品之间的<strong>相似度</strong>， 所以下面先整理常用的相似性度量方法， 然后再对每个算法的具体细节进行展开。</p>
<h2 id="相似性度量方法"><a href="#相似性度量方法" class="headerlink" title="相似性度量方法"></a>相似性度量方法</h2><h4 id="杰卡德相似系数"><a href="#杰卡德相似系数" class="headerlink" title="杰卡德相似系数"></a><strong>杰卡德相似系数</strong></h4><p>杰卡德(Jaccard)相似系数是衡量两个<strong>集合的相似度</strong>一种指标。两个用户$u$和$v$交互商品交集的数量占这两个用户交互商品并集的数量的比例，称为两个集合的杰卡德相似系数，用符号$sim_{uv}$表示，其中$N(u),N(v)$分别表示用户$u$和用户$v$交互商品的集合。 $$ sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)| \cup|N(v)|}} $$ 由于杰卡德相似系数一般无法反映具体用户的评分喜好信息， 所以常用来评估用户<strong>是否</strong>会对某商品进行打分， 而不是预估用户会对某商品打多少分。</p>
<h4 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a><strong>余弦相似度</strong></h4><p>余弦相似度(cosine similarity) 衡量了两个向量的夹角，夹角越小越相似。首先从集合的角度描述余弦相似度，相比于Jaccard公式来说就是分母有差异，不是两个用户交互商品的并集的数量，而是两个用户分别交互的商品数量的乘积，公式如下：<br>$$<br>sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}<br>$$<br>从向量的角度进行描述，令矩阵$A$为用户-商品交互矩阵(因为是TopN推荐并不需要用户对物品的评分，只需要知道用户对商品是否有交互就行)，即矩阵的每一行表示一个用户对所有商品的交互情况，有交互的商品值为1没有交互的商品值为0，矩阵的列表示所有商品。若用户和商品数量分别为$m,n$的话，交互矩阵$A$就是一个$m$行$n$列的矩阵。此时用户的相似度可以表示为(其中$u\cdot v$指的是向量点积)：<br>$$<br>sim_{uv} = cos(u,v) =\frac{u\cdot v}{|u|\cdot |v|}<br>$$<br>上述用户-商品交互矩阵在现实情况下是非常的稀疏了，为了避免存储这么大的稀疏矩阵，在计算用户相似度的时候一般会采用集合的方式进行计算。理论上向量之间的相似度计算公式都可以用来计算用户之间的相似度，但是会根据实际的情况选择不同的用户相似度度量方法。</p>
<p>这个在具体实现的时候， 可以使用<code>cosine_similarity</code>进行实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>]</span><br><span class="line">cosine_similarity([i, j])</span><br></pre></td></tr></table></figure>

<h4 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h4><p>皮尔逊相关系数的公式与余弦相似度的计算公式非常的类似，首先对于上述的余弦相似度的计算公式写成求和的形式 ：<br>$$<br>sim_{uv} = \frac{\sum_i r_{ui}·r_{vi}}{\sqrt{\sum_i r_{ui}^2}\sqrt{\sum_i r_{vi}^2}}<br>$$<br>其中$r_{ui},r_{vi}$分别表示用户$u$和用户$v$对商品$i$是否有交互(或者具体的评分值)。</p>
<p>如下是皮尔逊相关系数计算公式：<br>$$<br>sim(u,v)=\frac{\sum_{i\in I}(r_{ui}-\bar r_u)(r_{vi}-\bar r_v)}{\sqrt{\sum_{i\in I }(r_{ui}-\bar r_u)^2}\sqrt{\sum_{i\in I }(r_{vi}-\bar r_v)^2}}<br>$$<br>其中$r_{ui},r_{vi}$分别表示用户$u$和用户$v$对商品$i$是否有交互(或者具体的评分值)，$\bar r_u, \bar r_v$分别表示用户$u$和用户$v$交互的所有商品交互数量或者具体评分的平均值。所以相比余弦相似度，皮尔逊相关系数通过使用用户的<strong>平均分对各独立评分进行修正</strong>，减小了用户评分偏置的影响。具体实现， 我们也是可以调包， 这个计算方式很多， 下面是其中的一种：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"></span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>]</span><br><span class="line">pearsonr(i, j)</span><br></pre></td></tr></table></figure>

<p><a href="https://scipy.github.io/devdocs/reference/generated/scipy.stats.pearsonr.html?highlight=pearson#scipy.stats.pearsonr">皮尔逊相关系数 scipy官方文档</a></p>
<h2 id="编写代码计算两个用户、物品的相似度"><a href="#编写代码计算两个用户、物品的相似度" class="headerlink" title="编写代码计算两个用户、物品的相似度"></a>编写代码计算两个用户、物品的相似度</h2><p>详见notebook</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础4</title>
    <url>/2022/04/23/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%804/</url>
    <content><![CDATA[<hr>
<h1 id="任务4：协同过滤进阶"><a href="#任务4：协同过滤进阶" class="headerlink" title="任务4：协同过滤进阶"></a>任务4：协同过滤进阶</h1><ul>
<li>编写User-CF代码，通过用户相似度得到电影推荐</li>
<li>编写Item-CF代码，通过物品相似度得到电影推荐</li>
<li>进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/recpre">https://github.com/Guadzilla/recpre</a></p>
<span id="more"></span>
<h2 id="基于用户的协同过滤"><a href="#基于用户的协同过滤" class="headerlink" title="基于用户的协同过滤"></a>基于用户的协同过滤</h2><h3 id="UserCF原理介绍"><a href="#UserCF原理介绍" class="headerlink" title="UserCF原理介绍"></a>UserCF原理介绍</h3><p>基于用户的协同过滤算法(UserCF)的假设是：<strong>相似用户的兴趣也相似</strong>。所以，当一个用户A需要个性化推荐的时候， 我们可以先找到和他有相似兴趣的其他用户， 然后把那些用户喜欢的， 而用户A没有听说过的物品推荐给A。</p>
<p><img src="https://camo.githubusercontent.com/4cf6c62c1f1e533dffdd89db9bc045b560c95444a4a6e61b7b2d49cf28703f06/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2fe59bbee78987696d6167652d32303231303632393233323534303238392e706e67" alt="image-20210629232540289"  /></p>
<p><strong>UserCF算法主要包括两个步骤：</strong></p>
<ol>
<li>找到和目标用户兴趣相似的用户集合</li>
<li>找到这个集合中的用户喜欢的， 且目标用户没有听说过的物品推荐给目标用户。</li>
</ol>
<p>上面的两个步骤中， 第一个步骤里面， 我们会基于前面给出的相似性度量的方法找出与目标用户兴趣相似的用户， 而第二个步骤里面， 如何基于相似用户喜欢的物品来对目标用户进行推荐呢？ 这个要依赖于目标用户对相似用户喜欢的物品的一个喜好程度， 那么如何衡量这个程度大小呢？ 为了更好理解上面的两个步骤， 下面拿一个具体的例子把两个步骤具体化。</p>
<p><strong>以下图为例，此例将会用于本文各种算法中</strong></p>
<p><a href="https://camo.githubusercontent.com/38b1abf510c90bc3a8850a09ba19e39ea11b1f83b916f0432cc29fdaed665e5c/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f254535253942254245254537253839253837696d6167652d32303231303632393233323632323735382e706e67"><img src="https://camo.githubusercontent.com/38b1abf510c90bc3a8850a09ba19e39ea11b1f83b916f0432cc29fdaed665e5c/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f254535253942254245254537253839253837696d6167652d32303231303632393233323632323735382e706e67" alt="image-20210629232622758"></a></p>
<p>给用户推荐物品的过程可以<strong>形象化为一个猜测用户对商品进行打分的任务</strong>，上面表格里面是5个用户对于5件物品的一个打分情况，可以理解为用户对物品的喜欢程度</p>
<p>应用UserCF算法的两个步骤：</p>
<ol>
<li>首先根据前面的这些打分情况(或者说已有的用户向量）计算一下Alice和用户1， 2， 3， 4的相似程度， 找出与Alice最相似的n个用户</li>
<li>根据这n个用户对物品5的评分情况和与Alice的相似程度会猜测出Alice对物品5的评分， 如果评分比较高的话， 就把物品5推荐给用户Alice， 否则不推荐。</li>
</ol>
<p>关于第一个步骤， 上面已经给出了计算两个用户相似性的方法， 这里不再过多赘述， 这里主要解决第二个问题， 如何产生最终结果的预测。</p>
<p><strong>最终结果的预测</strong></p>
<p>根据上面的几种方法， 我们可以计算出向量之间的相似程度， 也就是可以计算出Alice和其他用户的相近程度， 这时候我们就可以选出与Alice最相近的前n个用户， 基于他们对物品5的评价猜测出Alice的打分值， 那么是怎么计算的呢？</p>
<p>这里常用的方式之一是<strong>利用用户相似度和相似用户的评价加权平均获得用户的评价预测</strong>， 用下面式子表示：</p>
<script type="math/tex; mode=display">
R_{\mathrm{u}, \mathrm{p}}=\frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot R_{\mathrm{s}, \mathrm{p}}\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}</script><p>这个式子里面， 权重$w_{u,s}$是用户$u$和用户$s$的相似度， $R_{s,p}$是用户$s$对物品$p$的评分。</p>
<p>还有一种方式如下：</p>
<script type="math/tex; mode=display">
P_{i, j}=\bar{R}_{i}+\frac{\sum_{k=1}^{n}\left(S_{i, k}\left(R_{k, j}-\bar{R}_{k}\right)\right)}{\sum_{k=1}^{n} S_{i, k}}</script><p>这种方式考虑的更加全面， 依然是用户相似度作为权值， 但后面不单纯是其他用户对物品的评分， 而是<strong>该物品的评分与此用户的所有评分的差值进行加权平均， 这时候考虑到了有的用户内心的评分标准不一的情况</strong>， 即有的用户喜欢打高分， 有的用户喜欢打低分的情况。</p>
<p>所以这一种计算方式更为推荐。下面的计算将使用这个方式。这里的$S_{i,k}$与上面的$w_{u,s}$的意思是类似的，表示的是用户i和用户k之间的相似度。</p>
<p>在获得用户$u$对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。 至此，基于用户的协同过滤算法的推荐过程完成。</p>
<p>根据上面的问题， 下面手算一下：</p>
<p>目标: 猜测Alice对物品5的得分：</p>
<ol>
<li><strong>计算Alice与其他用户的相似度（这里使用皮尔逊相关系数）</strong>:</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    ratings=&#123;<span class="string">&#x27;Alice&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user1&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user2&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user3&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user4&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> ratings</span><br><span class="line">ratings = loadData()</span><br><span class="line">ratings = pd.DataFrame(ratings).T</span><br><span class="line">ratings</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422153425763.png" alt="image-20220422153425763"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取出用户向量</span></span><br><span class="line">Alice = ratings.loc[<span class="string">&#x27;Alice&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user1 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user2 = ratings.loc[<span class="string">&#x27;user2&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user3 = ratings.loc[<span class="string">&#x27;user3&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user4 = ratings.loc[<span class="string">&#x27;user4&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义皮尔逊相似度</span></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearsonrSim</span>(<span class="params">x,y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    皮尔森相似度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> pearsonr(x,y)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算Alice和其它用户的相似度</span></span><br><span class="line">Alice_user1_similarity = pearsonrSim(Alice,user1)</span><br><span class="line">Alice_user2_similarity = pearsonrSim(Alice,user2)</span><br><span class="line">Alice_user3_similarity = pearsonrSim(Alice,user3)</span><br><span class="line">Alice_user4_similarity = pearsonrSim(Alice,user4)</span><br><span class="line">Alice_user1_similarity,Alice_user2_similarity,Alice_user3_similarity,Alice_user4_similarity</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出相似度</span></span><br><span class="line">(<span class="number">0.8528028654224415</span>, <span class="number">0.7071067811865475</span>, <span class="number">0.0</span>, -<span class="number">0.7921180343813393</span>)</span><br></pre></td></tr></table></figure>
<p>从这里看出, Alice用户1和用户2,用户3,用户4的相似度是0.85, 0.7, 0, -0.79。 所以如果n=2， 找到与Alice最相近的两个用户是用户1， 和Alice的相似度是0.85， 用户2， 和Alice相似度是0.7。</p>
<ol>
<li><strong>根据相似度用户计算Alice对物品5的最终得分</strong> 用户1对物品5的评分是3， 用户2对物品5的打分是5， 那么根据上面的计算公式， 可以计算出Alice对物品5的最终得分是 </li>
</ol>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{Alice}+\frac{\sum_{k=1}^{2}\left(S_{Alice,user k}\left(R_{userk, 物品5}-\bar{R}_{userk}\right)\right)}{\sum_{k=1}^{2} S_{Alice, userk}}=4+\frac{0.85*(3-2.4)+0.7*(5-3.8)}{0.85+0.7}=4.87</script><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422154709719.png" alt="image-20220422154709719"></p>
<ol>
<li><strong>根据用户评分对用户进行推荐</strong> 这时候， 我们就得到了Alice对物品5的得分是4.87， 根据Alice的打分对物品排个序从大到小：<script type="math/tex">物品1>物品5>物品3=物品4>物品2</script> 这时候，如果要向Alice推荐2款产品的话， 我们就可以推荐物品1和物品5给Alice</li>
</ol>
<p>至此， 基于用户的协同过滤算法原理介绍完毕。</p>
<h3 id="UserCF代码实现"><a href="#UserCF代码实现" class="headerlink" title="UserCF代码实现"></a>UserCF代码实现</h3><p>这里简单的通过编程实现上面的案例，为后面的大作业做一个热身， 梳理一下上面的过程其实就是三步： 计算用户相似性矩阵、得到前n个相似用户、计算最终得分。</p>
<p>所以我们下面的程序也是分为这三步：</p>
<ol>
<li><strong>首先， 先把数据表给建立起来</strong> 这里采用字典的方式， 之所以没有用pandas， 是因为上面举得这个例子其实是个个例， 在真实情况中， 我们知道， 用户对物品的打分情况并不会这么完整， 会存在大量的空值， 所以矩阵会很稀疏， 这时候用DataFrame， 会有大量的NaN。故这里用字典的形式存储。 用两个字典， 第一个字典是物品-用户的评分映射， 键是物品1-5， 用A-E来表示， 每一个值又是一个字典， 表示的是每个用户对该物品的打分。 第二个字典是用户-物品的评分映射， 键是上面的五个用户， 用1-5表示， 值是该用户对每个物品的打分。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    items=&#123;<span class="string">&#x27;A&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">5</span>, <span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">4</span>, <span class="number">4</span>: <span class="number">3</span>, <span class="number">5</span>: <span class="number">1</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;B&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">3</span>, <span class="number">2</span>: <span class="number">1</span>, <span class="number">3</span>: <span class="number">3</span>, <span class="number">4</span>: <span class="number">3</span>, <span class="number">5</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;C&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">2</span>, <span class="number">3</span>: <span class="number">4</span>, <span class="number">4</span>: <span class="number">1</span>, <span class="number">5</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;D&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">3</span>, <span class="number">4</span>: <span class="number">5</span>, <span class="number">5</span>: <span class="number">2</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;E&#x27;</span>: &#123;<span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">5</span>, <span class="number">4</span>: <span class="number">4</span>, <span class="number">5</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    users=&#123;<span class="number">1</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">2</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="number">3</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="number">4</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">5</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> items,users</span><br><span class="line"></span><br><span class="line">items, users = loadData()</span><br><span class="line">item_df = pd.DataFrame(items).T</span><br><span class="line">user_df = pd.DataFrame(users).T</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>计算用户相似性矩阵</strong> 这个是一个共现矩阵, 5*5，行代表每个用户， 列代表每个用户， 值代表用户和用户的相关性，这里的思路是这样， 因为要求用户和用户两两的相关性， 所以需要用双层循环遍历用户-物品评分数据， 当不是同一个用户的时候， 我们要去遍历物品-用户评分数据， 在里面去找这两个用户同时对该物品评过分的数据放入到这两个用户向量中。 因为正常情况下会存在很多的NAN， 即可能用户并没有对某个物品进行评分过， 这样的不能当做用户向量的一部分， 没法计算相似性。 还是看代码吧， 感觉不太好描述：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算用户相似性矩阵&quot;&quot;&quot;</span></span><br><span class="line">similarity_matrix = pd.DataFrame(-<span class="number">1</span> * np.ones((<span class="built_in">len</span>(users), <span class="built_in">len</span>(users))), index=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], columns=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> userx <span class="keyword">in</span> users:</span><br><span class="line">    <span class="keyword">for</span> usery <span class="keyword">in</span> users:</span><br><span class="line">        userxVec=[]</span><br><span class="line">        useryVec=[]</span><br><span class="line">        <span class="keyword">if</span> userx == usery:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            userx_history = users[userx].keys()</span><br><span class="line">            usery_history = users[usery].keys()</span><br><span class="line">            intersection = <span class="built_in">set</span>(userx_history).intersection(usery_history) <span class="comment"># 用户x和用户y行为历史的交集，否则有nan无法计算相似性</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> intersection:</span><br><span class="line">                userxVec.append(users[userx][i])</span><br><span class="line">                useryVec.append(users[usery][i])</span><br><span class="line">            similarity_matrix[userx][usery]=np.corrcoef(np.array(userxVec),np.array(useryVec))[<span class="number">0</span>][<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>得到如下user相似性矩阵：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422161751278.png" alt="image-20220422161751278"></p>
<p>注意相似度矩阵的初始值为-1，因为皮尔逊相关系数的取值为[-1,1]。</p>
<ol>
<li><strong>计算前n个相似的用户</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算前n个相似的用户&quot;&quot;&quot;</span></span><br><span class="line">n = <span class="number">2</span></span><br><span class="line">similar_users = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">    similar_users[user] = similarity_matrix[user].sort_values(ascending=<span class="literal">False</span>)[:n].index.tolist()</span><br><span class="line">    </span><br><span class="line">similar_users</span><br><span class="line">&#123;<span class="number">1</span>: [<span class="number">2</span>, <span class="number">3</span>], <span class="number">2</span>: [<span class="number">1</span>, <span class="number">4</span>], <span class="number">3</span>: [<span class="number">1</span>, <span class="number">2</span>], <span class="number">4</span>: [<span class="number">2</span>, <span class="number">1</span>], <span class="number">5</span>: [<span class="number">3</span>, <span class="number">4</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>经计算，与用户1最相似的2个用户分别是 用户2 和 用户3 。</p>
<ol>
<li><strong>计算最终得分</strong></li>
</ol>
<p>这里就是上面的那个公式：</p>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{Alice}+\frac{\sum_{k=1}^{2}\left(S_{Alice,user k}\left(R_{userk, 物品5}-\bar{R}_{userk}\right)\right)}{\sum_{k=1}^{2} S_{Alice, userk}}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算最后得分,用户1对物品E的预测评分&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有用户平均评分</span></span><br><span class="line">user_mean_rating = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">    user_mean = np.mean([value <span class="keyword">for</span> value <span class="keyword">in</span> users[user].values()])</span><br><span class="line">    user_mean_rating[user] = user_mean</span><br><span class="line"><span class="comment"># 计算预测得分</span></span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> similar_users[<span class="number">1</span>]:</span><br><span class="line">    weighted_scores += similarity_matrix[<span class="number">1</span>][user]</span><br><span class="line">    corr_values_sum += similarity_matrix[<span class="number">1</span>][user] * (users[user][<span class="string">&#x27;E&#x27;</span>] - user_mean_rating[user])</span><br><span class="line">predict = user_mean_rating[<span class="number">1</span>] + corr_values_sum/weighted_scores</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户1对物品E的预测评分为 <span class="subst">&#123;predict:<span class="number">.2</span>f&#125;</span> &#x27;</span>)</span><br><span class="line"></span><br><span class="line">用户<span class="number">1</span>对物品E的预测评分为 <span class="number">4.87</span></span><br></pre></td></tr></table></figure>
<p>计算结果如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422164842510.png" alt="image-20220422164842510" style="zoom:67%;" /></p>
<h3 id="UserCF的缺点"><a href="#UserCF的缺点" class="headerlink" title="UserCF的缺点"></a>UserCF的缺点</h3><p>UserCF算法存在两个重大问题：</p>
<ol>
<li>数据稀疏性。 一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。<strong>这导致UserCF不适用于那些正反馈获取较困难的应用场景</strong>(如酒店预订， 大件商品购买等低频应用)</li>
<li>算法扩展性。 基于用户的协同过滤需要维护用户相似度矩阵以便快速的找出Topn相似用户， 该矩阵的存储开销非常大，存储空间随着用户数量的增加而增加，<strong>不适合用户数据量大的情况使用</strong>。</li>
</ol>
<p>由于UserCF技术上的两点缺陷， 导致很多电商平台并没有采用这种算法， 而是采用了ItemCF算法实现最初的推荐系统。</p>
<h2 id="基于物品的协同过滤"><a href="#基于物品的协同过滤" class="headerlink" title="基于物品的协同过滤"></a>基于物品的协同过滤</h2><h3 id="ItemCF原理介绍"><a href="#ItemCF原理介绍" class="headerlink" title="ItemCF原理介绍"></a>ItemCF原理介绍</h3><p>基于物品的协同过滤(ItemCF)的基本思想是预先根据所有用户的历史偏好数据计算物品之间的相似性，然后把与用户喜欢的物品相类似的物品推荐给用户。比如物品a和c非常相似，因为喜欢a的用户同时也喜欢c，而用户A喜欢a，所以把c推荐给用户A。<strong>ItemCF算法并不利用物品的内容属性计算物品之间的相似度， 主要通过分析用户的行为记录计算物品之间的相似度， 该算法认为， 物品a和物品c具有很大的相似度是因为喜欢物品a的用户大都喜欢物品c</strong>。</p>
<p><strong>和UserCF类似，ItemCF算法主要包括两个步骤：</strong></p>
<ul>
<li>计算物品之间的相似度</li>
<li>根据物品的相似度和用户的历史行为给用户生成推荐列表（购买了该商品的用户也经常购买的其他商品）</li>
</ul>
<p>这里直接还是拿上面Alice的那个例子来看。</p>
<p><img src="https://camo.githubusercontent.com/38b1abf510c90bc3a8850a09ba19e39ea11b1f83b916f0432cc29fdaed665e5c/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f254535253942254245254537253839253837696d6167652d32303231303632393233323632323735382e706e67" alt="image-20210629232622758"></p>
<p>如果想知道Alice对物品5打多少分， 基于物品的协同过滤算法会这么做：</p>
<ol>
<li>首先计算一下物品5和物品1， 2， 3， 4之间的相似性(它们也是向量的形式， 每一列的值就是它们的向量表示， 因为ItemCF认为如果物品a和物品c具有很大的相似度，那么是因为喜欢物品a的用户大都喜欢物品c， 所以就可以基于每个用户对该物品的打分或者说喜欢程度来向量化物品)</li>
<li>找出与物品5最相近的n个物品（取n=2）</li>
<li>根据Alice对最相近的n个物品的打分去计算对物品5的打分情况，加入评分偏置的预测公式如下：</li>
</ol>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{物品5}+\frac{\sum_{k=1}^{2}\left(S_{物品5,物品 k}\left(R_{Alice, 物品k}-\bar{R}_{物品k}\right)\right)}{\sum_{k=1}^{2} S_{物品k, 物品5}}</script><p><strong>下面我们就可以具体计算一下，猜测Alice对物品5的打分：</strong></p>
<p>首先是步骤1：计算物品5和其它物品之间的相似度。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422193745961.png" alt="image-20220422193745961"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取item向量</span></span><br><span class="line">item5 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item5&#x27;</span>].values.tolist()</span><br><span class="line">item4 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item4&#x27;</span>].values.tolist()</span><br><span class="line">item3 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item3&#x27;</span>].values.tolist()</span><br><span class="line">item2 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item2&#x27;</span>].values.tolist()</span><br><span class="line">item1 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item1&#x27;</span>].values.tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算item相似度</span></span><br><span class="line">item51_similarity = pearsonrSim(item5,item1)</span><br><span class="line">item52_similarity = pearsonrSim(item5,item2)</span><br><span class="line">item53_similarity = pearsonrSim(item5,item3)</span><br><span class="line">item54_similarity = pearsonrSim(item5,item4)</span><br><span class="line">[x.<span class="built_in">round</span>(<span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> (item51_similarity,item52_similarity,item53_similarity,item54_similarity)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出相似度</span></span><br><span class="line">[<span class="number">0.97</span>, -<span class="number">0.48</span>, -<span class="number">0.43</span>, <span class="number">0.58</span>]</span><br></pre></td></tr></table></figure>
<p>步骤2：对相似度进行排序，选择最靠前的n=2个物品：item1和item4</p>
<p>步骤3：下面根据公式计算Alice对物品5的打分</p>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{物品5}+\frac{\sum_{k=1}^{2}\left(S_{物品5,物品 k}\left(R_{Alice, 物品k}-\bar{R}_{物品k}\right)\right)}{\sum_{k=1}^{2} S_{物品k, 物品5}}=\frac{13}{4}+\frac{0.97*(5-3.2)+0.58*(4-3.4)}{0.97+0.58}=4.6</script><p>这时候依然可以向Alice推荐物品5。</p>
<p>下面也是简单编程实现一下， 和上面的差不多：</p>
<h3 id="ItemCF代码实现"><a href="#ItemCF代码实现" class="headerlink" title="ItemCF代码实现"></a>ItemCF代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算物品的相似矩阵&quot;&quot;&quot;</span></span><br><span class="line">similarity_matrix = pd.DataFrame(-<span class="number">1</span> * np.ones((<span class="built_in">len</span>(items), <span class="built_in">len</span>(items))), index=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;E&#x27;</span>], columns=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;E&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每条物品-用户评分数据</span></span><br><span class="line"><span class="keyword">for</span> itemx <span class="keyword">in</span> items:</span><br><span class="line">    <span class="keyword">for</span> itemy <span class="keyword">in</span> items:</span><br><span class="line">        itemxVec = []</span><br><span class="line">        itemyVec = []</span><br><span class="line">        <span class="keyword">if</span> itemx == itemy:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            itemx_history = <span class="built_in">set</span>(items[itemx].keys())</span><br><span class="line">            itemy_history = <span class="built_in">set</span>(items[itemy].keys())</span><br><span class="line">            intersection = itemx_history.intersection(itemy_history)  <span class="comment"># 求交集，同时对两个物品都打分的用户，才有意义</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> intersection:</span><br><span class="line">                itemxVec.append(items[itemx][i])</span><br><span class="line">                itemyVec.append(items[itemy][i])</span><br><span class="line">            similarity_matrix[itemx][itemy] = pearsonrSim(itemxVec,itemyVec).<span class="built_in">round</span>(<span class="number">2</span>)</span><br><span class="line">            <span class="comment"># similarity_matrix[itemx][itemy] = np.corrcoef(np.array(itemxVec),np.array(itemyVec))[0][1] 两种计算方式等价</span></span><br></pre></td></tr></table></figure>
<p>得到物品相似度矩阵：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422195802322.png" alt="image-20220422195802322"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算前n个相似的用户&quot;&quot;&quot;</span></span><br><span class="line">n = <span class="number">2</span></span><br><span class="line">similar_items = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">    similar_items[item] = similarity_matrix[item].sort_values(ascending=<span class="literal">False</span>)[:n].index.tolist()</span><br><span class="line">    </span><br><span class="line">similar_items[<span class="string">&#x27;E&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 与E最相似的2个物品是A和D</span></span><br><span class="line">[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;D&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算最后得分,用户1对物品E的预测评分&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算物品平均打分情况</span></span><br><span class="line">item_ratings_mean = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> item,rating <span class="keyword">in</span> items.items():</span><br><span class="line">    item_ratings_mean[item] = np.mean([value <span class="keyword">for</span> value <span class="keyword">in</span> rating.values()])</span><br><span class="line"></span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> similar_items[<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    weighted_scores += similarity_matrix[<span class="string">&#x27;E&#x27;</span>][item]</span><br><span class="line">    corr_values_sum += similarity_matrix[<span class="string">&#x27;E&#x27;</span>][item] * (users[<span class="number">1</span>][item] -  item_ratings_mean[item])</span><br><span class="line"></span><br><span class="line">predict = item_ratings_mean[<span class="string">&#x27;E&#x27;</span>] + corr_values_sum/weighted_scores</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户1对物品E的预测得分为 <span class="subst">&#123;predict:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="9-协同过滤算法的问题分析"><a href="#9-协同过滤算法的问题分析" class="headerlink" title="9. 协同过滤算法的问题分析"></a>9. 协同过滤算法的问题分析</h3><p>协同过滤算法存在的问题之一就是<strong>泛化能力弱</strong>， 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。 导致的问题是<strong>热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐</strong>。 比如下面这个例子：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422203637025.png" alt="image-20220422203637025"></p>
<p>A, B, C, D是物品， 看右边的物品共现矩阵， 可以发现物品D与A、B、C的相似度比较大， 所以很有可能将D推荐给用过A、B、C的用户。 但是物品D与其他物品相似的原因是因为D是一件热门商品， 系统无法找出A、B、C之间相似性的原因是其特征太稀疏， 缺乏相似性计算的直接数据。 所以这就是协同过滤的天然缺陷：<strong>推荐系统头部效应明显， 处理稀疏向量的能力弱</strong>。</p>
<p>为了解决这个问题， 同时增加模型的泛化能力，2006年，<strong>矩阵分解技术(Matrix Factorization,MF</strong>)被提出， 该方法在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征， 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。</p>
<h2 id="进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？"><a href="#进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？" class="headerlink" title="进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？"></a>进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？</h2><p>矩阵乘法用在求两个向量的内积。当问题场景是仅预测用户是否会对物品评分，即共现矩阵只有 0 和 1 时，两个向量的内积可以用集合的形式表示。例如： <code>u = [1,0,0,1,0], v = [0,0,1,1,0]</code> ，矩阵乘法求得两个向量内积为 1 ，从集合的角度看，内积的计算结果其实也是用户 u 和 v 交互过的物品的交集元素个数。</p>
<p>所以集合角度的余弦相似度计算如下：</p>
<script type="math/tex; mode=display">
sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}</script><p>如果只建立 user 对 item 的索引，形式如： <code>&#123;uid: &#123;item1, item2,...&#125;, uid: &#123;item1, item2,...&#125;, ...&#125;</code>，计算两两用户的交互交集时，比较麻烦。建立倒排表形如：<code>&#123;item_id1: &#123;user_id1, user_id2, ... , user_idn&#125;, item_id2: ...&#125;</code> ，只需要对每个 item 遍历，就可以统计两两用户的交互交集。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立item-&gt;users倒排表</span></span><br><span class="line"><span class="comment"># 倒排表的格式为: &#123;item_id1: &#123;user_id1, user_id2, ... , user_idn&#125;, item_id2: ...&#125; 也就是每个item对应有那些用户有过点击</span></span><br><span class="line"><span class="comment"># 建立倒排表的目的就是为了更方便的统计用户之间共同交互的商品数量</span></span><br><span class="line">item_users = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> uid, items <span class="keyword">in</span> tqdm(tra_users.items()): <span class="comment"># 遍历每一个用户的数据,其中包含了该用户所有交互的item</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items: <span class="comment"># 遍历该用户的所有item, 给这些item对应的用户列表添加对应的uid</span></span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_users:</span><br><span class="line">            item_users[item] = <span class="built_in">set</span>()</span><br><span class="line">            item_users[item].add(uid)</span><br></pre></td></tr></table></figure>
<h2 id="课后思考"><a href="#课后思考" class="headerlink" title="课后思考"></a>课后思考</h2><p>1.<strong>什么时候使用UserCF，什么时候使用ItemCF？为什么？</strong></p>
<blockquote>
<ol>
<li>UserCF 由于是基于用户相似度进行推荐， 所以具备更强的社交特性， 这样的特点非常适于<strong>用户少， 物品多， 时效性较强的场合</strong>， 比如新闻推荐场景， 因为新闻本身兴趣点分散， 相比用户对不同新闻的兴趣偏好， 新闻的及时性，热点性往往更加重要， 所以正好适用于发现热点，跟踪热点的趋势。 另外还具有推荐新信息的能力， 更有可能发现惊喜, 因为看的是人与人的相似性, 推出来的结果可能更有惊喜，可以发现用户潜在但自己尚未察觉的兴趣爱好。</li>
<li>ItemCF 这个更适用于兴趣变化较为稳定的应用， 更接近于个性化的推荐， 适合<strong>物品少，用户多，用户兴趣固定持久， 物品更新速度不是太快的场合</strong>， 比如推荐艺术品， 音乐， 电影。</li>
</ol>
</blockquote>
<p>2.<strong>协同过滤在计算上有什么缺点？有什么比较好的思路可以解决（缓解）？</strong></p>
<blockquote>
<p>第一个问题就是<strong>泛化能力弱</strong>， 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。 导致的问题是<strong>热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐</strong>。</p>
</blockquote>
<p><strong>3.上面介绍的相似度计算方法有什么优劣之处？</strong></p>
<blockquote>
<p>cosine相似度还是比较常用的， 一般效果也不会太差， 但是对于评分数据不规范的时候， 也就是说， 存在有的用户喜欢打高分， 有的用户喜欢打低分情况的时候，有的用户喜欢乱打分的情况， 这时候consine相似度算出来的结果可能就不是那么准确了。所以对于这种用户评分偏置的情况， 余弦相似度就不是那么好了， 可以考虑使用下面的皮尔逊相关系数。</p>
<script type="math/tex; mode=display">
P_{i, j}=\bar{R}_{i}+\frac{\sum_{k=1}^{n}\left(S_{i, k}\left(R_{k, j}-\bar{R}_{k}\right)\right)}{\sum_{k=1}^{n} S_{i, k}}</script></blockquote>
<p>4.<strong>协同过滤还存在其他什么缺陷？有什么比较好的思路可以解决（缓解）？</strong></p>
<blockquote>
<p>协同过滤的特点就是完全没有利用到物品本身或者是用户自身的属性， 仅仅利用了用户与物品的交互信息就可以实现推荐，比较简单高效， 但这也是它的一个短板所在， 由于无法有效的引入用户年龄， 性别，商品描述，商品分类，当前时间，地点等一系列用户特征、物品特征和上下文特征， 这就造成了有效信息的遗漏，不能充分利用其它特征数据。</p>
<p>为了解决这个问题， 在推荐模型中引用更多的特征，<strong>推荐系统慢慢的从以协同过滤为核心到了以逻辑回归模型为核心</strong>， 提出了能够综合不同类型特征的机器学习模型。</p>
<p>演化图左边的时间线梳理完毕：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220424123935703.png" alt="《深度学习推荐系统》- 王喆"></p>
</blockquote>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>使用皮尔逊相似度计算的预测结果非常差，可能因为计算皮尔逊相似度时出现大量nan（不知道为什么…），然后我的做法是用0填充，pearsonr = 0 代表无关。</p>
<p>皮尔逊相关系数计算公式：</p>
<script type="math/tex; mode=display">
P_{i, j}=\bar{R}_{i}+\frac{\sum_{k=1}^{n}\left(S_{i, k}\left(R_{k, j}-\bar{R}_{k}\right)\right)}{\sum_{k=1}^{n} S_{i, k}}</script><p>预测结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Pearsonr_ItemCF(K=<span class="number">10</span>,N=<span class="number">10</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">recall: 0.18</span></span><br><span class="line"><span class="string">precision 0.61</span></span><br><span class="line"><span class="string">coverage 35.17</span></span><br><span class="line"><span class="string">Popularity 5.539</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">Cosine_Item_CF(K=<span class="number">10</span>,N=<span class="number">10</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">recall: 9.2</span></span><br><span class="line"><span class="string">precision 30.48</span></span><br><span class="line"><span class="string">coverage 19.18</span></span><br><span class="line"><span class="string">Popularity 7.171</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
</search>
