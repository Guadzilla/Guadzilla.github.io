<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>论文笔记：《Price DOES Matter! Modeling Price and Interest Preferences in Session-based Recommendation》</title>
    <url>/2022/05/22/CoHHN/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405093804827.png" alt="image-20220405093804827"></p>
<hr>
<p>原paper：<a href="https://arxiv.org/abs/2205.04181">[2205.04181] Price DOES Matter! Modeling Price and Interest Preferences in Session-based Recommendation (arxiv.org)</a></p>
<p>作者本人解读：<a href="https://mp.weixin.qq.com/s/kSayir_jVwZbhEPm0qtYPA">https://mp.weixin.qq.com/s/kSayir_jVwZbhEPm0qtYPA</a></p>
<hr>
<p>中译：基于用户价格偏好及兴趣偏好的会话推荐</p>
<p>总结：</p>
<p>&emsp;&emsp;异质超图=异质图+超图：异质图——融合异质特征，超图——捕捉高阶依赖</p>
<p>&emsp;&emsp;双通道聚合：intra + inter ，聚合同类和异类信息，获得价格和兴趣偏好的初级表示</p>
<p>&emsp;&emsp;协同指导学习：捕捉价格偏好和兴趣偏好之间的复杂联系，获得语义增强的价格和兴趣偏好的表示</p>
<span id="more"></span>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>question作者想解决什么问题？ </li>
</ul>
<p>&emsp;&emsp;现有会话推荐系统中关注如何建模用户的兴趣偏好，然而忽略了物品的一项重要属性，即价格。同时建模价格和兴趣偏好不是件容易的事情。<strong>首先</strong>，它很难处理来自物品的各种特征的异质信息来捕捉用户的价格偏好；<strong>其次</strong>，在决定用户选择时，很难对价格和兴趣偏好之间的复杂关系进行建模。</p>
<ul>
<li>method作者通过什么理论/模型来解决这个问题？</li>
</ul>
<p>&emsp;&emsp;提出协同指导的异质超图网络（Co-guided Heterogeneous Hypergraph Network，CoHHN）。</p>
<p>&emsp;&emsp;对于第一个挑战，提出用<strong>异质超图</strong>来表示异质信息，接着通过双通道聚合机制聚合这些信息，再通过attention层提取用户的价格偏好和兴趣偏好。</p>
<p>&emsp;&emsp;对于第二个挑战，提出<strong>协同指导的学习方法</strong>，用来建模价格和兴趣偏好之间的关系。</p>
<ul>
<li>answer作者给出的答案是什么？</li>
</ul>
<p>&emsp;&emsp;三个数据集中取得SOTA。</p>
<p>&emsp;&emsp;分析得出会话推荐任务中价格的重要性。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>动机</li>
</ul>
<p>&emsp;&emsp;现有会话推荐系统中关注如何建模用户的兴趣偏好，然而忽略了物品的一项重要属性，即价格。但是许多市场研究展示出用户的购买行为非常容易受到价格因素的影响，所以价格应该被纳入考虑。</p>
<ol>
<li><p>价格偏好的特点：动态，受商品类别影响。（例如用户可能为了性能而购买很贵的笔记本电脑，但是却买比较便宜的睡衣。）所以在纳入用户价格偏好时，我们需要考虑商品对应的类别。因此这种场景下，商品序列，商品价格，商品类别，这些<strong>异质信息</strong>会使得现有方法失效。</p>
</li>
<li><p>此外，经济学上有一种现象叫做<strong>价格弹性</strong>，即用户愿意为某一物品支付的金钱会随着用户对该物品的兴趣而波动。这一现象充分显示用户的购买行为是<strong>由价格和兴趣共同决定</strong>的。（例如越感兴趣的商品越愿意花高价钱买，愿意花低价买那些不是特别喜欢的商品）</p>
</li>
</ol>
<p>&emsp;&emsp;针对上面两个挑战，分别提出解决方案。</p>
<ol>
<li>现有的SOTA方法，只能捕捉 pairwise relation ，成对关系，例如商品序列、类别。本文提出的CoHHN中，<strong>异质超图</strong>结合了超图和异质图的优点。 <strong>超图</strong>可以捕捉比 pairwise 更高阶的依赖关系，具体是<item,category,price>。使用三种超边，特征超边，价格超边，会话超边捕捉<strong>异质</strong>信息。然后用双通道聚合机制，通过特征超边来传播节点 embedding ，基于这些节点 embedding ，分别基于价格超边和会话超边，利用 attention 层来提取原始的价格和兴趣偏好。</li>
<li>提出 <strong>协同指导学习方法 </strong>，用于建模价格和兴趣之间的复杂关系。具体来说，利用异质超图学到的原始价格和兴趣偏好，让他们互相指导对方的学习以增强语义信息。最后，基于商品特征、价格偏好、兴趣偏好，共同做出推荐。</li>
</ol>
<ul>
<li>基于什么样的假设？</li>
</ul>
<p>&emsp;&emsp;价格偏好的特点：动态，受商品类别影响。</p>
<p>&emsp;&emsp;价格弹性现象。用户愿意支付的钱受兴趣影响，购买行为由兴趣和价格共同决定。</p>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><ul>
<li>会话推荐定义（略）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405095200598.png" alt="image-20220405095200598" style="zoom:50%;" /></p>
<ul>
<li>价格离散化：每个类别由不同的价格 level 。根据 logistic 分布（上图），使得划分的每一部分概率相同。</li>
</ul>
<script type="math/tex; mode=display">
p_{i}=\left\lfloor\frac{\Phi\left(x_{p}\right)-\Phi(\min )}{\Phi(\max )-\Phi(\min )} \times \rho\right\rfloor</script><p>​     &emsp;&emsp;其中 $x_i$ 的价格 $x_p$ ，$x_i$ 所属类别的价格范围是 $[min,max]$ ，其中 $\Phi\left(x\right)$ 是概率分布函数。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405103854311.png" alt="image-20220405103854311" style="zoom:67%;" /></p>
<ul>
<li><p>异质超图构建：见上图，每个超边可以连接任意个节点，共有三种超边，特征超边，价格超边，会话超边。</p>
<ul>
<li>1）特征超边连接一个 item 的所有特征：id、price、category；2）价格超边连接同一个会话中的价格节点；3）会话超边连接会话中所有的id节点。</li>
<li>特征超边用来在异质节点间传播信息，价格超边和会话超边用来提取用户的价格和兴趣偏好。</li>
</ul>
</li>
<li><p><strong>问题：为什么没有类别超边呢？因为类别信息用特征超边传播了，而且目的是获取价格和兴趣偏好？</strong></p>
</li>
<li><strong>问题：特征超边只用来传播不同特征？</strong> </li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405101322691.png" alt="image-20220405101322691" style="zoom:50%;" /></p>
<h3 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h3><p>&emsp;&emsp;首先基于所有会话数据构建异质超图，其次通过 Dual-channel 聚合 intra-type 信息和 inter-type 信息（注意不是 intra-session 和 inter-session）；接着，通过 attention 层提取价格偏好和兴趣偏好，两种偏好经过协同指导学习增强语义，最后利用商品特征、价格偏好、兴趣偏好共同做出推荐。</p>
<h3 id="双通道聚合机制"><a href="#双通道聚合机制" class="headerlink" title="双通道聚合机制"></a>双通道聚合机制</h3><p>&emsp;&emsp;因为超边的存在，不同类别的节点会连接在一起。显然，拥有相同类型的节点提供同质的信息，不同类型的节点提供异质的信息，两种类型的节点对目标节点的贡献是不同类型的。所以设计两个通道， intro-type 通道和 inter-type 通道 。</p>
<p><strong>&emsp;&emsp;Intra-type 聚合。</strong> $v^t$ 是 target node 的 embedding，它的属于类别 $\tau$ 的邻接点集合为 $N^{\tau}_t$ 。同类聚合的目的是区分同类节点的重要性差异，并且聚合这些信息。公式如下：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{e}_{t}^{\tau}=\sum_{i} \alpha_{i} \mathbf{v}_{i}^{\tau} \\
\alpha_{i}=\frac{\exp \left(\mathbf{u}_{\tau} \mathbf{v}_{i}^{\tau}\right)}{\sum_{\mathbf{v}_{i}^{\tau} \in N_{t}^{\tau}} \exp \left(\mathbf{u}_{\tau} \mathbf{v}_{i}^{\tau}\right)}
\end{array}</script><p>&emsp;&emsp;其中 $u_{\tau}$ 是 attention vector，决定了类别 $\tau$ 的节点对目标节点的重要程度，这里不同类别有不同类别的 attention vector ，使得模型可以学到每个节点特定类别的 embedding 。 $e^{\tau}_t$ 是 target node  $v^t$ 的类别 embedding 。 公式抽象为： $\mathbf{e}_{t}^{\tau}=f_{a}\left(N_{t}^{\tau}\right)$ 。</p>
<p><strong>&emsp;&emsp;Inter-type 聚合。</strong> 基于学到的类别 embedding $e^{\tau}_t$ ，inter-type聚合的目的是聚合不同类别的 embedding 到目标节点上。公式如下：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{g}_{t}=\sigma\left(\mathbf{W}_{t}\left[\mathbf{v}^{t} ; \mathbf{e}_{t}^{\tau 1} ; \mathbf{e}_{t}^{\tau 2}\right]+\mathbf{W}_{t}^{\tau 1} \mathbf{e}_{t}^{\tau 1}+\mathbf{W}_{t}^{\tau 2} \mathbf{e}_{t}^{\tau 2}\right) \\
\mathbf{h}^{t}=\mathbf{v}^{t}+\mathbf{g}_{t} * \mathbf{e}_{t}^{\tau 1}+\left(1-\mathbf{g}_{t}\right) * \mathbf{e}_{t}^{\tau 2}
\end{array}</script><p>&emsp;&emsp;其中 $W _ t,W ^ {\tau 1} _ t,W ^ {\tau 2} _ t$ 是可学习参数。 $h_t$ 是经过邻接的异质节点与以增强过的 target node 的 embedding 。 公式抽象为： $\mathbf{h}^{t}=f_{b}\left(\mathbf{v}^{t}, \mathbf{e}_{t}^{\tau 1}, \mathbf{e}_{t}^{\tau 2}\right)$ 。</p>
<p>&emsp;&emsp;双通道聚合公式如下：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
\mathbf{h}^{i d} = f_{b}\left(\mathbf{v}^{i d}, f_{a}\left(N_{i d}^{p}\right), f_{a}\left(N_{i d}^{c}\right)\right)+\operatorname{avg}\left(N_{i d}^{i d}\right) \\
\mathbf{h}^{p} = f_{b}\left(\mathbf{v}^{p}, f_{a}\left(N_{p}^{i d}\right), f_{a}\left(N_{p}^{c}\right)\right) \\
\mathbf{h}^{c} = f_{b}\left(\mathbf{v}^{c}, f_{a}\left(N_{c}^{p}\right), f_{a}\left(N_{c}^{i d}\right)\right)
\end{array}</script><p>&emsp;&emsp;上标代表类别，c：category，id：item id，p：price。可以看到每个类别的 embedding 都会融合另外两个类别的信息。</p>
<p>&emsp;&emsp;<strong>值得注意的是，邻接关系主要是基于特征超边的。</strong></p>
<h3 id="偏好提取"><a href="#偏好提取" class="headerlink" title="偏好提取"></a>偏好提取</h3><p>&emsp;&emsp;基于 $\mathbf{h}^{i d}、\mathbf{h}^{p}、\mathbf{h}^{c}$ 这三个节点 embedding ，进一步提取价格和兴趣偏好。</p>
<p><strong>&emsp;&emsp;提取价格偏好</strong>。因为用户的价格偏好反映在他购买的商品，所以依据价格超边提取用户的价格偏好。用多头自注意力机制。没有加入时间信息是因为价格偏好相对稳定，并且表示集体性依赖而非序列性依赖。公式如下：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{E}_{p}=\left[\mathbf{h}_{1}^{p} ; \mathbf{h}_{2}^{p} ; \ldots ; \mathbf{h}_{m}^{p}\right]\\
\text { head }_{i}=\text { Attention }\left(W_{i}^{Q} \mathbf{E}_{p}, W_{i}^{K} \mathbf{E}_{p}, W_{i}^{V} \mathbf{E}_{p}\right)\\
\mathrm{S}_{p}=\left[\text { head }_{1} ; \text { head }_{2} ; \ldots ; \text { head }_{h}\right]
\end{array}</script><p>&emsp;&emsp;隐藏状态 $S_P$ 作为原始的价格偏好： $\hat{\mathbf{P}}_{p}=\mathrm{S}_{p}^{(t)}$ 。</p>
<p><strong>&emsp;&emsp;提取兴趣偏好</strong>。依据会话超边提取用户兴趣偏好。因为兴趣可能随时间漂移，所以加入位置编码学习动态兴趣。这里使用 reversed position embedding，会话中第 $i$ 个物品的计算公式如下：</p>
<script type="math/tex; mode=display">
\mathbf{v}_{i}^{*}=\tanh \left(\mathbf{W}_{f}\left[\mathbf{h}_{i}^{i d} ; \operatorname{pos}_{i}\right]+\mathbf{b}_{f}\right)</script><p>&emsp;&emsp;其中$\mathbf{W}_{f}$ 和 $\mathbf{b}_{f}$ 是可学习参数。再过 soft 注意力层：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{\mathrm{I}}_{p} &=\sum_{i=1}^{t} \beta_{i} \mathbf{h}_{i}^{i d} \\
\beta_{i} &=\mathbf{u} \sigma\left(\mathbf{A}_{1} \mathbf{v}_{i}^{*}+\mathbf{A}_{2} \overline{\mathbf{v}}^{*}+\mathbf{b}\right)
\end{aligned}</script><p>&emsp;&emsp;其中 $\mathbf{A}_{1}、 \mathbf{A}_{2}、\mathbf{b}$ 是可学习参数，$\mathbf u^T$ 是 attention vector。$\overline{\mathbf{v}}^{*}$ 是 $\mathbf{v}_{i}$ 的均值，$\hat{\mathrm{I}}_{p}$ 是原始的兴趣偏好。</p>
<h3 id="协同指导-Learning-Schema"><a href="#协同指导-Learning-Schema" class="headerlink" title="协同指导 Learning Schema"></a>协同指导 Learning Schema</h3><p>&emsp;&emsp;有了原始的兴趣偏好 $\hat{\mathrm{I}}_{p}$ 和原始的价格偏好 $S_P$ ，可以简单定义用户对物品 $x_i$ 的预测得分：</p>
<script type="math/tex; mode=display">
y_{i}=\hat{\mathbf{P}}_{p}^{\top} \mathbf{v}_{i}^{p}+\hat{\mathbf{I}}_{p}^{\top} \mathbf{v}_{i}^{i d}</script><p>&emsp;&emsp;但是这种简单的加法方式不能处理复杂的情况，比如用户对越感兴趣的商品越愿意花高价钱买，同时愿意花低价买那些不是特别感兴趣的商品。所以提出<strong>协同指导的学习方法</strong>。</p>
<p>&emsp;&emsp;首先用两种方式融合 价格偏好 和 兴趣偏好 ：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{m}_{c}=\tanh \left(\mathbf{W}_{1}^{p i}\left(\hat{\mathbf{P}}_{p} * \hat{\mathbf{I}}_{p}\right)+\mathbf{b}_{c}\right) \\
\mathbf{m}_{j}=\tanh \left(\mathbf{W}_{2}^{p i}\left(\hat{\mathbf{P}}_{p}+\hat{\mathbf{I}}_{p}\right)+\mathbf{b}_{j}\right)
\end{array}</script><p>&emsp;&emsp;$\mathbf{m}_{c}$ 和 $\mathbf{m}_{j}$ 用来表示<strong>不同语义空间下的价格和兴趣间交互关系</strong>。接着用门控机制进一步建模价格偏好和兴趣偏好之间的相互关系（mutual relations）。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{r}_{P} &=\sigma\left(\mathbf{W}_{1}^{r} \mathbf{m}_{c}+\mathbf{U}_{1}^{r} \mathbf{m}_{j}\right) \\
\mathbf{r}_{I} &=\sigma\left(\mathbf{W}_{2}^{r} \mathbf{m}_{c}+\mathbf{U}_{2}^{r} \mathbf{m}_{j}\right) \\
\mathbf{m}_{P} &=\tanh \left(\mathbf{W}^{P}\left(\mathbf{r}_{P} * \hat{\mathbf{P}}_{p}\right)+\mathrm{U}^{P}\left(\left(1-\mathbf{r}_{I}\right) * \hat{\mathbf{I}}_{p}\right)\right) \\
\mathbf{m}_{I} &=\tanh \left(\mathbf{W}^{I}\left(\mathbf{r}_{I} * \hat{\mathbf{I}}_{p}\right)+\mathbf{U}^{I}\left(\left(1-\mathbf{r}_{P}\right) * \hat{\mathbf{P}}_{p}\right)\right)
\end{aligned}</script><p>&emsp;&emsp;其中 $\mathbf W$ 和 $\mathbf U$ 是可学习参数，$\mathbf r$ 是记忆门。$\mathbf m$ 是语义增强后的价格和兴趣偏好表示。最终的价格和兴趣偏好表示如下：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{P}_{p}=\mathbf{m}_{P} *\left(\hat{\mathbf{P}}_{p}+\mathbf{m}_{I}\right) \\
\mathbf{I}_{p}=\mathbf{m}_{I} *\left(\hat{\mathbf{I}}_{p}+\mathbf{m}_{P}\right)
\end{array}</script><p> &emsp;&emsp;这里再次增强语义。</p>
<h3 id="预测和训练"><a href="#预测和训练" class="headerlink" title="预测和训练"></a>预测和训练</h3><p>&emsp;&emsp;预测得分：$y_{i}=\mathbf{P}_{p}^{\top} \mathbf{v}_{i}^{p}+\mathbf{I}_{p}^{\top} \mathbf{v}_{i}^{i d}$</p>
<p>&emsp;&emsp;转化为概率：$y_{i}=\frac{\exp \left(y_{i}\right)}{\sum_{j=1}^{n} \exp \left(y_{j}\right)}$</p>
<p>&emsp;&emsp;CE loss：$\mathcal{L}(\mathrm{p}, \mathrm{y})=-\sum_{j=1}^{n} p_{j} \log \left(y_{j}\right)+\left(1-p_{j}\right) \log \left(1-y_{j}\right)$ ，其中 $p_{j}$ 是真实标签。</p>
<h2 id="Experiment-Setup"><a href="#Experiment-Setup" class="headerlink" title="Experiment  Setup"></a>Experiment  Setup</h2><h3 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h3><ul>
<li>RQ1：SOTA了吗？</li>
<li>RQ2：价格因素的影响如何？</li>
<li>RQ3：协同指导学习方法的影响如何？</li>
<li>RQ4：不同价格 level 的影响如何？</li>
<li>RQ5：关键超参数的影响如何？</li>
</ul>
<h3 id="数据集-和-预处理"><a href="#数据集-和-预处理" class="headerlink" title="数据集 和 预处理"></a>数据集 和 预处理</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405130624751.png" alt="image-20220405130624751" style="zoom:50%;" /></p>
<ul>
<li><p><a href="https://www.kaggle.com/mkechinov/ecommerce-events-history-in-cosmetics-shop">Cosmetics</a> ，We use one month (October 2019) records and only retain the interactions with type “add_to_cart” or “purchase” in our work.</p>
</li>
<li><p><a href="https://competitions.codalab.org/competitions/11161">Diginetica-buy</a>.</p>
</li>
<li><p><a href="http://jmcauley.ucsd.edu/data/amazon/">Amazon</a>，Grocery and Gourmet Food.</p>
</li>
</ul>
<p>&emsp;&emsp;预处理：过滤掉长度为1的会话，出现次数小于10的物品。序列最后一位作为label，前面的用来建模用户偏好。时间顺序，前70%作为训练集，后20%作为验证集，最后10%作为测试集。</p>
<h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p>&emsp;&emsp;Precision、MRR，@10，@20.</p>
<h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><h4 id="RQ1：SOTA了吗？"><a href="#RQ1：SOTA了吗？" class="headerlink" title="RQ1：SOTA了吗？"></a>RQ1：SOTA了吗？</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405130908396.png" alt="image-20220405130908396"></p>
<p>&emsp;&emsp;SOTA了。进一步分析：（1）NARM和Bert4Rec比GRU4Rec好的原因，注意力机制，但Bert4Rec效果没有达到预期的原因是，序列比较短；（2）SR-GNN、LESSR在图上建模 pairwise relation，$S^2-$DHCN和COTReec与之相比建模了比成对关系更高阶的关系，所以表现更好，但是也局限建模在单一类型交互上。CoHNN在此基础上用更多的特征建模了价格和兴趣偏好，效果更好。</p>
<h4 id="RQ2：价格因素的影响如何？"><a href="#RQ2：价格因素的影响如何？" class="headerlink" title="RQ2：价格因素的影响如何？"></a>RQ2：价格因素的影响如何？</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405131901933.png" alt="image-20220405131901933" style="zoom:67%;" /></p>
<h5 id="价格特征和价格偏好的影响"><a href="#价格特征和价格偏好的影响" class="headerlink" title="价格特征和价格偏好的影响"></a>价格特征和价格偏好的影响</h5><p>&emsp;&emsp;CoHHN-c：去掉category特征，CoHHN-p：去掉价格特征（自然也没有协同指导学习？），CoHHN-pp：节点更新时考虑价格特征但没有提取价格偏好（自然也没有协同指导学习？）。</p>
<p>（1）CoHHN-pp &gt; CoHHN-p、CoHHN-c &gt; CoHHN-p ：加入价格特征有用；</p>
<p>（2）CoHHN &gt; CoHHN-pp ：考虑价格偏好有用；</p>
<p>（3）CoHHN &gt; CoHHN-c：考虑类别特征有用，可以更准确地学习价格偏好。</p>
<h5 id="不同分布划分的影响"><a href="#不同分布划分的影响" class="headerlink" title="不同分布划分的影响"></a>不同分布划分的影响</h5><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405133111306.png" alt="image-20220405133111306" style="zoom: 67%;" /></p>
<p>&emsp;&emsp;logistic 分布 &gt; 均匀分布 ，logistic 分布能更准确反映价格信息。</p>
<p>&emsp;&emsp;均匀分布 &gt; COTREC ，考虑价格因素，即使用均匀分布也高于之前的 SOTA ，说明考虑价格因素的有效性。</p>
<h3 id="RQ3：协同指导学习方法的影响如何？"><a href="#RQ3：协同指导学习方法的影响如何？" class="headerlink" title="RQ3：协同指导学习方法的影响如何？"></a>RQ3：协同指导学习方法的影响如何？</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405133652178.png" alt="image-20220405133652178" style="zoom:67%;" /></p>
<p>&emsp;&emsp;CoHHN-co：对价格偏好和兴趣偏好简单加和，没用协同指导学习方法。</p>
<p>&emsp;&emsp;CoHHN &gt; CoHHN-co ：证明价格偏好和兴趣偏好之间的关系复杂，简单加和无法提取。协同指导学习的方法可以。</p>
<h3 id="RQ4：不同价格-level-的影响如何？"><a href="#RQ4：不同价格-level-的影响如何？" class="headerlink" title="RQ4：不同价格 level 的影响如何？"></a>RQ4：不同价格 level 的影响如何？</h3><p>&emsp;&emsp;也就是每个类别的价格分的那几个桶，各自的表现如何。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405134334807.png" alt="image-20220405134334807" style="zoom:67%;" /></p>
<p>&emsp;&emsp;CoHHN在所有 level 都优于 COTREC，可能的原因是，离散化价格的方法可以优化价格分配。</p>
<p>&emsp;&emsp;其次，在中低价位，两个模型表现都比较好，这和大多数人的购物习惯一致，倾向于购买中低价位的商品。</p>
<h3 id="RQ5：关键超参数的影响如何？"><a href="#RQ5：关键超参数的影响如何？" class="headerlink" title="RQ5：关键超参数的影响如何？"></a>RQ5：关键超参数的影响如何？</h3><h4 id="价格-level-的数量的影响"><a href="#价格-level-的数量的影响" class="headerlink" title="价格 level 的数量的影响"></a>价格 level 的数量的影响</h4><p>&emsp;&emsp;也就是每个类别，价格各分几个桶，决定了建模的价格粒度。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405134742217.png" alt="image-20220405134742217" style="zoom: 67%;" /></p>
<p>（1）越低粒度越粗，但是太高会使得价格相似的物品也区分开，就没有意义了。</p>
<p>（2）其次，不同数据集价格分布不同，也会导致最优效果的粒度不同。</p>
<p>（3）从绝对值来看，三个数据集接近最优 number of level 的附近，呈现出稳定的性能。本文认为，得益于所提出的离散化方法。</p>
<h4 id="双通道聚合迭代次数"><a href="#双通道聚合迭代次数" class="headerlink" title="双通道聚合迭代次数"></a>双通道聚合迭代次数</h4><p>&emsp;&emsp;类似GNN层数。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405135551415.png" alt="image-20220405135551415" style="zoom: 67%;" /></p>
<p>（1）2、3层性能最好，层数过多容易过平滑。</p>
<p>（2）Amazon 商品个数最少，所以达到最优性能需要的迭代次数最少。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>&emsp;&emsp;异质超图=异质图+超图：异质图——融合异质特征，超图——捕捉高阶依赖</p>
<p>&emsp;&emsp;双通道聚合：intra + inter ，聚合同类和异类信息，获得价格和兴趣偏好的初级表示</p>
<p>&emsp;&emsp;协同指导学习：捕捉价格偏好和兴趣偏好之间的复杂联系，获得语义增强的价格和兴趣偏好的表示</p>
<h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>&emsp;&emsp;融合更多特征，了例如品牌，卖家声誉等等。该模型可以适用于很多需要挖掘异质信息的任务。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>会话推荐</tag>
        <tag>CoHHN</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker容器数据卷</title>
    <url>/2022/04/05/Docker%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405151429046.png" alt="image-20220405151429046" style="zoom:50%;" /></p>
<h2 id="什么是数据卷"><a href="#什么是数据卷" class="headerlink" title="什么是数据卷"></a>什么是数据卷</h2><p>数据不应该放在容器中，如果容器删除，数据就会丢失！==需求：数据 持久化==</p>
<p>MYSQL，容器删了=删库跑路！==需求：MYSQL数据可以存储在本地==</p>
<p>==&gt; 需要容器之间有一个数据共享的技术！Docker容器中产生的数据，同步到本地！</p>
<p>这就是卷技术！说白了就是目录的挂载，将容器内的目录，挂载到 Linux 上</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405151429046.png" alt="image-20220405151429046" style="zoom:50%;" /></p>
<p><strong>总结：容器的持久化和同步操作！ 容器间也可以数据共享！</strong></p>
<h2 id="使用数据卷"><a href="#使用数据卷" class="headerlink" title="使用数据卷"></a>使用数据卷</h2><blockquote>
<p>方式一：直接使用命令来挂载 -V</p>
</blockquote>
<span id="more"></span>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run -it -v 主机目录：容器内目录 -p 主机端口：容器内端口 </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~$ sudo docker run -it -v /home/dutir/xxx/ceshi:/home centos /bin/bash</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看容器id</span></span><br><span class="line">xxx@data:~$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                  PORTS                                                  NAMES</span><br><span class="line">43b6593461e7        centos                     &quot;/bin/bash&quot;              2 minutes ago       Up 2 minutes                                                                   quizzical_varahamihira</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看容器详细信息</span></span><br><span class="line">xxx@data:~$ sudo docker inspect 43b6593461e7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 看到挂载</span></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405153329429.png" alt="image-20220405153329429" style="zoom:67%;" />、</p>
<p><strong>测试文件的同步</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183512106.png" alt="image-20220405183512106" style="zoom:80%;" /></p>
<p><strong>再来测试</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.停止容器</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.宿主机上修改文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.启动容器</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.容器内的数据依旧是同步的</span></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183540730.png" alt="image-20220405183540730"></p>
<p>好处：我们以后修改只需要在本地修改即可，容器内会自动同步！</p>
<h2 id="实战：安装MySQL"><a href="#实战：安装MySQL" class="headerlink" title="实战：安装MySQL"></a>实战：安装MySQL</h2><p>思考：MySQL 的数据持久化问题，</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405160002622.png" alt="image-20220405160002622" style="zoom: 67%;" /></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取镜像</span></span><br><span class="line">docker pull mysql:5.7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行容器，需要做数据挂载！  <span class="comment"># 安装启动mysql。需要配置密码，这是要注意的，去docker hub官方文档看（上图）</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 官方测试 $ docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -e 配置环境，这里要配置密码</span> </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动我们的</span></span><br><span class="line">-d 后台运行</span><br><span class="line">-p 端口映射</span><br><span class="line">-v 数据卷挂载，可挂载多个</span><br><span class="line">-e 环境配置</span><br><span class="line">--name 容器名字</span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker run -d -p 3310:3306 -v /home/dutir/xxx/mysql_test/conf:/etc/mysql/conf.d -v /home/dutir/xxx/mysql_test/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动成功之后，我们在本地使用 sqlyog 连接测试</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sqlyog 连接到本地的3310，本地3310侦听服务器宿主机的3310，宿主机的3310和容器内的3306映射，这个时候就可以连接上了</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在本地sqlyog测试创建一个数据库，查看一下我们映射的路径是否ok！</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> test_for_docker以后，docker里多出这个数据库</span></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183614297.png" alt="image-20220405183614297" style="zoom:67%;" /></p>
<p>即使把容器删除</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183721449.png" alt="image-20220405183721449" style="zoom:67%;" /></p>
<p>发现，我们挂载到本地的数据卷依旧没有丢失，这就实现了容器持久化功能！</p>
<h2 id="具名和匿名挂载"><a href="#具名和匿名挂载" class="headerlink" title="具名和匿名挂载"></a>具名和匿名挂载</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 匿名挂载（不指定主机名）</span></span><br><span class="line">-v 容器内路径</span><br><span class="line">docker run -d -P --name nginx01 -v /etc/nginx nginx</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有本地volume（数据卷）	乱码的都是匿名卷</span></span><br><span class="line">xxx@data:~$ sudo docker volume ls</span><br><span class="line">DRIVER              VOLUME NAME</span><br><span class="line">local               51ed233d17e5c740a92f70e075335dd59cbcf913dd03390d64880893a6a6b043</span><br><span class="line">local               mysql_lawbda_my-db</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里发现，这种就是匿名挂在，我们在 -v 时只写了容器内的路径，没有写容器外的路径！</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 具名挂载</span></span><br><span class="line">docker run -d -P --name nginx01 -v juming-nginx:/etc/nginx nginx</span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker volume ls</span><br><span class="line">DRIVER              VOLUME NAME</span><br><span class="line">local               51ed233d17e5c740a92f70e075335dd59cbcf913dd03390d64880893a6a6b043</span><br><span class="line">local				juming-nginx</span><br><span class="line">local               mysql_lawbda_my-db</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过 -v 卷名：容器内路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看一下这个卷（这里查看mysql_lawbda_my-db）</span></span><br><span class="line">xxx@data:~$ sudo docker volume inspect mysql_lawbda_my-db</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;CreatedAt&quot;: &quot;2022-04-04T17:14:49+08:00&quot;,</span><br><span class="line">        &quot;Driver&quot;: &quot;local&quot;,</span><br><span class="line">        &quot;Labels&quot;: &#123;</span><br><span class="line">            &quot;com.docker.compose.project&quot;: &quot;mysql_lawbda&quot;,</span><br><span class="line">            &quot;com.docker.compose.version&quot;: &quot;1.27.4&quot;,</span><br><span class="line">            &quot;com.docker.compose.volume&quot;: &quot;my-db&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/mysql_lawbda_my-db/_data&quot;,</span><br><span class="line">        &quot;Name&quot;: &quot;mysql_lawbda_my-db&quot;,</span><br><span class="line">        &quot;Options&quot;: null,</span><br><span class="line">        &quot;Scope&quot;: &quot;local&quot;</span><br></pre></td></tr></table></figure>
<p>所有达到docker容器内的卷，没有指定目录的情况下都是在<code>/val/lib/docker/volumes/xxxx/_data</code></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405164118444.png" alt="image-20220405164118444" style="zoom: 67%;" /></p>
<p>我们通过具名挂载 可以方便找到我们的一个卷，大多数情况在使用<code>具名挂载</code> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如何确定是具名挂载还是匿名挂在，还是指定路径挂载！</span></span><br><span class="line"></span><br><span class="line">-v 容器内路径				 # 匿名挂载</span><br><span class="line">-v 卷名:容器内路径    			# 具名挂载</span><br><span class="line">-v /宿主机路径:容器内路径	   	  # 指定路径挂载 </span><br></pre></td></tr></table></figure>
<p>拓展：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 通过 -v 容器内路径:ro  rw  改变读写权限</span></span><br><span class="line">ro	read only	# 只读</span><br><span class="line">rw  readwrite	# 可读可写</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一旦设置了容器权限，容器对我们挂载出来的内容就有限定了！</span></span><br><span class="line">docker run -d -P --name nginx01 -v juming-nginx:/etc/nginx:ro nginx</span><br><span class="line">docker run -d -P --name nginx01 -v juming-nginx:/etc/nginx:rw nginx</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ro  只要看到ro，就说明这个路径只能通过宿主机来改变，容器内部是无法操作的</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="初识Dockerfile"><a href="#初识Dockerfile" class="headerlink" title="初识Dockerfile"></a>初识Dockerfile</h2><blockquote>
<p>方式二：生成镜像的时候就挂载出来</p>
</blockquote>
<p>Dockerfile 就是用来构建 docker 镜像的构建文件！ 就是一段命令脚本！先体验一下！</p>
<p>通过这个脚本可以生成镜像，镜像是一层一层的，脚本就是一个一个的命令，每个命令就是一层。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建一个dockerfile文件，名字可以随机， 建议 Dockerfile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件中的内容  指令（都是大写） 参数</span></span><br><span class="line"></span><br><span class="line">FROM centos</span><br><span class="line"></span><br><span class="line">VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]	</span><br><span class="line"></span><br><span class="line">CMD echo &quot;----end----&quot;</span><br><span class="line"></span><br><span class="line">CMD /bin/bash</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里的每个命令，就是镜像的一层！</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker build 构建镜像</span></span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# docker build -f /home/dutir/xxx/docker-test-volume/dockerfile1 -t wjm/centos:1.0 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/4 : FROM centos</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 5d0da3dc9764</span></span><br><span class="line">Step 2/4 : VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> eedc2cf7a5b3</span></span><br><span class="line">Removing intermediate container eedc2cf7a5b3</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 80396b388983</span></span><br><span class="line">Step 3/4 : CMD echo &quot;----end----&quot;</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 734dc31604b0</span></span><br><span class="line">Removing intermediate container 734dc31604b0</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 5ef9da00ac40</span></span><br><span class="line">Step 4/4 : CMD /bin/bash</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 593084c92533</span></span><br><span class="line">Removing intermediate container 593084c92533</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 8edda645979e</span></span><br><span class="line">Successfully built 8edda645979e</span><br><span class="line">Successfully tagged wjm/centos:1.0</span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动一下自己生成的镜像</span></span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# docker images</span><br><span class="line">REPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">wjm/centos                  1.0                 8edda645979e        4 minutes ago       231MB</span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# docker run -it 8edda645979e /bin/bash</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405170430808.png" alt="image-20220405170430808"></p>
<p>这个目录就是我们生成镜像时自动挂载的，数据卷目录。</p>
<p>这个和外部一定有一个同步的目录！dockerfile里只指定了容器内目录，一定是匿名挂载，所以容器外、宿主机上肯定有乱码的挂载。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405170800369.png" alt="image-20220405170800369" style="zoom: 67%;" /></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker inspect 查看容器信息</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 找到 Mounts，查看卷挂载的路径，符合匿名挂载的默认路径/var/lib/docker/volumes/xxx/</span></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405171126981.png" alt="image-20220405171126981" style="zoom:67%;" /></p>
<p>在容器里的 volume01 下新建一个 container.txt 文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@4b4b467cf777 /]# ls</span><br><span class="line">bin  etc   lib	  lost+found  mnt  proc  run   srv  tmp  var	   volume02</span><br><span class="line">dev  home  lib64  media       opt  root  sbin  sys  usr  volume01</span><br><span class="line">[root@4b4b467cf777 /]# cd volume01</span><br><span class="line">[root@4b4b467cf777 volume01]# ls</span><br><span class="line">[root@4b4b467cf777 volume01]# touch container.txt</span><br><span class="line">[root@4b4b467cf777 volume01]# ls</span><br><span class="line">container.txt</span><br><span class="line">[root@4b4b467cf777 volume01]# </span><br></pre></td></tr></table></figure>
<p>测试一下刚才的文件是否同步出去。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 复制 Mounts 下的路径 /var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> 到这里</span></span><br><span class="line"></span><br><span class="line">root@data:~# cd /var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data</span><br><span class="line">root@data:/var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data# ls</span><br><span class="line">container.txt		# 发现新建的 container.txt 文件</span><br><span class="line">root@data:/var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data#</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这种方式我们未来使用的十分多，因为我们通常会构建自己的镜像！</p>
<p>假设构建镜像时没有挂载卷，要手动镜像挂载 -v 卷名:容器内路径 </p>
<h2 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h2><blockquote>
<p>即容器和容器间同步</p>
</blockquote>
<p>多个 Mysql 数据如何同步？</p>
<p>==所有 docker 之间共享的卷都是独立的，每个docker有自己的数据卷。新建时拷贝父容器的备份。所以只要有一份存在，数据都不会丢失。==</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405172422841.png" alt="image-20220405172422841"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动3个容器，通过我们刚才自己写的镜像启动</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 首先启动 docker01</span> </span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405172959557.png" alt="image-20220405172959557"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动 docker02，volume 从 docker01 继承</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker01 就叫做数据卷容器</span></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405173218738.png" alt="image-20220405173218738"></p>
<p>在 docker01 修改数据， docker02 会同步吗？测试一下。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405173548023.png" alt="image-20220405173548023"></p>
<p>回到 docker02 查看</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405173743825.png" alt="image-20220405173743825"></p>
<p>docker01 创建的内容同步到 docker02 了</p>
<p>再从 docker01 继承，启动 docker03，新建 docker03.txt</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405174426597.png" alt="image-20220405174426597"></p>
<p>在 docker03 里创建的文件，在 docker01 和 docker02 里都同步了。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405175122829.png" alt="image-20220405175122829"></p>
<p>只要是通过 —volume-from ，我们就可以实现容器间的数据共享。</p>
<p>把 docker01 整个 rm 掉，docker02 和 docker03 的文件都还在。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 测试：可以删除docker01，查看一下docker02和docker03是否还可以访问这个文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 依旧可以访问</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试：查看宿主机的volume，还是存在</span></span><br><span class="line">root@data:~# cd /var/lib/docker/volumes/2bdf8b38df84ffea344d0f758e7be2c2045c54f270a9da362b12246a1d78d636/_data</span><br><span class="line">root@data:/var/lib/docker/volumes/2bdf8b38df84ffea344d0f758e7be2c2045c54f270a9da362b12246a1d78d636/_data# ls</span><br><span class="line">docker01.txt  docker03.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>==所有 docker 之间共享的卷都是独立的，每个docker有自己的数据卷。新建时 <strong>拷贝</strong> 父容器的备份。所以只要有一份存在，数据都不会丢失。==</p>
<p>回到问题：多个 Mysql 数据如何同步？</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> -v 容器内路径，匿名挂载</span></span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker run -d -p 3310:3306 -v /etc/mysql/conf.d -v /var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 用 --volumes-from 可以实现两个容器同步</span></span><br><span class="line">xxx@data:~$ sudo docker run -d -p 3310:3306  -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 --volumes-from mysql01 mysql:5.7</span><br></pre></td></tr></table></figure>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止（所有容器都停止）。</p>
<p>但是一旦你持久化到了本地，这个时候，本地的数据是不会删除的。所以重要的文件存在宿主机上就好啦。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以上是看 b 站教学视频记的笔记。</p>
<p>教程地址：<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=9">【狂神说Java】Docker最新超详细版教程通俗易懂_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker镜像原理</title>
    <url>/2022/04/05/Docker%E9%95%9C%E5%83%8F%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404192342184.png" alt="image-20220404192342184" style="zoom:67%;" /></p>
<h2 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h2><p>镜像：一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，包含运行某个软件所需的所有内容，包括代码、库、环境变量和配置文件。</p>
<p>每个文件叠加过后就是我们的应用，虽然是叠加而来，但是对外却是一个整体的系统文件</p>
<h2 id="镜像加载原理"><a href="#镜像加载原理" class="headerlink" title="镜像加载原理"></a>镜像加载原理</h2><blockquote>
<p>UFS文件系统</p>
</blockquote>
<p>下载时看到一层层的就是这个。</p>
<p><strong>联合文件系统（UnionFS）</strong>是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。另外，不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的改动层，大大提高了存储的效率。</p>
<p><strong>特性</strong>：一次性同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。</p>
<blockquote>
<p>Docker镜像加载原理</p>
</blockquote>
<span id="more"></span>
<ul>
<li>bootfs(boot file system) 主要包含bootloader和kernel, bootloader 主要是 <strong>引导加载 </strong>kernel,当我们加载镜像的时候，会通过bootloader加载kernal，Docker镜像最底层是bootfs，当boot加载完成后整个kernal内核都在内存中了，bootfs也就可以卸载，值得注意的是，bootfs是被所有镜像共用的，许多镜像images都是在base image(rootfs)基础上叠加的 .</li>
</ul>
<ul>
<li>rootfs (root file system)，在bootfs之 上.包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是<strong>各种不同的操作系统发行版</strong>，比如Ubuntu, Centos等等 。所以说每个 docker 就是一个小的虚拟机环境。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404192342184.png" alt="image-20220404192342184" style="zoom:67%;" /></p>
<p>Linux 不同发行版之间，bootfs 都是一样的，只有 rootfs 不一样 。 所以当 Docker 镜像加载时， bootfs 直接使用宿主机的内核，只需要提供 rootfs ，这部分十分精简，所以 centos 镜像可以很小（才不到300M），并且加载速度很快（虚拟机启动分钟级，容器启动秒级）。</p>
<h2 id="分层理解"><a href="#分层理解" class="headerlink" title="分层理解"></a>分层理解</h2><blockquote>
<p> 镜像分层</p>
</blockquote>
<p>用 <code>docker inspect</code> 观察下载下来的镜像：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404194829940.png" alt="image-20220404194829940" style="zoom: 50%;" /></p>
<p>这里的 Layers 就是每一层的文件，UFS叠加以后成为整个镜像。</p>
<blockquote>
<p>特性</p>
</blockquote>
<p>以 tomcat 镜像为例，它是一个有6个层级的镜像，pull到本地，再创建一个新的容器，此时整个 tomcat 会作为一整个镜像层，而你做的所有操作都会记录在容器层。如果想保存新的镜像，镜像层和容器层会再次打包，形成一个新的镜像。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404194637448.png" alt="image-20220404194637448"></p>
<h2 id="commit镜像"><a href="#commit镜像" class="headerlink" title="commit镜像"></a>commit镜像</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker commit 提交热熔器成为一个新的副本</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 命令和git类似</span></span><br><span class="line">docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器id 目标镜像名：[TAG]</span><br></pre></td></tr></table></figure>
<p>测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.启动一个默认的tomcat</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.发现这个默认的tomcat 是没有webapps应用，这是镜像的原因，官方的镜像默认 webapps下面是没有文件的</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.自己拷贝了一些基本文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.将修改过的容器通过commit提交为一个镜像！我们以后就是用修改过的镜像就可以，这就是我们自己的一个修改过的镜像</span></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183351927.png" alt="image-20220405183351927" style="zoom:80%;" /></p>
<p>入门Docker！接下来，容器数据卷！DokcerFile！Docker网络！</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以上是看 b 站教学视频记的笔记。</p>
<p>教程地址：<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=9">【狂神说Java】Docker最新超详细版教程通俗易懂_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker常用命令</title>
    <url>/2022/04/05/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a>帮助命令</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker version		# 显示docker 的版本信息</span><br><span class="line">docker info			# 显示docker 的系统信息，包括镜像和容器的数量</span><br><span class="line">docker 命令 --help 	# 帮助命令</span><br></pre></td></tr></table></figure>
<p>帮助文档的地址：<a href="https://docs.docker.com/reference/">参考文档|Docker 文档</a></p>
<h2 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h2><p><strong>docker images 查看所有本地的主机上的镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xxx@data:~/solrdata$ sudo docker images </span><br><span class="line">REPOSITORY                        TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">solr                              8                   ce1fcccc6f5e        5 days ago          563MB</span><br><span class="line">solr                              latest              ce1fcccc6f5e        5 days ago          563MB</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解释</span></span><br><span class="line">REPOSITORY 镜像的仓库源</span><br><span class="line">TAG        镜像的标签</span><br><span class="line">IMAGE ID   镜像的id</span><br><span class="line">CREATE     镜像的创建时间</span><br><span class="line">SIZE       镜像的大小</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可选项</span></span><br><span class="line">Options:</span><br><span class="line">  -a, --all             # 列出所有镜像</span><br><span class="line">  -q, --quiet           # 只显示镜像的id</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker images -aq</span><br><span class="line">ce1fcccc6f5e</span><br><span class="line">ce1fcccc6f5e</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p><strong>docker search 搜索镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xxx@data:~/solrdata$ sudo docker search mysql</span><br><span class="line">NAME                             DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">mysql                            MySQL is a widely used, open-source relation…   12350               [OK]                </span><br><span class="line">mariadb                          MariaDB Server is a high performing open sou…   4753                [OK]                </span><br><span class="line">mysql/mysql-server               Optimized MySQL Server Docker images. Create…   916                                     [OK]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可选项，通过收藏来过滤</span></span><br><span class="line">--filter=STARS=3000		# 搜索出来的镜像就是STARS大于3000的</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker search mysql --filter=STARS=3000</span><br><span class="line">NAME                DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">mysql               MySQL is a widely used, open-source relation…   12350               [OK]                </span><br><span class="line">mariadb             MariaDB Server is a high performing open sou…   4753                [OK]                </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>docker pull 下载镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载镜像 docker pull 镜像名[:tag]</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker pull mysql</span><br><span class="line">Using default tag: latest	# 如果不写tag,就默认latest最新版</span><br><span class="line">latest: Pulling from library/mysql</span><br><span class="line">f003217c5aae: Pull complete 	# 分层下载,docker image的核心 联合文件系统</span><br><span class="line">65d94f01a09f: Pull complete </span><br><span class="line">43d78aaa6078: Pull complete </span><br><span class="line">a0f91ffbdf69: Pull complete </span><br><span class="line">59ee9e07e12f: Pull complete </span><br><span class="line">04d82978082c: Pull complete </span><br><span class="line">70f46ebb971a: Pull complete </span><br><span class="line">db6ea71d471d: Pull complete </span><br><span class="line">c2920c795b25: Pull complete </span><br><span class="line">26c3bdf75ff5: Pull complete </span><br><span class="line">9ec1f1f78b0e: Pull complete </span><br><span class="line">4607fa685ac6: Pull complete </span><br><span class="line">Digest: sha256:1c75ba7716c6f73fc106dacedfdcf13f934ea8c161c8b3b3e4618bcd5fbcf195	# 签名</span><br><span class="line">Status: Downloaded newer image for mysql:latest</span><br><span class="line">docker.io/library/mysql:latest	# 真实地址</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 等价于它</span></span><br><span class="line">docker pull mysql</span><br><span class="line">docker pull docker.io/library/mysql:latest</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定版本下载</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker pull mysql:5.7</span><br><span class="line">5.7: Pulling from library/mysql</span><br><span class="line">f003217c5aae: Already exists </span><br><span class="line">65d94f01a09f: Already exists </span><br><span class="line">43d78aaa6078: Already exists </span><br><span class="line">a0f91ffbdf69: Already exists </span><br><span class="line">59ee9e07e12f: Already exists </span><br><span class="line">04d82978082c: Already exists </span><br><span class="line">70f46ebb971a: Already exists 	</span><br><span class="line">ba61822c65c2: Pull complete 	# 指定版本只需要下载和之前不同的部分</span><br><span class="line">dec59acdf78a: Pull complete </span><br><span class="line">0a05235a6981: Pull complete </span><br><span class="line">c87d621d6916: Pull complete </span><br><span class="line">Digest: sha256:1a73b6a8f507639a8f91ed01ace28965f4f74bb62a9d9b9e7378d5f07fab79dc</span><br><span class="line">Status: Downloaded newer image for mysql:5.7</span><br><span class="line">docker.io/library/mysql:5.7</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183232675.png" alt="image-20220405183232675"></p>
<p><strong>docker rmi 删除镜像</strong></p>
<p>rmi=rm 删除 + i 镜像(image),删除镜像</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rmi 镜像id 					# 删除指定镜像</span><br><span class="line">docker rmi 镜像id 镜像id 镜像id	 	 # 删除多个镜像</span><br><span class="line">docker rmi -f $(docker images -aq)	 # 删除全部镜像</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker rmi f26e21ddd20d	# 镜像id</span><br></pre></td></tr></table></figure>
<h2 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h2><p><strong>说明:我们有了镜像才可以创建容器,linux,下载一个centos镜像来测试学习</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull centos</span><br></pre></td></tr></table></figure>
<p><strong>新建容器并启动</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run [可选参数] image</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 参数说明</span></span><br><span class="line">--name=“Nmae” 	容器名字  solr01 solr02 用来区分容器</span><br><span class="line">-d            	后台方式运行，类型nohup，docker里只要加上-d</span><br><span class="line">-it				使用交互方式运行，进入容器查看内容</span><br><span class="line">-p				指定容器的端口 -p 8080:8080</span><br><span class="line">	-p ip:主机端口：容器端口</span><br><span class="line">	-p 主机端口:容器端口（容器）</span><br><span class="line">	-p 容器端口</span><br><span class="line">	容器端口</span><br><span class="line">-P				随机指定端口</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试,启动并进入容器 -it交互模式</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker run -it centos /bin/bash</span><br><span class="line">[root@17e53f625cbc /]# ls	# 查看容器内部的centos,基础版本,很多命令是不完善的</span><br><span class="line">bin  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 从容器中退回主机</span></span><br><span class="line">[root@17e53f625cbc /]# exit</span><br><span class="line">exit</span><br><span class="line">xxx@data:~/solrdata$ ls</span><br><span class="line">data  log4j2.xml  logs</span><br><span class="line">xxx@data:~/solrdata$ </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>列出所有运行中的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker ps 命令</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> 列出当前正在运行的容器</span></span><br><span class="line">-a		# 列出当前正在运行的容器+带出历史运行过的容器</span><br><span class="line">-n=?	# 显示最近创建的容器</span><br><span class="line">-q		# 只显示容器的编号</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS               PORTS               NAMES</span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS               PORTS               NAMES</span><br><span class="line">17e53f625cbc        centos                                                 &quot;/bin/bash&quot;              3 minutes ago       Exited (0) 58 seconds ago                                                            magical_haibt</span><br></pre></td></tr></table></figure>
<p><strong>退出容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">exit	# 直接容器停止并退出</span><br><span class="line">Crtl + P + Q # 容器不停止退出</span><br></pre></td></tr></table></figure>
<p><strong>删除容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rm 容器id					# 删除指定容器,不能删除正在运行的容器,如果要强制删除,rm -f</span><br><span class="line">docker rm -f $(docker ps -aq)	 # 删除所有容器</span><br><span class="line">docker ps -a -q|xargs docker rm	 # 删除所有容器(高级)</span><br></pre></td></tr></table></figure>
<p><strong>启动和停止容器的操作</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker start 容器id		# 启动容器</span><br><span class="line">docker restart 容器id		# 重启容器</span><br><span class="line">docker stop 容器id		# 停止当前正在运行的容器</span><br><span class="line">docker kill 容器id		# 强制停止当前容器</span><br></pre></td></tr></table></figure>
<h2 id="常用其他命令"><a href="#常用其他命令" class="headerlink" title="常用其他命令"></a>常用其他命令</h2><p><strong>后台启动容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker run -d 镜像名</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker run -d centos</span><br><span class="line">7c5bca7cedbe3d4c32aa5273b9068a9d154a2e5f7a665f5861f8369a1a592862</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 问题 docker ps 发现centos停止了</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS               PORTS               NAMES</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 常见的坑,docker 容器使用后台运行,就必须要有一个前台进程,docker发现没有应用,就会自动停止</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> nginx,容器启动后,发现自己没有提供服务,就会立刻停止,就是没有程序了</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>查看日志</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 可选项</span></span><br><span class="line">Options:</span><br><span class="line">  -f, --follow         Follow log output</span><br><span class="line">      --tail string    Number of lines to show from the end of the logs (default &quot;all&quot;)</span><br><span class="line">  -t, --timestamps     Show timestamps</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker logs -f -t 508f4663f65e # 没有日志</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自己编写一段shell脚本</span></span><br><span class="line">&quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line">xxx@data:~/solrdata$ sudo docker run -d centos /bin/bash -c &quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                  PORTS                                                  NAMES</span><br><span class="line">5a96132edf5d        centos                     &quot;/bin/bash -c &#x27;while…&quot;   55 seconds ago      Up 53 seconds                                                                  priceless_elbakyan</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示日志</span></span><br><span class="line"> -tf		   # 显示日志</span><br><span class="line"> --tail number # 要显示的日志条数</span><br></pre></td></tr></table></figure>
<p><strong>查看容器中的进程信息</strong> ps</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker top 容器id</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker top 5a96132edf5d</span><br><span class="line">UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD</span><br><span class="line">root                4109297             4109277             0                   15:33               ?                   00:00:00            /bin/bash -c while true;do echo wjm;sleep 1;done</span><br><span class="line">root                4112148             4109297             0                   15:40               ?                   00:00:00            /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 1</span><br></pre></td></tr></table></figure>
<p><strong>查看镜像的元数据</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker inspect 容器id</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker inspect 5a96132edf5d</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;Id&quot;: &quot;5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67&quot;,</span><br><span class="line">        &quot;Created&quot;: &quot;2022-04-04T07:33:47.224853805Z&quot;,</span><br><span class="line">        &quot;Path&quot;: &quot;/bin/bash&quot;,</span><br><span class="line">        &quot;Args&quot;: [</span><br><span class="line">            &quot;-c&quot;,</span><br><span class="line">            &quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;State&quot;: &#123;</span><br><span class="line">            &quot;Status&quot;: &quot;running&quot;,</span><br><span class="line">            &quot;Running&quot;: true,</span><br><span class="line">            &quot;Paused&quot;: false,</span><br><span class="line">            &quot;Restarting&quot;: false,</span><br><span class="line">            &quot;OOMKilled&quot;: false,</span><br><span class="line">            &quot;Dead&quot;: false,</span><br><span class="line">            &quot;Pid&quot;: 4109297,</span><br><span class="line">            &quot;ExitCode&quot;: 0,</span><br><span class="line">            &quot;Error&quot;: &quot;&quot;,</span><br><span class="line">            &quot;StartedAt&quot;: &quot;2022-04-04T07:33:48.561816958Z&quot;,</span><br><span class="line">            &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Image&quot;: &quot;sha256:5d0da3dc976460b72c77d94c8a1ad043720b0416bfc16c52c45d4847e53fadb6&quot;,</span><br><span class="line">        &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/resolv.conf&quot;,</span><br><span class="line">        &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/hostname&quot;,</span><br><span class="line">        &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/hosts&quot;,</span><br><span class="line">        &quot;LogPath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67-json.log&quot;,</span><br><span class="line">        &quot;Name&quot;: &quot;/priceless_elbakyan&quot;,</span><br><span class="line">        &quot;RestartCount&quot;: 0,</span><br><span class="line">        &quot;Driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">        &quot;Platform&quot;: &quot;linux&quot;,</span><br><span class="line">        &quot;MountLabel&quot;: &quot;&quot;,</span><br><span class="line">        &quot;ProcessLabel&quot;: &quot;&quot;,</span><br><span class="line">        &quot;AppArmorProfile&quot;: &quot;docker-default&quot;,</span><br><span class="line">        &quot;ExecIDs&quot;: null,</span><br><span class="line">        &quot;HostConfig&quot;: &#123;</span><br><span class="line">            &quot;Binds&quot;: null,</span><br><span class="line">            &quot;ContainerIDFile&quot;: &quot;&quot;,</span><br><span class="line">            &quot;LogConfig&quot;: &#123;</span><br><span class="line">                &quot;Type&quot;: &quot;json-file&quot;,</span><br><span class="line">                &quot;Config&quot;: &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;NetworkMode&quot;: &quot;default&quot;,</span><br><span class="line">            &quot;PortBindings&quot;: &#123;&#125;,</span><br><span class="line">            &quot;RestartPolicy&quot;: &#123;</span><br><span class="line">                &quot;Name&quot;: &quot;no&quot;,</span><br><span class="line">                &quot;MaximumRetryCount&quot;: 0</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;AutoRemove&quot;: false,</span><br><span class="line">            &quot;VolumeDriver&quot;: &quot;&quot;,</span><br><span class="line">            &quot;VolumesFrom&quot;: null,</span><br><span class="line">            &quot;CapAdd&quot;: null,</span><br><span class="line">            &quot;CapDrop&quot;: null,</span><br><span class="line">            &quot;Capabilities&quot;: null,</span><br><span class="line">            &quot;Dns&quot;: [],</span><br><span class="line">            &quot;DnsOptions&quot;: [],</span><br><span class="line">            &quot;DnsSearch&quot;: [],</span><br><span class="line">            &quot;ExtraHosts&quot;: null,</span><br><span class="line">            &quot;GroupAdd&quot;: null,</span><br><span class="line">            &quot;IpcMode&quot;: &quot;private&quot;,</span><br><span class="line">            &quot;Cgroup&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Links&quot;: null,</span><br><span class="line">            &quot;OomScoreAdj&quot;: 0,</span><br><span class="line">            &quot;PidMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Privileged&quot;: false,</span><br><span class="line">            &quot;PublishAllPorts&quot;: false,</span><br><span class="line">            &quot;ReadonlyRootfs&quot;: false,</span><br><span class="line">            &quot;SecurityOpt&quot;: null,</span><br><span class="line">            &quot;UTSMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;UsernsMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;ShmSize&quot;: 67108864,</span><br><span class="line">            &quot;Runtime&quot;: &quot;runc&quot;,</span><br><span class="line">            &quot;ConsoleSize&quot;: [</span><br><span class="line">                0,</span><br><span class="line">                0</span><br><span class="line">            ],</span><br><span class="line">            &quot;Isolation&quot;: &quot;&quot;,</span><br><span class="line">            &quot;CpuShares&quot;: 0,</span><br><span class="line">            &quot;Memory&quot;: 0,</span><br><span class="line">            &quot;NanoCpus&quot;: 0,</span><br><span class="line">            &quot;CgroupParent&quot;: &quot;&quot;,</span><br><span class="line">            &quot;BlkioWeight&quot;: 0,</span><br><span class="line">            &quot;BlkioWeightDevice&quot;: [],</span><br><span class="line">            &quot;BlkioDeviceReadBps&quot;: null,</span><br><span class="line">            &quot;BlkioDeviceWriteBps&quot;: null,</span><br><span class="line">            &quot;BlkioDeviceReadIOps&quot;: null,</span><br><span class="line">            &quot;BlkioDeviceWriteIOps&quot;: null,</span><br><span class="line">            &quot;CpuPeriod&quot;: 0,</span><br><span class="line">            &quot;CpuQuota&quot;: 0,</span><br><span class="line">            &quot;CpuRealtimePeriod&quot;: 0,</span><br><span class="line">            &quot;CpuRealtimeRuntime&quot;: 0,</span><br><span class="line">            &quot;CpusetCpus&quot;: &quot;&quot;,</span><br><span class="line">            &quot;CpusetMems&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Devices&quot;: [],</span><br><span class="line">            &quot;DeviceCgroupRules&quot;: null,</span><br><span class="line">            &quot;DeviceRequests&quot;: null,</span><br><span class="line">            &quot;KernelMemory&quot;: 0,</span><br><span class="line">            &quot;KernelMemoryTCP&quot;: 0,</span><br><span class="line">            &quot;MemoryReservation&quot;: 0,</span><br><span class="line">            &quot;MemorySwap&quot;: 0,</span><br><span class="line">            &quot;MemorySwappiness&quot;: null,</span><br><span class="line">            &quot;OomKillDisable&quot;: false,</span><br><span class="line">            &quot;PidsLimit&quot;: null,</span><br><span class="line">            &quot;Ulimits&quot;: null,</span><br><span class="line">            &quot;CpuCount&quot;: 0,</span><br><span class="line">            &quot;CpuPercent&quot;: 0,</span><br><span class="line">            &quot;IOMaximumIOps&quot;: 0,</span><br><span class="line">            &quot;IOMaximumBandwidth&quot;: 0,</span><br><span class="line">            &quot;MaskedPaths&quot;: [</span><br><span class="line">                &quot;/proc/asound&quot;,</span><br><span class="line">                &quot;/proc/acpi&quot;,</span><br><span class="line">                &quot;/proc/kcore&quot;,</span><br><span class="line">                &quot;/proc/keys&quot;,</span><br><span class="line">                &quot;/proc/latency_stats&quot;,</span><br><span class="line">                &quot;/proc/timer_list&quot;,</span><br><span class="line">                &quot;/proc/timer_stats&quot;,</span><br><span class="line">                &quot;/proc/sched_debug&quot;,</span><br><span class="line">                &quot;/proc/scsi&quot;,</span><br><span class="line">                &quot;/sys/firmware&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;ReadonlyPaths&quot;: [</span><br><span class="line">                &quot;/proc/bus&quot;,</span><br><span class="line">                &quot;/proc/fs&quot;,</span><br><span class="line">                &quot;/proc/irq&quot;,</span><br><span class="line">                &quot;/proc/sys&quot;,</span><br><span class="line">                &quot;/proc/sysrq-trigger&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;GraphDriver&quot;: &#123;</span><br><span class="line">            &quot;Data&quot;: &#123;</span><br><span class="line">                &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e-init/diff:/var/lib/docker/overlay2/81761618a33f3926b117dce5b1a9ae7094d898b3d32f20d50da147a2c0c1dfd0/diff&quot;,</span><br><span class="line">                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e/merged&quot;,</span><br><span class="line">                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e/diff&quot;,</span><br><span class="line">                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e/work&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;Name&quot;: &quot;overlay2&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Mounts&quot;: [],</span><br><span class="line">        &quot;Config&quot;: &#123;</span><br><span class="line">            &quot;Hostname&quot;: &quot;5a96132edf5d&quot;,</span><br><span class="line">            &quot;Domainname&quot;: &quot;&quot;,</span><br><span class="line">            &quot;User&quot;: &quot;&quot;,</span><br><span class="line">            &quot;AttachStdin&quot;: false,</span><br><span class="line">            &quot;AttachStdout&quot;: false,</span><br><span class="line">            &quot;AttachStderr&quot;: false,</span><br><span class="line">            &quot;Tty&quot;: false,</span><br><span class="line">            &quot;OpenStdin&quot;: false,</span><br><span class="line">            &quot;StdinOnce&quot;: false,</span><br><span class="line">            &quot;Env&quot;: [</span><br><span class="line">                &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;Cmd&quot;: [</span><br><span class="line">                &quot;/bin/bash&quot;,</span><br><span class="line">                &quot;-c&quot;,</span><br><span class="line">                &quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;Image&quot;: &quot;centos&quot;,</span><br><span class="line">            &quot;Volumes&quot;: null,</span><br><span class="line">            &quot;WorkingDir&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Entrypoint&quot;: null,</span><br><span class="line">            &quot;OnBuild&quot;: null,</span><br><span class="line">            &quot;Labels&quot;: &#123;</span><br><span class="line">                &quot;org.label-schema.build-date&quot;: &quot;20210915&quot;,</span><br><span class="line">                &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;,</span><br><span class="line">                &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;,</span><br><span class="line">                &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;,</span><br><span class="line">                &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;NetworkSettings&quot;: &#123;</span><br><span class="line">            &quot;Bridge&quot;: &quot;&quot;,</span><br><span class="line">            &quot;SandboxID&quot;: &quot;e8757375dc8ce5c604047bace5801859349b35be01c4d0983f6e18aefad6d7c8&quot;,</span><br><span class="line">            &quot;HairpinMode&quot;: false,</span><br><span class="line">            &quot;LinkLocalIPv6Address&quot;: &quot;&quot;,</span><br><span class="line">            &quot;LinkLocalIPv6PrefixLen&quot;: 0,</span><br><span class="line">            &quot;Ports&quot;: &#123;&#125;,</span><br><span class="line">            &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/e8757375dc8c&quot;,</span><br><span class="line">            &quot;SecondaryIPAddresses&quot;: null,</span><br><span class="line">            &quot;SecondaryIPv6Addresses&quot;: null,</span><br><span class="line">            &quot;EndpointID&quot;: &quot;35abfaa04e627a268c7c0a2279bbd919c831c66dcb7e87ca7afa95694b2c6cd0&quot;,</span><br><span class="line">            &quot;Gateway&quot;: &quot;172.17.0.1&quot;,</span><br><span class="line">            &quot;GlobalIPv6Address&quot;: &quot;&quot;,</span><br><span class="line">            &quot;GlobalIPv6PrefixLen&quot;: 0,</span><br><span class="line">            &quot;IPAddress&quot;: &quot;172.17.0.5&quot;,</span><br><span class="line">            &quot;IPPrefixLen&quot;: 16,</span><br><span class="line">            &quot;IPv6Gateway&quot;: &quot;&quot;,</span><br><span class="line">            &quot;MacAddress&quot;: &quot;02:42:ac:11:00:05&quot;,</span><br><span class="line">            &quot;Networks&quot;: &#123;</span><br><span class="line">                &quot;bridge&quot;: &#123;</span><br><span class="line">                    &quot;IPAMConfig&quot;: null,</span><br><span class="line">                    &quot;Links&quot;: null,</span><br><span class="line">                    &quot;Aliases&quot;: null,</span><br><span class="line">                    &quot;NetworkID&quot;: &quot;99b4b446329155dc0d91f3989f0cd78ac1c354467b34c72c1ac8a51085632eb8&quot;,</span><br><span class="line">                    &quot;EndpointID&quot;: &quot;35abfaa04e627a268c7c0a2279bbd919c831c66dcb7e87ca7afa95694b2c6cd0&quot;,</span><br><span class="line">                    &quot;Gateway&quot;: &quot;172.17.0.1&quot;,</span><br><span class="line">                    &quot;IPAddress&quot;: &quot;172.17.0.5&quot;,</span><br><span class="line">                    &quot;IPPrefixLen&quot;: 16,</span><br><span class="line">                    &quot;IPv6Gateway&quot;: &quot;&quot;,</span><br><span class="line">                    &quot;GlobalIPv6Address&quot;: &quot;&quot;,</span><br><span class="line">                    &quot;GlobalIPv6PrefixLen&quot;: 0,</span><br><span class="line">                    &quot;MacAddress&quot;: &quot;02:42:ac:11:00:05&quot;,</span><br><span class="line">                    &quot;DriverOpts&quot;: null</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>进入当前正在运行的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 我们通常容器都是使用后台方式运行的,需要进入容器修改一些配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 命令1 docker <span class="built_in">exec</span> -it 容器id bashShell</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE        COMMAND        CREATED        STATUS        PORTS        NAMES</span><br><span class="line">5a96132edf5d        centos        &quot;/bin/bash -c &#x27;while…&quot;        12 minutes ago        Up 12 minutes        priceless_elbakyan</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker exec -it 5a96132edf5d /bin/bash</span><br><span class="line">[root@5a96132edf5d /]# ls</span><br><span class="line">bin  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">[root@5a96132edf5d /]# ps -ef</span><br><span class="line">UID          PID    PPID  C STIME TTY          TIME CMD</span><br><span class="line">root           1       0  0 07:33 ?        00:00:00 /bin/bash -c while true;do echo wjm;sleep 1;done</span><br><span class="line">root         750       0  0 07:46 pts/0    00:00:00 /bin/bash</span><br><span class="line">root         771       1  0 07:46 ?        00:00:00 /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 1</span><br><span class="line">root         772     750  0 07:46 pts/0    00:00:00 ps -ef</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 命令2 docker attach 容器id</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker attach 5a96132edf5d </span><br><span class="line">正在执行当前代码...</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span>		<span class="comment"># 进入容器后开启一个新的终端,可以在里面操作(常用)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker attach		<span class="comment"># 进入容器正在执行的终端,不会启动新的进程!,while true的话就死循环了</span></span></span><br></pre></td></tr></table></figure>
<p><strong>从容器内拷贝文件到主机上</strong></p>
<p>容器内和容器外是隔离的,如何拷贝?</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404155319337.png" alt="image-20220404155319337"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker cp 容器id:容器内路径 目的的主机路径</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看帮助 看到是可以双向拷贝的</span></span><br><span class="line">xxx@data:~$ sudo docker cp --help</span><br><span class="line">Usage:	docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-</span><br><span class="line">	docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH</span><br><span class="line"></span><br><span class="line">Copy files/folders between a container and the local filesystem</span><br><span class="line"></span><br><span class="line">Use &#x27;-&#x27; as the source to read a tar archive from stdin</span><br><span class="line">and extract it to a directory destination in a container.</span><br><span class="line">Use &#x27;-&#x27; as the destination to stream a tar archive of a</span><br><span class="line">container source to stdout.</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -a, --archive       Archive mode (copy all uid/gid information)</span><br><span class="line">  -L, --follow-link   Always follow symbol link in SRC_PATH</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看当前主机目录下的文件</span></span><br><span class="line">xxx@data:~$ ls</span><br><span class="line">solrdata  total.csv</span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入docker容器内部</span></span><br><span class="line">xxx@data:~$ sudo docker attach 825f1e107b64</span><br><span class="line">[root@825f1e107b64 /]# ls</span><br><span class="line">bin  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">[root@825f1e107b64 /]# cd home/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在容器内新建一个文件</span></span><br><span class="line">[root@825f1e107b64 home]# touch test.java</span><br><span class="line">[root@825f1e107b64 home]# ls</span><br><span class="line">test.java</span><br><span class="line">[root@825f1e107b64 home]# exit</span><br><span class="line">exit</span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器内/home/test.java拷贝到主机上的默认目录</span></span><br><span class="line">xxx@data:~$ sudo docker cp 825f1e107b64:/home/test.java .	</span><br><span class="line">xxx@data:~$ ls</span><br><span class="line">solrdata  test.java  total.csv</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 拷贝是一个手动过程,未来我们使用 -v 卷的技术可以实现自动同步 /home /home</span></span><br></pre></td></tr></table></figure>
<h2 id="小节"><a href="#小节" class="headerlink" title="小节"></a>小节</h2><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404163030861.png" alt="image-20220404163030861"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Commands:</span><br><span class="line">  attach      Attach to a running container						# 当前 shell 下 attach 连接指定运行镜像</span><br><span class="line">  build       Build an image from a Dockerfile					# 通过 Dockerfile 定制镜像</span><br><span class="line">  commit      Create a new image from a container&#x27;s changes		# 提交当前容器为新镜像</span><br><span class="line">  cp          Copy files/folders between a container and the local filesystem	# 从容器中拷贝文件到宿主机中</span><br><span class="line">  create      Create a new container							# 创建一个新的容器,同 run ,但不启动容器</span><br><span class="line">  diff        Inspect changes  on a container&#x27;s filesystem		# 查看 docker 容器的变化</span><br><span class="line">  events      Get real time events from the server				# 从 docker 服务获取容器实时事件</span><br><span class="line">  exec        Run a command in a running container				# 再已存在的容器中运行命令</span><br><span class="line">  export      Export a container&#x27;s filesystem as a tar archive	# 导出容器的内容流作为一个 tar 归档文件[对应 import ]</span><br><span class="line">  history     Show the history of an image						# 展示一个镜像形成历史</span><br><span class="line">  images      List images										# 列出系统当前镜像</span><br><span class="line">  import      Import the contents from a tarball to create a filesystem image	# 从 tar 包中的内容创建一个新的文件系统映像[对应 import ]</span><br><span class="line">  info        Display system-wide information					# 展示系统相关信息</span><br><span class="line">  inspect     Return low-level information on Docker objects	# 查看容器详细信息</span><br><span class="line">  kill        Kill one or more running containers				# kill 指定容器</span><br><span class="line">  load        Load an image from a tar archive or STDIN			# 从 tar 包中加载一个镜像[对应 sava ]</span><br><span class="line">  login       Log in to a Docker registry						# 注册或者登录一个 docker 源服务器</span><br><span class="line">  logout      Log out from a Docker registry					# 退出登录</span><br><span class="line">  logs        Fetch the logs of a container						# 输出容器的日志</span><br><span class="line">  pause       Pause all processes within one or more containers	# 暂停容器</span><br><span class="line">  port        List port mappings or a specific mapping for the container	# 查看映射端口对应的容器内部源端口</span><br><span class="line">  ps          List containers									# 列出容器列表</span><br><span class="line">  pull        Pull an image or a repository from a registry		# 从 docker 镜像源服务器拉取指定镜像</span><br><span class="line">  push        Push an image or a repository to a registry		# 推送指定镜像至 docker 镜像源服务器</span><br><span class="line">  rename      Rename a container								# 重命名容器</span><br><span class="line">  restart     Restart one or more containers					# 重构其容器</span><br><span class="line">  rm          Remove one or more containers						# 删除一个或多个容器</span><br><span class="line">  rmi         Remove one or more images							# 删除一个或多个镜像[无容器使用该镜像时才能删除,否则需要删除相关容器才能继续,或者 -f 强制删除]</span><br><span class="line">  run         Run a command in a new container					# 创建一个新的容器并运行命令</span><br><span class="line">  save        Save one or more images to a tar archive (streamed to STDOUT by default)	# 保存镜像为 tar 包[对应 load ]</span><br><span class="line">  search      Search the Docker Hub for images					# 在 docker hub 中搜索镜像</span><br><span class="line">  start       Start one or more stopped containers				# 启动容器</span><br><span class="line">  stats       Display a live stream of container(s) resource usage statistics	# 展示容器的实时资源占用情况</span><br><span class="line">  stop        Stop one or more running containers				# 停止容器</span><br><span class="line">  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE	# 给源中镜像打标签</span><br><span class="line">  top         Display the running processes of a container		# 查看容器中运行的进程信息</span><br><span class="line">  unpause     Unpause all processes within one or more containers	# 取消暂停容器</span><br><span class="line">  update      Update configuration of one or more containers	# 更新容器的配置</span><br><span class="line">  version     Show the Docker version information				# 查看 docker 版本号</span><br><span class="line">  wait        Block until one or more containers stop, then print their exit codes	# 截取容器停止时的退出状态值</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>docker命令很多,上面都是最常用的.</p>
<h2 id="作业练习"><a href="#作业练习" class="headerlink" title="作业练习"></a>作业练习</h2><h3 id="Docker-安装-Nginx"><a href="#Docker-安装-Nginx" class="headerlink" title="Docker 安装 Nginx"></a>Docker 安装 Nginx</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1. 搜索镜像 search ,建议到 docker hub 上搜索查看详细文档</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 下载镜像 pull</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. 运行测试</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -d 后台运行</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --name 给容器命名</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -p 宿主机(linux服务器)端口,容器(docker)内部端口</span></span><br><span class="line">xxx@data:~$ sudo docker run -d --name nginx01 -p 3344:80 nginx</span><br><span class="line">659cd3e2b3825a85c7e4b02ad2fe52be267fa2fd5425391162b859fec5f3c9c4</span><br><span class="line">xxx@data:~$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE             COMMAND             CREATED             STATUS             PORTS             NAMES</span><br><span class="line">659cd3e2b382        nginx             &quot;/docker-entrypoint.…&quot;             33 seconds ago             Up 31 seconds             0.0.0.0:3344-&gt;80/tcp             nginx01</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试访问成功</span></span><br><span class="line">xxx@data:~$ curl localhost:3344</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">.......</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入容器</span></span><br><span class="line">xxx@data:~$ sudo docker exec -it nginx01 /bin/bash</span><br><span class="line">root@659cd3e2b382:/# whereis nginx</span><br><span class="line">nginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx</span><br><span class="line">root@659cd3e2b382:/# cd /etc/nginx</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>端口暴露的概念</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404172634182.png" alt="image-20220404172634182" style="zoom:50%;" /></p>
<p>思考问题: 我们每次改动nginx配置文件,都需要进入容器内部 ? 十分麻烦,要是可以在容器外部提供一个映射路径,达到在容器外部修改文件,容器内部就可以自动修改?   <strong>-v 数据卷</strong></p>
<h3 id="Docker-安装-tomcat"><a href="#Docker-安装-tomcat" class="headerlink" title="Docker 安装 tomcat"></a>Docker 安装 tomcat</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 官方的使用</span></span><br><span class="line">docker run -it --rm tomcat:9.0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 我们之前的使用都是 -d 后台,停止容器后,容器还是可以查到.--rm 表示用完就删.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 所以 docker run -it --rm tomcat:9.0 一般用来测试,用完即删</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载再启动</span></span><br><span class="line">docker pull tomcat:9.0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动运行</span></span><br><span class="line">docker run -d -p 3344:8080 --name tomcat01 tomcat</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试访问没有问题</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入容器</span></span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker exec -it tomcat01 /bin/bash</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# ls</span><br><span class="line">BUILDING.txt	 LICENSE  README.md	 RUNNING.txt  conf  logs	    temp     webapps.dist</span><br><span class="line">CONTRIBUTING.md  NOTICE   RELEASE-NOTES  bin	      lib   native-jni-lib  webapps  work</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# ls -al</span><br><span class="line">total 176</span><br><span class="line">drwxr-xr-x 1 root root  4096 Apr  1 19:34 .</span><br><span class="line">drwxr-xr-x 1 root root  4096 Mar 30 05:14 ..</span><br><span class="line">-rw-r--r-- 1 root root 19004 Mar 31 14:24 BUILDING.txt</span><br><span class="line">-rw-r--r-- 1 root root  6210 Mar 31 14:24 CONTRIBUTING.md</span><br><span class="line">-rw-r--r-- 1 root root 60269 Mar 31 14:24 LICENSE</span><br><span class="line">-rw-r--r-- 1 root root  2333 Mar 31 14:24 NOTICE</span><br><span class="line">-rw-r--r-- 1 root root  3378 Mar 31 14:24 README.md</span><br><span class="line">-rw-r--r-- 1 root root  6905 Mar 31 14:24 RELEASE-NOTES</span><br><span class="line">-rw-r--r-- 1 root root 16507 Mar 31 14:24 RUNNING.txt</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 bin</span><br><span class="line">drwxr-xr-x 1 root root  4096 Apr  4 09:43 conf</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 lib</span><br><span class="line">drwxrwxrwx 1 root root  4096 Apr  4 09:43 logs</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 native-jni-lib</span><br><span class="line">drwxrwxrwx 2 root root  4096 Apr  1 19:34 temp</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 webapps</span><br><span class="line">drwxr-xr-x 7 root root  4096 Mar 31 14:24 webapps.dist</span><br><span class="line">drwxrwxrwx 2 root root  4096 Mar 31 14:24 work</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# cd webapps</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat/webapps# ls</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat/webapps# </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 发现问题 1.linux命令少了  2. 404 not found --&gt;没有webapps    是镜像的原因,默认是最小的镜像,所有不必要的都剔除了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 保证最小可运行环境</span></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404174410059.png" alt="image-20220404174410059" style="zoom:50%;" /></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 更改配置再访问</span></span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# cp -r webapps.dist/* webapps</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# cd webapps</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat/webapps# ls</span><br><span class="line">ROOT  docs  examples  host-manager  manager</span><br></pre></td></tr></table></figure>
<p>404 变成了完整页面, 全程都是在docker里修改,同样可以部署</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404175010930.png" alt="image-20220404175010930" style="zoom:50%;" /></p>
<p>思考问题: 我们以后要部署项目,如果每次都要进入容器是不是十分麻烦?要是可以在容器外部提供一个映射路径,webapps,我们在外部放置项目,就自动同步到内部就好了!</p>
<p>现在 docker 容器: tomcat + 网站内容 ; 如果将来 docker 容器放 mysql + 数据库数据 , 再如果把容器删了 , 就相当于删库跑路了…. 非常不科学 . (期待 -v 数据卷 如何解决)</p>
<h3 id="Docker-部署-es-kibana-没有实际部署"><a href="#Docker-部署-es-kibana-没有实际部署" class="headerlink" title="Docker 部署 es + kibana(没有实际部署)"></a>Docker 部署 es + kibana(没有实际部署)</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> es 暴露的端口很多</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> es 十分耗内存</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> es 的数据一般需要放置到安全目录! 挂载</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --net somenetwork ? 网络配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载 启动</span></span><br><span class="line">docker run -d --name elasticsearch01 --net somenetwork -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch:7.6.2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动了 Linux就卡住了   因为非常占内存</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker stats 查看cpu的状态</span></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404181625831.png" alt="image-20220404181625831" style="zoom:67%;" /></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 赶紧关闭,增加内存限制,修改配置文件 -e 环境配置修改 限制最小最大占用内存</span></span><br><span class="line">docker run -d --name elasticsearch02 --net somenetwork -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot; elasticsearch:7.6.2</span><br><span class="line"><span class="meta">#</span><span class="bash"> docker stats 查看cpu的状态</span></span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404181645136.png" alt="image-20220404181645136" style="zoom: 67%;" /></p>
<p>使用 kibana 连接 es ? 思考网络如何连接.</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404182053887.png" alt="image-20220404182053887" style="zoom:50%;" /></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以上是看 b 站教学视频记的笔记。</p>
<p>教程地址：<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=9">【狂神说Java】Docker最新超详细版教程通俗易懂_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《GAG：Global Attributed Graph Neural Network for Streaming Session-based Recommendation》</title>
    <url>/2022/03/13/GAG/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220309191028621.png" alt="image-20220309191028621"></p>
<hr>
<p>原paper：<a href="https://doi.org/10.1145/3397271.3401109"><em>GAG: Global Attributed Graph Neural Network for Streaming Session-Based Recommendation</em></a></p>
<p>源码解读：（近期发布）</p>
<hr>
<p>中译：基于流会话推荐的全局属性图神经网络</p>
<p>总结：将SSRM的encoder部分换成了图神经网络模型，并且沿用了NARM、SRGNN等采用的注意力机制，将用户信息作为全局信息融入GNN模型中，解决了保存用户长期兴趣的问题；改进了reservior的采样策略：计算推荐结果和真实交互的Wasserstein距离作为信息量指标，从而计算采样概率，改进采样策略。</p>
<p>展望：如何引入跨会话信息到SSR问题中，十分值得研究。</p>
<hr>
<span id="more"></span>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>question作者想解决什么问题？ </p>
<p>1）SR任务中，用户信息常常被忽略，所以难以抓住用户的长期兴趣。SSR任务中，流数据常常是单个交互而不是会话数据。</p>
<p>2）如何设计适合SSR的通用的reservoir。</p>
</li>
<li><p>method作者通过什么理论/模型来解决这个问题？</p>
<p>针对1），作者提出 <strong>G</strong>lobal <strong>A</strong>ttributed <strong>G</strong>raph （GAG）neural network，全局属性的图神经网络。每当新数据到达时，GAG可以同时考虑全局属性和当前场景，以获得会话和用户的更全面的表示。</p>
<p>针对2）作者提出了 Wasserstein 存储库，帮助保存历史数据的<strong>代表性</strong>画像。</p>
</li>
<li><p>answer作者给出的答案是什么？</p>
<p>GAG + Wasserstein reservoir，取得了SOTA。</p>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>why作者为什么研究这个课题？    </p>
<p>会话推荐的发展现状：大部分会话推荐模型都专注与静态场景，“流会话”(Streaming session)的设定更贴近实际，但却很少研究。由于用户的喜好在随时间变化，所以对新数据做预测，仍然使用在原来数据上训练的静态模型是不合理的，为了更准确地捕捉用户兴趣，模型应该用最新的数据在线更新。</p>
<p>流推荐的发展现状：一些方法利用了存储技术解决流任务，但是它们的缺点是，交互数据都已相同概率存储到存储库中，这是一种离散的方式存储会话，有信息损失，捕捉不了连续的会话序列模式。还有一些在线学习的方法，每当新数据到来，模型就相应地更新，这会导致模型对新数据过拟合并且无法有效保留用户的长期兴趣。</p>
<p>只有SSRM提出了一个结合两者的解决方案，但有不足之处。</p>
</li>
<li><p>how当前研究到了哪一阶段？</p>
<p>SSRM。SSRM有两方面不足：1）计算信息量时，需要预先获得所有物品的隐含表示（MF方法得到的），别的方法不适用。2）模型将会话推荐的方法（GRU4Rec）和矩阵分解（MF）直接结合，这里原文是用MF计算的权重，对GRU4Rec的隐藏状态加权求和。很难学到用户和物品间更复杂的关联。</p>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 </li>
</ul>
<p>LastFM：<a href="http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz">http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz</a></p>
<p>Gowalla：<a href="https://snap.stanford.edu/data/loc-gowalla.html">https://snap.stanford.edu/data/loc-gowalla.html</a></p>
<ul>
<li>数据划分</li>
</ul>
<p>给数据集 $D$ 中的会话按时间排序，分成前60%作为训练集，和后40%作为候选集。为了模拟线上的流数据输入，将候选集再划分成5个等长切片作为测试机。第一个测试机和10%的训练集作为验证集。实验中，若要预测第 $i$ 个测试集的序列行为，那么 $i$ 之前的测试集切片都用作在线训练。</p>
<ul>
<li>重要指标 </li>
</ul>
<p>MRR@20、Recall@20</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>会话推荐Session-based Recommendation<ul>
<li>属于序列推荐</li>
</ul>
</li>
<li>流推荐Streaming Recommendation<ul>
<li>属于在线学习Online Learning，大多数在线学习更关注新数据，往往不能记忆历史交互</li>
<li>随机采样Random Sampling 方法是为了解决 “历史遗忘“问题而提出的，它通过引入一个储存库来保存用户的长期交互。</li>
</ul>
</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>Item embdding： $ x_i =Embed_v(v_i)$ ；User embdding： $ p_j =Embed_u(u_j)$  ；</p>
<p>User u 在时间步 t 的会话序列：$ S_{u,t}=[v_1,v_2,…,v_l]$ </p>
<p>任务目标，根据用户 $u$ 的历史会话 $\{S_{u,0},S_{u,1},…,S_{u,t}\}$ 预测下一个可能交互的物品 $ v_{t+1}$ </p>
<p>与会话推荐不同的是，这里假设所有会话 $S$ 都以很快的速度达到，所以受限于算力，必须选取高效的方式处理历史会话信息和当前会话信息。而会话推荐，没有流数据这一设定，可以同时处理用户所有序列，不必考虑效率。</p>
<h3 id="GAG模型框架"><a href="#GAG模型框架" class="headerlink" title="GAG模型框架"></a>GAG模型框架</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220309205703438.png" alt="image-20220309205703438"></p>
<p>GAG的主要工作由两部分构成，1）GAG model：将用户信息转化为全局属性并将其融入到会话图中；2）Wasserstein reservoir存储库策略用来学习流数据。</p>
<h3 id="Global-Attributed-Graph-GAG"><a href="#Global-Attributed-Graph-GAG" class="headerlink" title="Global Attributed Graph (GAG)"></a>Global Attributed Graph (GAG)</h3><h4 id="全局属性的会话图"><a href="#全局属性的会话图" class="headerlink" title="全局属性的会话图"></a>全局属性的会话图</h4><p>建图方式和SRGNN一样，建成有向图，不同的是加入了 <strong>全局属性（用户属性）</strong> $u$  变成三元组 $G_s = (u,V_s,E_s)$ ，图的边定义为：$E_s=(w_{s,(n-1)n},v_{n-1},v_n)$ ，也是三元组， $w$ 是权重，和SRGNN计算方式一样，基于该边出现的频率。</p>
<h4 id="全局属性的图神经网络"><a href="#全局属性的图神经网络" class="headerlink" title="全局属性的图神经网络"></a>全局属性的图神经网络</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220312132405693.png" alt="image-20220312132405693"></p>
<p>会话图作为输入进到GAG模型，模型的计算从边、节点到全局属性。</p>
<ol>
<li>逐边更新 per-edge update</li>
</ol>
<p>边特征在这里指的是边的权值，是固定值，不是dense vector，不会更新。所以边信息只用来更新节点特征和全局特征。因为是有向图，所以对于一条边来说，需要双向更新，一个节点既作为sender，也作为receiver。更新公式：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
e_{k,in}' &= \phi ^e_{in}(e_k,v_{r_k},v_{s_k},u)
\\&= w_k \cdot MLP(v_{s_k}||u),
\\
e_{k,out}' &= \phi ^e_{out}(e_k,v_{r_k},v_{s_k},u)
\\&= w_k \cdot MLP(v_{s_k}||u),
\end{aligned}
\end{equation}</script><p>其中两个MLP是不共享权重的，因为含义不同，一个是计算sender方向的特征，一个是计算receiver方向的特征。</p>
<ol>
<li>逐点更新 per-node update</li>
</ol>
<p>逐点更新是基于逐边更新的结果的。逐点更新的结果是包含所有入or出的邻居信息的标准化后的加和。以第一个公式为例， $s_j$ 是所有指向 $r_i$（也即 $i$ ） 的邻居。节点 $i$ 的<strong>节点</strong>的 in-coming feature 是所有指向 $i$ 的<strong>边的</strong>标准化后的 out-going feature 的加和。 标准化是将 $j$ 指向 $i$ 的这条边的 out-going feature 除以 $\sqrt{i的入度 \cdot j的出度}$ ，可以看出， $j$ 的出度越大（从 $j$ 发出的边越多），$i$ 的入度越大（指向 $i$ 的边越多），都会导致 $i$ 来自节点 $j$ 的 in-coming feature 值越小。这是符合直觉的。</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
v_{i,in}' =\sum_{j \in \{ v_{s_j} = v_{r_i}\}} \frac{e_{j,out'}}{\sqrt{N_{in}(i)N_{out}(j)}} 
\\
v_{i,out}' =\sum_{j \in \{ v_{r_j} = v_{s_i}\}} \frac{e_{j,in'}}{\sqrt{N_{out}(i)N_{in}(j)}} 

\end{aligned}
\end{equation}</script><ol>
<li>节点的最终表示</li>
</ol>
<p>最后节点 $i$ 表示融合了自己的 in-coming feature 和 out-going feature，节点最后的表示实际上包含了 1）自身的节点信息；2）邻居信息（通过边传播）；3）连接的边的权重；4）全局属性：</p>
<script type="math/tex; mode=display">
v_i'= MLP (v_{i,in}' || v_{i,out}')</script><ol>
<li>会话表示</li>
</ol>
<p>与NARM、STAMP、SRGNN工作类似，也用序列中的最后一个节点对其它节点做 self-attention 。有的工作用内积计算最后一个节点与其他节点的相似度作为权重，如NARM；也有用MLP获得权重的，如STAMP、SRGNN。这里用MLP。</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\\
u' &= \phi ^ u (V',u)
\\
&= Self-Atten(v_l',v_i',u) + u

\end{aligned}
\end{equation}</script><p>Self-Atten 分为以下两个部分：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}

\alpha _i &= MLP(v_l'||v_i'||u)
\\
u_{s,g} &= \sum ^n _{i=1} \alpha_i v_i'
\end{aligned}
\end{equation}</script><p>这样得到的 $u_{s,g}$ 可以看作short-term兴趣增强的会话表示，再融合全局属性 $u$ ，得到长短期兴趣结合的会话表示： $u’ = u_{s,g} + u$ 。这种residual connection残差连接的方式，还可以减轻直接学习全局属性 $u$ 的负担。</p>
<h3 id="推荐-预测"><a href="#推荐-预测" class="headerlink" title="推荐/预测"></a>推荐/预测</h3><p>Session embedding 和 item embedding 做内积，再经过softmax得到概率分布： $\hat y = Softmax(u’^T X)$ 。</p>
<h3 id="Wasserstein-Reservoir"><a href="#Wasserstein-Reservoir" class="headerlink" title="Wasserstein Reservoir"></a>Wasserstein Reservoir</h3><p>将离线模型拓展到流设定下，提出Wasserstein reservoir方法。提出该方法的目标是：用新来的数据更新模型，并且保持从历史交互中学到的知识。</p>
<p>传统的在线学习方法通常只用新数据更新模型，所以导致模型会忘记过去的知识。为了避免这一点，本文利用reservoir来保持对历史数据的长期记忆，reservoir技术在流数据库管理系统中非常常见。</p>
<p>如何选择reservoir中的数据？之前的方法是：使用随机采样方法。每个新数据都以 $\frac{|C|}{t}$ 的概率随机替换掉已经在 $C$ 中的数据。这种方法被证明是从当前数据集中随机采样，并且可以保持模型的long-term memory。</p>
<p>作者认为，使用以上的随机采样方法得到 $C$ ，把它当作训练数据来训练模型的方式不好，原因如下：随机采样难以更关心新数据（time descent probability），但是最近的数据又是非常重要的。所以应该用新来的数据和reservoir $C$ 中的老数据一起更新预训练的模型，而不光光是reservoir $C$ 中的老数据。</p>
<p>但是，即便用新老数据一起更新模型，由于随机采样策略没变，训练数据中的大部分数据都是long-term数据，模型早就学得很好了，所以用它来训练对模型更新帮助不大。</p>
<p>如果当前模型在最新会话上预测结果不好，可能意味着用户兴趣转移or当前模型无法捕捉一些转换模式。这样的数据称之为”有信息量的数据“，对模型更新意义更大。</p>
<p>在本文中，一个会话的信息量被定义为模型预测的分布 $\hat y$ 和真实交互 $y$ 的距离。下面是三种计算距离的算法：</p>
<ol>
<li>Wasserstein 距离（EMD 距离）：</li>
</ol>
<script type="math/tex; mode=display">
d_{W}\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=\inf _{\gamma \in \Pi\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)} \mathbb{E}_{(x, y) \sim \gamma}[\|x-y\|]</script><ol>
<li>Kullback-Leibler（KL）散度：</li>
</ol>
<script type="math/tex; mode=display">
d_{K L}\left(\mathbb{P}_{r} \| \mathbb{P}_{g}\right)=\sum_{i=1}^{n} P_{r}(x) \log \frac{P_{r}(x)}{P_{g}(x)}</script><ol>
<li>Total Variation（全变分）距离：</li>
</ol>
<script type="math/tex; mode=display">
d_{T V}\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=\sup _{A \in \Sigma}\left|\mathbb{P}_{r}(A)-\mathbb{P}_{g}(A)\right|</script><p>在推荐任务中，真实分布是one-hot向量，只有真实标签处为1。</p>
<p>KL散度，也称交叉熵、相对熵。不选择KL散度的原因是：在推荐任务中，公式简化为：$d_{K L}(\mathbf{y} | \hat{\mathbf{y}})=-\log P_{g}\left(v_{i}\right)$ ，实际上只衡量了真实标签处的差异，没有考虑整个分布之间的差异。而且KL散度本身就是非对称性函数：$D(p | q) \neq D(q | p)$ ，用它作为一个真正的距离度量可能不是很合适。</p>
<p>不选择全变分距离的原因是：在推荐任务中，公式简化为： $d_{T V}(\mathbf{y}, \hat{\mathbf{y}})=\max _{j \neq i}\left(1-P_{g}\left(v_{i}\right), P_{g}\left(v_{j}\right)\right)$  ，这个结果要么只衡量了真实标签以外的差异，要么只衡量了真实标签。</p>
<h4 id="在线训练算法描述"><a href="#在线训练算法描述" class="headerlink" title="在线训练算法描述"></a>在线训练算法描述</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220313210103872.png" alt="image-20220313210103872"></p>
<p>$S$ 是用来更新模型的，与 $C$ 无关。</p>
<p>因为流数据中会有新用户和新物品出现，为了防止模型忽略这些新的会话，它们会被直接加入 $S$ 当作训练数据。</p>
<p>$C \cup C^{n e w} - S$ 即其余的会话数据，分别计算它们的Wasserstein距离，再根据以下公式计算各自的采样概率：</p>
<script type="math/tex; mode=display">
p_{\text {sample }}\left(s_{i}\right)=\frac{d_{i}}{\sum_{s_{j} \in C \cup C^{n e w}-S} d_{j}},</script><p>采样完以后就得到训练数据 $S$ ，这个 $S$ 是对当前模型来说信息量最大的数据集，用它来更新模型最有效。</p>
<p>最后还要更新 reservoir $C$ ，用随机采样算法来更新，以保持模型的 long-term 记忆。</p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>$ Cross \ Entropy \ Loss=-\sum_{i=1}^{l} \mathrm{y}_{i} \log \left(\hat{\mathrm{y}}_{i}\right) $</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><h3 id="对比实验结果"><a href="#对比实验结果" class="headerlink" title="对比实验结果"></a>对比实验结果</h3><p>S-POP居然比GRU4Rec结果好，可能因为S-POP能抽取出会话间的信息。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314084551972.png" alt="image-20220314084551972"></p>
<h3 id="细化评价指标的top-k"><a href="#细化评价指标的top-k" class="headerlink" title="细化评价指标的top k"></a>细化评价指标的top k</h3><p>SSRM方法相比于另外三个基于图的方法，效果下降得幅度更大，说明图结构更适合做会话表示任务，也说明图结构有一定的泛化能力。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314085304068.png" alt="image-20220314085304068"></p>
<h3 id="全局属性的影响"><a href="#全局属性的影响" class="headerlink" title="全局属性的影响"></a>全局属性的影响</h3><p>三个消融实验的对比模型如下：</p>
<ul>
<li><p>FGNN：节点更新层和输出层都不加入用户信息。</p>
</li>
<li><p>GAG-FGNN：将节点更新函数换成FGNN的节点更新层，但是保留全局属性更新  $u’ = u_{s,g} + u$ 。</p>
</li>
<li><p>GAG-NoGA：节点更新层不变，但是去掉全局属性更新函数 $u’ = u_{s,g} + u$ 。</p>
</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314091613738.png" alt="image-20220314091613738"></p>
<p>结果如下。GAG-FGNN和GAG-NoGA都在模型中融入了全局信息，前者在用户信息更新部分，后者在GNN的节点更新部分。相比于没有全局信息的FGNN，两者都有提升，说明融入全局信息对推荐是有用的。GAG-NoGA比GAG-FGNN提升更大，说明在节点更新阶段融入全局信息更有效。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314092750543.png" alt="image-20220314092750543"></p>
<h3 id="Wasserstein-Reservoir的影响"><a href="#Wasserstein-Reservoir的影响" class="headerlink" title="Wasserstein Reservoir的影响"></a>Wasserstein Reservoir的影响</h3><p>消融实验的对比模型如下：</p>
<ul>
<li>GAG-Static：直接去掉在线训练部分</li>
<li>GAG-RanUni：从原reservoir和新数据的并集里，随机采样。这也是最普遍的设计。</li>
<li>GAG-FixNew：直接保留新数据，剩下的数据随机采样。</li>
<li>GAG-WassUni：对所有原reservoir和新数据计算Wasserstein距离，然后根据这个距离采样。（原模型是先全部保存所有带有新用户和新物品的会话，再根据Wass距离采样）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314100934020.png" alt="image-20220314100934020"></p>
<p>结果如下：</p>
<p>Static结果最差，因为：1）兴趣漂移；2）新用户、新物品出现。</p>
<p>纯随机采样的GAG-RanUni，在在线模型中结果最差，不如有策略地选择。</p>
<p>GAG-WassUni优于大部分策略，说明采用Wass距离的有效性。</p>
<p>GAG和GAG-WassUni相比，GAG结果更好，说明保留新用户和新物品的重要性。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314100603124.png" alt="image-20220314100603124"></p>
<h3 id="Reservoir-效率分析"><a href="#Reservoir-效率分析" class="headerlink" title="Reservoir 效率分析"></a>Reservoir 效率分析</h3><p>有两个参数reservoir的设计影响很大：reservoir大小（ $C$ ）和窗口大小（ $S$ ）。一方面，reservoir的大小表示reservoir的容量，这决定了推荐系统在线更新的存储要求。另一方面，窗口大小限制了多少数据实例将被抽样用于在线训练，这代表了推荐系统在线更新的工作负荷。</p>
<h4 id="Reservoir-size-的影响"><a href="#Reservoir-size-的影响" class="headerlink" title="Reservoir size 的影响"></a>Reservoir size 的影响</h4><p>Reservoir容量越大，新数据保存的概率就越低，模型就更注重历史数据。但是在流设定下，新数据更能代表用户最近的兴趣。SOTA模型SSRM在$\frac{|D|}{20}$ 时表现最好，而GAG是 $\frac{|D|}{100}$ ，所以GAG效率更高。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314103059323.png" alt="image-20220314103059323"></p>
<h4 id="Window-size-的影响"><a href="#Window-size-的影响" class="headerlink" title="Window size 的影响"></a>Window size 的影响</h4><p>很明显，窗口大小越大，模型表现越好。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314104151741.png" alt="image-20220314104151741"></p>
<h3 id="超参数的影响"><a href="#超参数的影响" class="headerlink" title="超参数的影响"></a>超参数的影响</h3><h4 id="Embedding-size影响"><a href="#Embedding-size影响" class="headerlink" title="Embedding size影响"></a>Embedding size影响</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314104403575.png" alt="image-20220314104403575"></p>
<h4 id="GNN-layer层数影响"><a href="#GNN-layer层数影响" class="headerlink" title="GNN layer层数影响"></a>GNN layer层数影响</h4><p>一般来说，由于梯度爆炸，GNN模型总是受到模型深度增加的影响。在我们的实验中，GAG模型的性能随着GNN 层数增加而下降，这与常见的观察是一致的。此外，会话的连通性比传统的图数据要小，这也限制了更深的GNN模型的能力。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314104558903.png" alt="image-20220314104558903"></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>流会话推荐</tag>
        <tag>GAG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Graph-Enhanced Multi-Task Learning of Multi-Level Transition Dynamics for Session-based Recommendation》</title>
    <url>/2021/10/20/MTD/</url>
    <content><![CDATA[<hr>
<p>原paper：<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16534">https://ojs.aaai.org/index.php/AAAI/article/view/16534</a></p>
<p>开源代码：<a href="https://github.com/sessionRec/MTD">https://github.com/sessionRec/MTD</a></p>
<hr>
<h3 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h3><p>大多数现有的基于会话的推荐技术并没有很好地设计来捕捉复杂的转换动态(complex transition dynamics)，这些动态表现为时间有序和多层次相互依赖的关系结构。</p>
<p>complex transition dynamics 的”complex”体现在：multi-level relation(intra- and inter-session item relation) . 会话内：short-term and long-term item transition，会话间：long-range cross-session dependencies。复杂依赖的例子见Figure 1.</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/MTD-1.png" alt="MTD-1"></p>
<span id="more"></span>
<h3 id="主要贡献："><a href="#主要贡献：" class="headerlink" title="主要贡献："></a>主要贡献：</h3><p>1.开发了一种新的会话推荐框架，可以捕获会话内和会话间的物品转换模式（多层次转换动态）</p>
<p>2.开发了一种位置感知的注意力机，用于学习会话内的序列行为和session-specific knowledge。此外，在图神经网络范例的基础上，建立了全局上下文增强的会话间关系编码器，赋予MTD来捕获会话间项目依赖关系。</p>
<p>3.在三个数据集上取得了SOTA，Yoochoose、Diginetica、Retailrocket。</p>
<h3 id="网络图："><a href="#网络图：" class="headerlink" title="网络图："></a>网络图：</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/MTD-2.png" alt="MTD-2"></p>
<h3 id="方法论："><a href="#方法论：" class="headerlink" title="方法论："></a>方法论：</h3><h4 id="1-学习会话内的物品关系"><a href="#1-学习会话内的物品关系" class="headerlink" title="1.学习会话内的物品关系"></a>1.学习会话内的物品关系</h4><p>1）Self-attention层：</p>
<p>QKV self-attention + feed forward network</p>
<p>2）位置感知的物品级的注意力聚合模块：</p>
<script type="math/tex; mode=display">
\alpha _ i = \delta(\mathbf{g}^T \cdot \sigma(\mathbf{W} _ 3\cdot\mathbf{x} _ {s,I} + \mathbf{W} _ 4\cdot\mathbf{x} _ {s,i}))</script><script type="math/tex; mode=display">
\mathbf{x}^* _ s = \sum_{i=1}^I\alpha _ i\cdot\mathbf{x _ \mathit{s,i}}</script><script type="math/tex; mode=display">
\mathbf{q} _ s = \mathbf{W} _ c[\mathbf{x} _ {s,I},\mathbf{x}^* _ s,\mathbf{p} _ s]</script><p>$\mathbf{x}_{s,I}$表示最后一次点击，$\mathbf{x^*_s}$表示聚合后的会话表示，$\mathbf{p}_s$表示加入物品相对位置信息的会话表示，$\mathbf{q}_s $是最后的会话表示。</p>
<p>3）loss = $\mathit{L_in}$</p>
<h4 id="2-对全局转换动态建模"><a href="#2-对全局转换动态建模" class="headerlink" title="2.对全局转换动态建模"></a>2.对全局转换动态建模</h4><p>1）用图神经网络结构和GCN对inter-session的依赖建模</p>
<p>2）用<strong>互信息学习</strong>来增强跨会话的建模物品间关系的encoder</p>
<p>3）loss = $\mathit{L_cr}$</p>
<h4 id="3-Model-Inference"><a href="#3-Model-Inference" class="headerlink" title="3.Model Inference"></a>3.Model Inference</h4><p>定义loss：</p>
<script type="math/tex; mode=display">
\mathit{L}=\mathit{L} _ {cr} + \lambda_1\mathit{L} _ {in}+\lambda_2||\Theta||^2 _ 2</script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch Geometric 学习笔记</title>
    <url>/2021/11/14/Pytorch%20Geometric/</url>
    <content><![CDATA[<hr>
<p>官网永远是最好的学习资料：<a href="https://pytorch-geometric.readthedocs.io/en/latest/">https://pytorch-geometric.readthedocs.io/en/latest/</a></p>
<p>跟着配套colaboratory的教程走，大概一天能学完五个教程，学完也算基本入门pytroch-geometric了。</p>
<hr>
<p><a href="https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=gUFSrDPxuQ23">1. Introduction.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li>This concludes the first introduction into the world of GNNs and PyTorch Geometric. In the follow-up sessions, you will learn how to achieve state-of-the-art classification results on a number of real-world graph datasets.</li>
<li>概要：介绍图的基本结构，GCN怎么用。</li>
</ul>
<p><a href="https://colab.research.google.com/drive/14OvFnAXggxB8vM4e8vSURUp1TaKnovzX">2. Node Classification.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li>In this chapter, you have seen how to apply GNNs to real-world problems, and, in particular, how they can effectively be used for boosting a model’s performance. In the next section, we will look into how GNNs can be used for the task of graph classification.</li>
<li>概要：用GNN实现某些真实的节点分类任务，与MLP效果更好。</li>
</ul>
<span id="more"></span>
<p><a href="https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=CN3sRVuaQ88l">3. Graph Classification.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li><p>In this chapter, you have learned how to apply GNNs to the task of graph classification. You have learned how graphs can be batched together for better GPU utilization, and how to apply readout layers for obtaining graph embeddings rather than node embeddings.</p>
<p>In the next session, you will learn how you can utilize PyTorch Geometric to let Graph Neural Networks scale to single large graphs.</p>
</li>
<li><p>概要：学习了应用GNN实现图分类。学习了GNN上的mini-batch是如何构造以更好利用GPU。学习了如何用readout层获取图的表示。</p>
<ul>
<li>和图像一样用padding和rescaling让图大小相同太浪费空间，所以用对角矩阵相连的方法处理。在torch里是用稀疏矩阵存储的，所以开销不大。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211118204147577.png" alt="image-20211118204147577"></p>
<ul>
<li>Dataloader和torch里差不多 <code>train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)</code>。</li>
<li><code>DataBatch(edge_index=[2, 2636], x=[1188, 7], edge_attr=[2636, 4], y=[64], batch=[1188], ptr=[65])</code> 里的batch记录的是每个节点在哪个图里，batch = [0,…,0,1,…,1,2,…,2]表示一个batch里有三张图</li>
<li>nn.GraphConv() 有residual connection。</li>
<li>图的表示可以写成所有节点的均值<code>x = global_mean_pool(x, batch)</code> </li>
</ul>
</li>
</ul>
<p><a href="https://colab.research.google.com/drive/1XAjcjRHrSR_ypCk_feIWFbcBKyT4Lirs#scrollTo=SDOmdUe0C3U1">4. Scaling GNNs.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li><p>In this chapter, you have been presented a method to scale GNNs to large graphs, which otherwise would not fit into GPU memory.</p>
<p>This also concludes the hands-on tutorial on <strong>deep graph learning with PyTorch Geometric</strong>. If you want to learn more about GNNs or PyTorch Geometric, feel free to check out <strong><a href="https://pytorch-geometric.readthedocs.io/en/latest/?badge=latest">PyG’s documentation</a></strong>, <strong><a href="https://github.com/rusty1s/pytorch_geometric">its list of implemented methods</a></strong> as well as <strong><a href="https://github.com/rusty1s/pytorch_geometric/tree/master/examples">its provided examples</a></strong>, which cover additional topics such as <strong>link prediction</strong>, <strong>graph attention</strong>, <strong>mesh or point cloud convolutions</strong> and <strong>other methods for scaling up GNNs</strong>.</p>
<p><em>Happy hacking!</em></p>
</li>
<li><p>概要：介绍了了降低显存的方法，cluster-gnn，使得训练超大图成为可能。</p>
</li>
<li><p>不再在整个图上划分mini-batch，先分成sub-graph再分mini-batch，解决了邻居爆炸（。。邻居数量）问题</p>
</li>
<li><p>分太开也不好，所以cluster以后随机对sub-graph再连接</p>
<ul>
<li><code>ClusterData</code> converts a <code>Data</code> object into a dataset of subgraphs containing <code>num_parts</code> partitions.</li>
<li>Given a user-defined <code>batch_size</code>, <code>ClusterLoader</code> implements the stochastic partitioning scheme in order to create mini-batches.</li>
</ul>
</li>
<li><p>这种采样方法，只用改划分数据的代码，训练过程不变。</p>
</li>
</ul>
<p><a href="https://colab.research.google.com/drive/1D45E5bUK3gQ40YpZo65ozs7hg5l-eo_U?usp=sharing#scrollTo=iWRxB3JYFXNF">5. Point Cloud Classification.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li><p>概要：介绍了点云分类任务的三大步骤。又在PointNet++和PPFNet的实践中，介绍了如何自定义MessagePassing以及采样策略。</p>
</li>
<li><p>PointNet++</p>
<ul>
<li><p>Grouping阶段，用knn graph或者半径图</p>
<ul>
<li><p>```python<br>from torch_cluster import knn_graph<br>根据点的坐标计算最近的k个点，连起来</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 邻居聚合阶段。聚合邻居信息</span><br><span class="line"></span><br><span class="line">  - 从</span><br><span class="line"></span><br><span class="line">    ```python</span><br><span class="line">    class PointNetLayer(torch_geometric.nn.MessagePassing)：</span><br><span class="line">    	def __init__(self, in_channels, out_channels):</span><br><span class="line">            pass</span><br><span class="line">        def forward(self, h, pos, edge_index):</span><br><span class="line">            pass</span><br><span class="line">        def message(self, h_j, pos_j, pos_i):</span><br><span class="line">            pass</span><br></pre></td></tr></table></figure>
<p>继承，并定义出一个与<code>GraphConv()</code> 、<code>GCNConv()</code> 同一级别的类，例如一种新的卷积层。</p>
</li>
<li><p>MessagePassing接口通过自动处理消息传播，来帮助我们创建<strong>消息传递图神经网络</strong>。只需要定义 message 功能即可。</p>
</li>
<li><p><code>def message()</code>  定义如何构建一个可学习的message给每条边（每个边对应一个邻居，所以也可以看成定义message给每个邻居），以及传播的规则</p>
</li>
<li><p><code>def forward()</code>  调用propagate()，开始传播</p>
</li>
<li><p>PPFNet，解决旋转不变性</p>
</li>
</ul>
</li>
<li><p>downsampling（下采样）阶段</p>
<ul>
<li><p><strong>Farthest Point Sampling</strong> (FPS) 最远点采样。使得每次采点都和已经采样的点距离最远。这种方式证明比随机采样更能覆盖整个点集。</p>
</li>
<li><p>不同batch中fps是独立的，所以要传入batch向量</p>
<ul>
<li><pre><code class="lang-python">index = fps(pos, batch, ratio=0.5)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[6. GNN Explanation.ipynb - Colaboratory (google.com)](https://colab.research.google.com/drive/1fLJbFPz0yMCQg81DdCP5I8jXw9LoggKO?usp=sharing#scrollTo=F1op-CbyLuN4)</span><br><span class="line"></span><br><span class="line">- 占坑</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line"></span><br><span class="line">建个新环境</span><br><span class="line"></span><br></pre></td></tr></table></figure>
conda create -n pyg python==3.8.0
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">安装torch</span><br><span class="line"></span><br></pre></td></tr></table></figure>
pip install torch==1.10.0
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">安装依赖包</span><br><span class="line"></span><br><span class="line">## 方法1：</span><br><span class="line"></span><br><span class="line">![image-20211115163907936](C:\Users\pc\OneDrive\Typora文档\images\image-20211115163907936.png)</span><br><span class="line"></span><br><span class="line">官网安装比较省事，但是可以看到只有最新的几个版本，如果你的pytorch版本比较旧（旧也是为了稳定...）可以尝试方法2。</span><br><span class="line"></span><br><span class="line">## 方法2：</span><br><span class="line"></span><br><span class="line">https://data.pyg.org/whl/</span><br><span class="line"></span><br><span class="line">根据pytorch版本和cuda版本，在这个网站选择对应版本进入，例如我是torch-1.10和cuda-10.2，所以进入https://data.pyg.org/whl/torch-1.10.0+cu102.html</span><br><span class="line"></span><br><span class="line">然后根据系统类型和python版本，下好安装包，如下</span><br><span class="line"></span><br><span class="line">torch_**scatter**-2.0.9-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">torch_**sparse**-0.6.12-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">torch_**cluster**-1.5.9-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">torch_spline_conv-1.2.1-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">最后用pip离线离线安装</span><br><span class="line"></span><br></pre></td></tr></table></figure>
pip install xxx.whl
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意要切换到安装目录，且按顺序安装 scatter—&gt;sparse—&gt;cluster—&gt;spline</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>RecHub推荐项目学习1：Torch-RecHub框架</title>
    <url>/2022/06/14/RecHub%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A01/</url>
    <content><![CDATA[<h1 id="Task01"><a href="#Task01" class="headerlink" title="Task01"></a>Task01</h1><p>&emsp;&emsp;Task01：熟悉Torch-RecHub框架设计与使用方法</p>
<p>&emsp;&emsp;参考资料：0613晚直播讲解，直播ppt，<a href="https://github.com/datawhalechina/torch-rechub">RecHub源码</a></p>
<h2 id="Torch-RecHub-简介"><a href="#Torch-RecHub-简介" class="headerlink" title="Torch-RecHub 简介"></a>Torch-RecHub 简介</h2><p>&emsp;&emsp;一句话概括：一个轻量级的pytorch推荐模型框架（详见ppt）。</p>
<p>&emsp;&emsp;比较认可的一点是：“模型训练与模型定义解耦，无basemodel概念，易拓展”，因为之前接触过 RUC 的开源框架 Recbole ，emm只能说对新手不是很友好（但不否认是一个伟大的开源项目），不友好主要就体现在各种 basemodel 的封装继承导致比较难修改。</p>
<span id="more"></span>
<h2 id="Torch-RecHub-框架"><a href="#Torch-RecHub-框架" class="headerlink" title="Torch-RecHub 框架"></a>Torch-RecHub 框架</h2><h3 id="框架图"><a href="#框架图" class="headerlink" title="框架图"></a>框架图</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220614130949977.png" alt="image-20220614130949977"></p>
<p>&emsp;&emsp;从框架图可以看出，<strong>数据</strong>（特征工程、预处理）、<strong>模型定义</strong>、<strong>模型训练</strong>三部分是完全解耦的，除此之外还有 utils ，包含了损失函数、激活函数、优化器、采样器、评估等其它功能，可能因为东西比较多所以统一叫 utils 吧。</p>
<h3 id="查看项目文件树"><a href="#查看项目文件树" class="headerlink" title="查看项目文件树"></a>查看项目文件树</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220614132446809.png" alt="image-20220614132446809"></p>
<p>&emsp;&emsp;框架图体现设计思路，实际项目文件结构没有严格按照框架图来设计，还是有一些差别的。</p>
<ol>
<li>Data （数据）模块没有独立的 module ，预处理似乎是写到了 examples 里（毕竟每个数据集的处理方式都不一样，难以统一），三种 Feature 的处理写在了 <code>basic/features.py</code> 里。</li>
<li>框架图里 Utils 的功能基本都写在了 <code>basic</code> 里</li>
</ol>
<h2 id="Toy-example"><a href="#Toy-example" class="headerlink" title="Toy example"></a>Toy example</h2><p>&emsp;&emsp;以 dataset=ml-1m, model=GRU4Rec 为例介绍整个 pipeline。</p>
<p>&emsp;&emsp;文件位于 <code>examples/matching/run_ml_gru4rec.py</code> </p>
<p>&emsp;&emsp;为了和框架图对应上，本人将 pipeline 分为三个环节：定义阶段、训练阶段、推理阶段。定义阶段里定义了 Torch-RecHub 架构里最核心的三个模块， Data 、Model 和 Trainer；训练阶段做训练模型；推理阶段做模型评估。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220614225829571.png" alt="image-20220614225829571"  /></p>
<p>&emsp;&emsp;下面是主体代码，将逐步介绍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#========== 1.定义阶段 ==========#</span></span><br><span class="line"><span class="comment"># 数据预处理和特征工程</span></span><br><span class="line">user_features, history_features, item_features, neg_item_feature, x_train, y_train, all_item, test_user = get_movielens_data(dataset_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 DataGenerator (Dataset + Dataloader)</span></span><br><span class="line">dg = MatchDataGenerator(x=x_train, y=y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 model</span></span><br><span class="line">model = GRU4Rec(user_features, history_features, item_features, neg_item_feature, user_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">128</span>, <span class="number">64</span>, <span class="number">16</span>]&#125;, temperature=<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 trainer</span></span><br><span class="line"><span class="comment">#mode=1 means pair-wise learning</span></span><br><span class="line">trainer = MatchTrainer(model,</span><br><span class="line">                        mode=<span class="number">2</span>,</span><br><span class="line">                        optimizer_params=&#123;</span><br><span class="line">                            <span class="string">&quot;lr&quot;</span>: learning_rate,</span><br><span class="line">                            <span class="string">&quot;weight_decay&quot;</span>: weight_decay</span><br><span class="line">                        &#125;,</span><br><span class="line">                        n_epoch=epoch,</span><br><span class="line">                        device=device,</span><br><span class="line">                        model_path=save_dir,</span><br><span class="line">                        gpus=[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">train_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=batch_size, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#========== 2.训练阶段 ==========#</span></span><br><span class="line">trainer.fit(train_dl)</span><br><span class="line"></span><br><span class="line"><span class="comment">#========== 3.推理阶段 ==========#</span></span><br><span class="line"><span class="comment"># model inference</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;inference embedding&quot;</span>)</span><br><span class="line">user_embedding = trainer.inference_embedding(model=model, mode=<span class="string">&quot;user&quot;</span>, data_loader=test_dl, model_path=save_dir)</span><br><span class="line">item_embedding = trainer.inference_embedding(model=model, mode=<span class="string">&quot;item&quot;</span>, data_loader=item_dl, model_path=save_dir)</span><br><span class="line"><span class="built_in">print</span>(user_embedding.shape, item_embedding.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存 embedding</span></span><br><span class="line"><span class="comment">#torch.save(user_embedding.data.cpu(), save_dir + &quot;user_embedding.pth&quot;)</span></span><br><span class="line"><span class="comment">#torch.save(item_embedding.data.cpu(), save_dir + &quot;item_embedding.pth&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">match_evaluation(user_embedding, item_embedding, test_user, all_item, topk=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-定义阶段"><a href="#1-定义阶段" class="headerlink" title="1.定义阶段"></a>1.定义阶段</h3><h4 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h4><h5 id="预处理和特征工程"><a href="#预处理和特征工程" class="headerlink" title="预处理和特征工程"></a>预处理和特征工程</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#========== 1.定义阶段 ==========#</span></span><br><span class="line"><span class="comment"># 数据预处理和特征工程</span></span><br><span class="line">user_features, history_features, item_features, neg_item_feature, x_train, y_train, all_item, test_user = get_movielens_data(dataset_path)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这个示例使用的数据集是 Movielens，调用<code>get_movielens_data()</code>函数进行预处理，代码里比较通用的部分在于提取三种特征：Dense Feature 、 Sparse Feature 和 Sequence Feature。</p>
<blockquote>
<p><strong>Dense Feature</strong>：数值型特征，例如年龄、薪资、日点击量等。</p>
</blockquote>
<p>&emsp;&emsp;这里好像已经把所有 dense 特征都处理成 sparse 了。</p>
<blockquote>
<p><strong>Sparse Feature</strong>：类别型特征，例如城市、学历、性别等。主要使用了 sklearn 的 LabelEncoder。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, LabelEncoder</span><br><span class="line"></span><br><span class="line">feature_max_idx = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> sparse_features:</span><br><span class="line">    lbe = LabelEncoder()</span><br><span class="line">    data[feature] = lbe.fit_transform(data[feature]) + <span class="number">1</span>	<span class="comment"># lbe默认从0开始编号，这里+1表示从1开始编号</span></span><br><span class="line">    feature_max_idx[feature] = data[feature].<span class="built_in">max</span>() + <span class="number">1</span>		</span><br><span class="line">    <span class="keyword">if</span> feature == user_col:</span><br><span class="line">        user_map = &#123;encode_id + <span class="number">1</span>: raw_id <span class="keyword">for</span> encode_id, raw_id <span class="keyword">in</span> <span class="built_in">enumerate</span>(lbe.classes_)&#125;  <span class="comment">#encode user id: raw user id</span></span><br><span class="line">    <span class="keyword">if</span> feature == item_col:</span><br><span class="line">        item_map = &#123;encode_id + <span class="number">1</span>: raw_id <span class="keyword">for</span> encode_id, raw_id <span class="keyword">in</span> <span class="built_in">enumerate</span>(lbe.classes_)&#125;  <span class="comment">#encode item id: raw item id</span></span><br><span class="line">np.save(<span class="string">&quot;./data/ml-1m/saved/raw_id_maps.npy&quot;</span>, (user_map, item_map))	<span class="comment"># 保存 user_id 和 item_id 的索引</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里需要留意两点，第一，所有类别特征都从 1 开始编号。第二，user_id 和 item_id 也作为 Sparse Feature 来处理，并且保存了 user_id 和 item_id 的索引。</p>
<blockquote>
<p><strong>Sequence Feature</strong>：序列特征，分为有序（时序）兴趣序列：例如最近一周点击过的 item list 和 无序标签特征：例如电影类型（动作|悬疑|犯罪）。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> load_cache:  <span class="comment">#if you have run this script before and saved the preprocessed data</span></span><br><span class="line">    x_train, y_train, x_test = np.load(<span class="string">&quot;./data/ml-1m/saved/data_cache.npy&quot;</span>, allow_pickle=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment">#Note: mode=2 means list-wise negative sample generate, saved in last col &quot;neg_items&quot;</span></span><br><span class="line">    df_train, df_test = generate_seq_feature_match(data,</span><br><span class="line">                                                    user_col,</span><br><span class="line">                                                    item_col,</span><br><span class="line">                                                    time_col=<span class="string">&quot;timestamp&quot;</span>,</span><br><span class="line">                                                    item_attribute_cols=[],</span><br><span class="line">                                                    sample_method=<span class="number">1</span>,</span><br><span class="line">                                                    mode=<span class="number">2</span>,</span><br><span class="line">                                                    neg_ratio=<span class="number">3</span>,</span><br><span class="line">                                                    min_item=<span class="number">0</span>)</span><br><span class="line">    x_train = gen_model_input(df_train, user_profile, user_col, item_profile, item_col, seq_max_len=<span class="number">50</span>, padding=<span class="string">&#x27;post&#x27;</span>, truncating=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line">    y_train = np.array([<span class="number">0</span>] * df_train.shape[<span class="number">0</span>])  <span class="comment">#label=0 means the first pred value is positiva sample</span></span><br><span class="line">    x_test = gen_model_input(df_test, user_profile, user_col, item_profile, item_col, seq_max_len=<span class="number">50</span>, padding=<span class="string">&#x27;post&#x27;</span>, truncating=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line">    np.save(<span class="string">&quot;./data/ml-1m/saved/data_cache.npy&quot;</span>, (x_train, y_train, x_test))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;调用 <code>torch_rechub/utils/match.py</code> 的 <code>generate_seq_feature_match</code> 函数构建序列特征。该函数主要实现以下功能：</p>
<p><strong>提取点击序列特征</strong></p>
<p>&emsp;&emsp;具体地，对每个 user 在规定时间段内的点击序列用留一法进行切分，即最后一次点击作为测试集 label ，前 n-1 次作为测试集序列；用 <em>数据增强</em> 的方式生成训练集。举个例子比较容易说明，例如用户依次点击了 [A, C, B, D, E]，那么很自然地得到一条训练集：[A, C, B, D] —&gt; E ，用 [A, C, B, D] 作为输入，预测 label = E。数据增强的意思是，除了将最后一个标签作为 label 外，每个标签（除第一次点击以外）也都可以作为 label ，label 之前的序列都当作输入。这个例子里就可以额外产生：[A, C, B]—&gt; D， [A, C]—&gt; B ，[A]—&gt; C。所以由 [A, C, B, D, E] 可以生成E、D、B、C 为 label 的四条训练数据。只不过为了训练和评估模型，把 [A, C, B, D] —&gt; E 当作测试集了。</p>
<p><strong>提取无序标签特征</strong></p>
<p>&emsp;&emsp;其实也是有序的，把标签特征也当作序列里的 item 。 line 118：<code>sample.append(hist[attr_col].tolist()[:i])</code> </p>
<p><strong>负采样</strong></p>
<p>&emsp;&emsp;提供了三种负采样方式，point-wise、pair-wise 和 list-wise。</p>
<p>&emsp;&emsp;调用 <code>gen_model_input</code> ，对序列进行 truncating 和 padding，生成训练集和测试集，最后输出的是 pandas.DataFrame 格式。</p>
<p>&emsp;&emsp;最后再对上面三种特征进行封装，对 Sparse Feature 主要定义了每个特征的词表大小、 embedding 维度、embedding 初始化方法，对 Sequence Feature 还定义了 pooling 的方式，目前支持 [“mean”, “sum”, “concat”] 三种方式。</p>
<h5 id="定义-DataGenerator"><a href="#定义-DataGenerator" class="headerlink" title="定义 DataGenerator"></a>定义 DataGenerator</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义 DataGenerator (Dataset + Dataloader)</span></span><br><span class="line"><span class="comment"># 1.</span></span><br><span class="line">dg = MatchDataGenerator(x=x_train, y=y_train)</span><br><span class="line"><span class="comment"># 2.</span></span><br><span class="line">train_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=batch_size, num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;DataGenerator 主要继承了 torch 的 Dataset 类和 Dataloader 类，最终会返回三个 dataloader，分别是训练集 dataloader 、测试集 dataloader 和 item_dataloader，前两个比较好理解，最后一个 item_dataloader</p>
<h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><h5 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义 model</span></span><br><span class="line">model = GRU4Rec(user_features, history_features, item_features, neg_item_feature, user_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">128</span>, <span class="number">64</span>, <span class="number">16</span>]&#125;, temperature=<span class="number">0.02</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;所有模型都在 <code>torch_rechub/models</code> 下，都可以直接 call （所有计算都在 model.forward() 里）</p>
<h4 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h4><h5 id="定义训练器"><a href="#定义训练器" class="headerlink" title="定义训练器"></a>定义训练器</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义 trainer</span></span><br><span class="line">trainer = MatchTrainer(model,</span><br><span class="line">                        mode=<span class="number">2</span>,</span><br><span class="line">                        optimizer_params=&#123;</span><br><span class="line">                            <span class="string">&quot;lr&quot;</span>: learning_rate,</span><br><span class="line">                            <span class="string">&quot;weight_decay&quot;</span>: weight_decay</span><br><span class="line">                        &#125;,</span><br><span class="line">                        n_epoch=epoch,</span><br><span class="line">                        device=device,</span><br><span class="line">                        model_path=save_dir,</span><br><span class="line">                        gpus=[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;Trainer 主要定义了模型训练的有关参数，比如：训练模式：{0:point-wise, 1:pair-wise, 2:list-wise}，不同训练模式决定使用不同的损失函数；学习率；权重衰减系数；训练轮次；训练设备；gpu编号等。</p>
<p>&emsp;&emsp;Trainer 的方法有 fit（用于训练）、evaluate（用于评估）、predict（用于预测）、inference_embedding（用于推断），但是似乎这个代码只用到了 fit 和 inference_embedding，evaluate 和 predict 的具体用法还不清楚，predict 和 inference_embedding 的区别也不清楚。</p>
<h3 id="2-训练阶段"><a href="#2-训练阶段" class="headerlink" title="2. 训练阶段"></a>2. 训练阶段</h3><h4 id="Train-1"><a href="#Train-1" class="headerlink" title="Train"></a>Train</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#========== 2.训练阶段 ==========#</span></span><br><span class="line">trainer.fit(train_dl)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;只需要调用 trainer.fit()</p>
<h3 id="3-推理阶段"><a href="#3-推理阶段" class="headerlink" title="3. 推理阶段"></a>3. 推理阶段</h3><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#========== 3.推理阶段 ==========#</span></span><br><span class="line"><span class="comment"># model inference</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;inference embedding&quot;</span>)</span><br><span class="line">user_embedding = trainer.inference_embedding(model=model, mode=<span class="string">&quot;user&quot;</span>, data_loader=test_dl, model_path=save_dir)</span><br><span class="line">item_embedding = trainer.inference_embedding(model=model, mode=<span class="string">&quot;item&quot;</span>, data_loader=item_dl, model_path=save_dir)</span><br><span class="line"><span class="built_in">print</span>(user_embedding.shape, item_embedding.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存 embedding</span></span><br><span class="line"><span class="comment">#torch.save(user_embedding.data.cpu(), save_dir + &quot;user_embedding.pth&quot;)</span></span><br><span class="line"><span class="comment">#torch.save(item_embedding.data.cpu(), save_dir + &quot;item_embedding.pth&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">match_evaluation(user_embedding, item_embedding, test_user, all_item, topk=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;user_embedding 和 item_embedding 分别调用不同模式（mode=[“user”, “item”]）的 model 推理得到。这似乎是双塔特有的方式：user 和 item 联合训练，但是推理时解耦，分别推理。（第一次接触双塔的代码，不确定说的对不对…）</p>
<p>&emsp;&emsp;<code>match_evaluation()</code> 用 annoy 进行向量检索召回 topk 个 item ，接着调用  <code>torch_rechub/basic/metric.py</code> 中的 topk_metrics() 函数评估，涵盖了 NDCG、MRR、Recall、Hit、Precision 。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;这次任务有赖神直播讲解，讲解后再摸盘整个项目就比较顺利了。于是通过 debug 一个 toy example 大致了解了 Torch-RecHub 项目的架构、工程设计，以及大致的 pipeline。因为我是做会话推荐的，对 GRU4Rec 这个模型比较熟悉，所以上来就用它来当作 example 学习，原以为会很顺利，结果发现在 Model 里使用了我不太熟悉的”双塔结构“，有点懵逼。因为会话推荐任务里，其实没有 user 侧信息，所以根本不需要 model.inference_embedding()，只需要调用 model.predict() 做模型推断，所以刚开始一直没转过弯，也没有理解这两个的区别。后来想起曾经听过赖神的双塔分享，才恍然大悟。。</p>
<p>&emsp;&emsp;第一次打卡任务顺利完成！希望可以坚持！</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>Datawhale组队学习</tag>
        <tag>RecHub</tag>
      </tags>
  </entry>
  <entry>
    <title>RecHub推荐项目学习2：精排模型 DeepFM、DIN</title>
    <url>/2022/06/19/RecHub%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A02/</url>
    <content><![CDATA[<h1 id="Task02"><a href="#Task02" class="headerlink" title="Task02"></a>Task02</h1><p>&emsp;&emsp;Task02：精排模型 DeepFM DIN</p>
<p>&emsp;&emsp;参考资料：<a href="https://datawhalechina.github.io/fun-rec/#/">FunRec文档</a>，<a href="https://github.com/datawhalechina/torch-rechub">RecHub源码</a></p>
<h1 id="推荐模型发展的时间线"><a href="#推荐模型发展的时间线" class="headerlink" title="推荐模型发展的时间线"></a>推荐模型发展的时间线</h1><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618103615992.png" alt="image-20220618103615992" style="zoom:50%;" /></p>
<p>&emsp;&emsp;这张图来自[1]，放出这张图的原因是便于从时间线上感受这些模型的发展。本期学习的 DIN 还算是比较独立的存在，它在前面模型 DNN 思想的基础上加入了注意力机制。而 DeepFM ，从时间线上可以看到 DeepFM 模型是在 FM、FNN、PNN、Wide&amp;Deep 之后推出的，其实也是对这些模型的改进，为了更好地理解 DeepFM，至少得先了解它们。</p>
<span id="more"></span>
<h1 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h1><p>&emsp;&emsp;DeepFM 提出的动机主要有两点：</p>
<ul>
<li><p>CTR 预估任务中特征交叉至关重要，但是特征交叉通常需要非常专业的特征工程。例如Wide&amp;Deep的Wide部分需要手工构造pairwise特征，huge size并且费人力，复杂。</p>
</li>
<li><p>一些模型例如FNN、PNN只能学习到高阶特征组合；Wide&amp;Deep在输出层直接将低阶和高阶特征相加组合，很容易让模型最终偏向（bias）学习到低阶或者高阶的特征，而不能很好地结合。</p>
</li>
</ul>
<p>针对上面两个问题，DeepFM的解决方案分别是：</p>
<ul>
<li>改进 Wide&amp;Deep 的 Wide 部分，使用 FM 代替手工构造特征</li>
<li>Wide 部分和 Deep 部分使用相同的 embedding 输入，不会导致 bias</li>
</ul>
<h2 id="DeepFM-的前辈们"><a href="#DeepFM-的前辈们" class="headerlink" title="DeepFM 的前辈们"></a>DeepFM 的前辈们</h2><p>&emsp;&emsp;上面提到了FM、FNN、PNN、Wide&amp;Deep，为了更好地理解 DeepFM ，这里简单介绍下：FM 捕捉低阶特征组合，FNN、PNN 捕捉高阶特征组合，Wide&amp;Deep 的 Wide 部分捕捉低阶特征组合，Deep 部分捕捉高阶特征组合。更详细的介绍如下</p>
<h3 id="低阶特征组合：FM"><a href="#低阶特征组合：FM" class="headerlink" title="低阶特征组合：FM"></a>低阶特征组合：FM</h3><p>&emsp;&emsp;用[2]的例子介绍 FM ，假设一个广告分类的问题，根据用户和广告位相关的特征，预测用户是否点击了广告。源数据如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618155318348.png" alt="image-20220618155318348"></p>
<p>&emsp;&emsp;Clicked? 是 label ，Country、Day、Ad_type 是三类特征，并且都是类别型特征，需要经过 one-hot 编码才能转换为数值型特征，经过 one-hot 编码后如下所示：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220618155325444.png" alt="image-20220618155325444"></p>
<p>&emsp;&emsp;可以看出，经过 one-hot 编码，每条数据都变稀疏了，每个样本有7维特征，但仅有3维特征具有非零值。在实际场景中，商品种类、用户职业、地区等类别非常多，经过 one-hot 编码后样本维度会迅速上升，且非常稀疏。</p>
<h4 id="POLY2"><a href="#POLY2" class="headerlink" title="POLY2"></a>POLY2</h4><p>&emsp;&emsp;这时候其实可以用 LR 训练了，但是通过观察大量的样本数据可以发现，某些特征经过<strong>关联</strong>之后，与label之间的相关性就会提高。例如，“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。因此，引入两个<strong>特征的组合</strong>是非常有意义的，这也就是常说的<strong>特征交叉</strong>。因此，一个非常自然的想法诞生了，在 LR 的基础上加入<strong>二阶特征组合</strong>，这也是 <strong>POLY2</strong> 的思路：</p>
<script type="math/tex; mode=display">
y(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{i j} x_{i} x_{j}</script><p>&emsp;&emsp;公式中的 $x_i \in \{0,1\}$ 表示每个特征。</p>
<p>&emsp;&emsp;这样做考虑了<strong>二阶特征交叉</strong>，但是也产生了新的问题。</p>
<ul>
<li>假设经过 one-hot 编码以后的样本特征维度为 $n$ ，那么上述公式中的 $w_{ij}$ 共有 $\frac{n(n-1)}{2}$ 个，权重参数量从 $O(n)$ 上升到了 $O(n^2)$ 。</li>
<li>并且 $w_{ij}$ 还有一个“训练难”的问题，因为每个参数 $w_{ij}$ 的训练需要大量 $x_i$ 和 $x_j$ 都非零的样本；由于样本数据本来就比较稀疏，满足“ $x_i$ 和 $x_j$ 都非零”的样本将会非常少。训练样本的不足，很容易导致参数 $w_{ij}$ 不准确，最终将严重影响模型的性能。</li>
</ul>
<script type="math/tex; mode=display">
y(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n} <\mathbf{w}_i,\mathbf{w}_j> x_{i} x_{j}</script><p>&emsp;&emsp;受矩阵分解算法的启发，FM 提出隐向量的概念，用两个向量的内积 $&lt;\mathbf{w}_i,\mathbf{w}_j&gt;$ 代替 $w_{ij}$ ，也就是每一维特征都对应一个隐向量，共有 $n$ 个隐向量，在做二阶特征交叉时，用两个向量的内积表示这两个组合特征的权重 $w_{ij}$，这样一来就解决了 POLY2 的两个问题：</p>
<ul>
<li>权重参数量由 $O(n^2)$ 减少到  $O(nk),k&lt;&lt;n$ 。</li>
<li>隐向量的引入使得 $ x_{h} x_{i}$ 的参数和 $ x_{i} x_{j}$ 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计 FM 的二次项参数。具体来说， $ x_{h} x_{i}$ 和 $ x_{i} x_{j}$ 的系数分别为 $&lt;\mathbf{w}_h,\mathbf{w}_i&gt;$ 和$&lt;\mathbf{w}_i,\mathbf{w}_j&gt;$，它们之间有共同项 $\mathbf{w}_i$。也就是说，所有包含“ $x_i$ 的非零组合特征”（存在某个 $j\neq i$，使得 $ x_{i} x_{j}\neq 0$ ）的样本都可以用来学习隐向量 $\mathbf{w}_i$，这很大程度上缓解了数据稀疏性的问题。相比POLY2， FM虽然丢失了某些具体特征组合的精确记忆能力， 但是泛化能力大大提高。  </li>
</ul>
<h4 id="计算优化"><a href="#计算优化" class="headerlink" title="计算优化"></a>计算优化</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619134325217.png" alt="image-20220619134325217" style="zoom: 80%;" /></p>
<p>&emsp;&emsp;补一个小知识点，FM的计算公式可以<a href="https://zhuanlan.zhihu.com/p/343174108">化简</a>[4]，将时间复杂度从 $O(n^2)$ 减少到 $O(n)$ 。</p>
<h3 id="高阶特征组合：DNN、FNN"><a href="#高阶特征组合：DNN、FNN" class="headerlink" title="高阶特征组合：DNN、FNN"></a>高阶特征组合：DNN、FNN</h3><h4 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h4><p>&emsp;&emsp;比起 FM 的二阶特征组合能力，DNN 能实现<strong>更高阶的特征组合</strong>。但是 DNN 也存在一些问题[3]：</p>
<p>&emsp;&emsp;当我们使用DNN网络解决推荐问题的时候，存在网络参数过于庞大的问题，这是因为在进行特征处理的时候我们需要使用one-hot编码来处理离散特征，这会导致输入的维度猛增。这里借用AI大会的一张图片：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101907599.png" alt="image-20220619101907599"></p>
<p>&emsp;&emsp;这样庞大的参数量也是不实际的。为了解决 DNN 参数量过大的局限性，可以采用非常经典的 Field 思想，将 OneHot 特征转换为 Dense Vector </p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101925322.png" alt="image-20220619101925322" style="zoom:50%;" /></p>
<p>&emsp;&emsp;此时通过增加全连接层就可以实现<strong>高阶的特征组合</strong>，如下图所示：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101941306.png" alt="image-20220619101941306" style="zoom:50%;" /></p>
<h4 id="FNN"><a href="#FNN" class="headerlink" title="FNN"></a>FNN</h4><p>&emsp;&emsp;DNN 能够实现高阶特征组合，但是低阶的特征组合也很重要，于是一些模型例如 FNN ，在 DNN 基础上，增加 FM 来表示低阶的特征组合，以下是 FNN 的模型图。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619101451894.png" alt="image-20220619101451894" style="zoom: 60%;" />       <img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619102848286.png" alt="image-20220619102848286" style="zoom: 60%;" />   </p>
<p>&emsp;&emsp;可以看出 FNN 在Embedding 层做了改进，利用 FM 的权重进行参数初始化，这样做其实有两点好处：</p>
<ul>
<li>因为 1）Embedding 层的参数数量巨大；2）在进行梯度下降优化时，只有与非零特征相连的 Embedding 层权重会被更新。这两点原因导致 Embedding 层收敛速度很慢，利用 FM 训练好的隐向量初始化 Embedding 层的参数，相当于在初始化神经网络参数时，已经引入了有价值的先验信息。 也就是说， 神经网络训练的起点更接近目标最优点， 自然<strong>加速了整个神经网络的收敛过程</strong>。  </li>
<li>模型图的 Embedding 层， $w_0$ 是 FM 公式里的偏置项，$w_1$ 是一阶特征组合项，剩下的是二阶特征组合项，所以 FNN 也加入了低阶特征组合（虽然经过 DNN 后这些低阶特征组合几乎没有了）</li>
</ul>
<h3 id="Wide-amp-Deep"><a href="#Wide-amp-Deep" class="headerlink" title="Wide&amp;Deep"></a>Wide&amp;Deep</h3><p>&emsp;&emsp;紧接着上面，“经过 DNN 后这些低阶特征组合几乎没有了”，这是实际上是由于 FNN 中，“ FM 预训练，再用 DNN 训练最终的模型”，这样<strong>串行</strong>的模式导致的，也就是虽然 FM 学到了低阶特征组合，但是 DNN 的全连接结构导致低阶特征在 DNN 中又被组合成了高阶特征组合，所以没有保留下低阶特征组合。看来我们已经找到问题了，将<strong>串行</strong>方式改进为<strong>并行</strong>方式能比较好的解决这个问题。于是Google提出了 Wide&amp;Deep 模型，见下图。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619104157758.png" alt="image-20220619104157758"></p>
<p>&emsp;&emsp;Wide&amp;Deep 的设计初衷是为了赋予模型<strong>记忆能力</strong>和<strong>泛化能力</strong>，记忆能力通过 Wide 部分实现，泛化能力通过 Deep 部分实现。</p>
<blockquote>
<p>“记忆能力” 可以被理解为模型直接学习并利用历史数据中物品或者特征的“共现频率”的能力。 一般来说， 协同过滤、 逻辑回归等简单模型有较强的“记忆能力”。 由于这类模型的结构简单， 原始数据往往可以直接影响推荐结果， 产生类似于“如果点击过A， 就推荐B”这类规则式的推荐， 这就相当于模型直接记住了历史数据的分布特点， 并利用这些记忆进行推荐。</p>
<p>“泛化能力” 可以被理解为模型传递特征的相关性， 以及发掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力。 矩阵分解比协同过滤的泛化能力强， 因为矩阵分解引入了隐向量这样的结构， 使得数据稀少的用户或者物品也能生成隐向量， 从而获得有数据支撑的推荐得分， 这就是非常典型的将全局数据传递到稀疏物品上， 从而提高泛化能力的例子。 再比如， 深度神经网络通过特征的多次自动组合， 可以深度发掘数据中潜在的模式， 即使是非常稀疏的特征向量输入， 也能得到较稳定平滑的推荐概率， 这就是简单模型所缺乏的“泛化能力”。</p>
</blockquote>
<p>&emsp;&emsp;但是如果深入探究 Wide&amp;Deep 的构成方式，虽然将整个模型的结构调整为了并行结构，在实际的使用中 Wide 部分需要较为精巧的特征工程，换句话说人工处理对于模型的效果具有比较大的影响，大家可以看到下图红圈内的 Wide 部分采用了两个 id 类特征的乘积，这是 Google 团队<strong>根据业务精心选择</strong>的想让模型<strong>直接记忆</strong>的特征组合。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619104313037.png" alt="image-20220619104313037"></p>
<p>&emsp;&emsp;Wide&amp;Deep 其实还有一个没那么容易能够发现的问题，我们看下面这张图（图片来自FunRec）：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619123835759.png" alt="image-20220619123835759"></p>
<p>&emsp;&emsp;在模型前向计算的时候，Wide 和 Deep 部分的输入不同，Wide 的输入只有低阶特征组合，Deep 则可以输入低阶和高阶；两部分各自输出一个标量 logits，最后学习它们的权重系数加权求和，再过 sigmoid 激活。DeepFM 的论文里，指出这样做可能使得模型最终<strong>偏向学习到低阶或者高阶的特征</strong>，不能做到很好的结合，究其原因还是高阶和低阶特征的输入是分开的。</p>
<h2 id="回到DeepFM"><a href="#回到DeepFM" class="headerlink" title="回到DeepFM"></a>回到DeepFM</h2><p>&emsp;&emsp;综合上述几个模型，FM 能够高效进行特征交叉捕捉<strong>低阶</strong>特征组合；DNN、FNN、PNN 能够捕捉<strong>高阶特征组合</strong>；Wide&amp;Deep 结合两者，同时捕捉低阶和高阶特征，但仍有两个问题：1）学习有偏，最后会偏向学习低阶或者高阶特征。2）Wide 部分需要手工设计特征，费时费力。</p>
<p>&emsp;&emsp;至此，终于轮到 DeepFM 登场。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619132058752.png" alt="image-20220619132058752"></p>
<p>&emsp;&emsp;DeepFM 仍然沿用了 Wide&amp;Deep “记忆+泛化”两部分建模的思想，设计 FM Component 负责“记忆”，Deep Component 负责“泛化”。其中巧妙地利用 FM 的思想解决了 Wide&amp;Deep 的两个问题：<strong>特征工程困难</strong>和<strong>学习有偏</strong>。</p>
<p>&emsp;&emsp;具体是如何用 FM 思想解决这两部分问题的呢？下面看 FM 部分的模型图：</p>
<h3 id="FM-Component"><a href="#FM-Component" class="headerlink" title="FM Component"></a>FM Component</h3><p>&emsp;&emsp;FM 部分，改进 Wide&amp;Deep  的 Wide，使得不再需要手动构造（二阶）交叉特征，也能捕捉低阶特征组合。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619133002341.png" alt="image-20220619133002341" style="zoom:67%;" /></p>
<p>&emsp;公式：</p>
<script type="math/tex; mode=display">
y_{F M}=\langle w, x\rangle+\sum_{i=1}^{d} \sum_{j=i+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{i} \cdot x_{j}</script><blockquote>
<p>这里每个 field 都是 one-hot ，原文里说的，所以不用纠结同一个 field 里是否是 multi-hot 了，如果是 multi-hot 也可以用各种 pooling 方式转化成一个向量。</p>
</blockquote>
<p>&emsp;&emsp;在 FM Layer，共有两种操作，Addition 和 Inner Product（分别用绿色和蓝色箭头标出）。</p>
<ul>
<li><strong>Addition</strong>. 对 Sparse Feature（Field level）线性加和，即 $\langle w, x\rangle$ .</li>
<li><strong>Inner Product</strong>. 将每个 field 的 one-hot 向量转化成 dense embedding，把它看作 FM 的 latent vector（Dense Embeddings 层在 Deep Component 里介绍），然后做点积操作即  $\sum_{i=1}^{d} \sum_{j=i+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{i} \cdot x_{j}$ . </li>
</ul>
<p>&emsp;&emsp;这里我曾经纠结了很久，因为不知道特征究竟是如何组合的，是 $\frac{n(n-1)}{2}$ 次组合，还是 $\frac{m(m-1)}{2}$ 次组合。最后查阅资料加上自己整理，回答是这样的：</p>
<ul>
<li>$\sum_{i=1}^{d} \sum_{j=i+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{i} \cdot x_{j}$  反映二阶特征交叉，$V_i$ , $V_j$ 是特征对应的 latent vector，也就是对应的embedding。<br>所以这里其实不是所有 feature（所有field里的所有feature，num=$n$）的两两交叉，其实是 $m$ 个域的特征交叉，不过每个域是可以涵盖到域里所有的 feature 的（每个域从域包含的特征里选择一个，因为是 one-hot）。也就是说，当一条数据输入进去的时候，不会对所有 feature 做特征交叉（即 $\frac{n(n-1)}{2}$ 次组合），而是会对所有域做特征交叉（即 $\frac{m(m-1)}{2}$ 次组合），但是当数据量足够多时，就能涵盖到所有feature的交叉。</li>
</ul>
<h3 id="Deep-Component"><a href="#Deep-Component" class="headerlink" title="Deep Component"></a>Deep Component</h3><p>&emsp;&emsp;Deep 部分和 DNN 一样，捕捉高阶特征组合。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619135133347.png" alt="image-20220619135133347"></p>
<p>&emsp;&emsp;用全连接的方式将 Dense Embedding 输入到 Hidden Layer ，这里面 Dense Embeddings 就是用 Field 思想解决 DNN 中的参数爆炸问题，这也是推荐模型中常用的处理方法。然后 Dense Embeddings 拼接以后传入 DNN 。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619135849196.png" alt="image-20220619135849196"></p>
<p>&emsp;&emsp;上图是 Dense Embeddings 层的结构，这里有两点需要指出：1）尽管不同 field 的长度可能不同，但 embedding 维度 $k$ 都是相同的；2）FM 里的 latent vector 现在充当作为网络的权重参数（回忆一下矩阵乘法，有 1 的地方对应的一列权重参数就是 latent vector），它们是学习得到的，被用来将 field 的 one-hot 向量压缩成 embedding 向量。</p>
<p>&emsp;与 FNN 不同的是，这里的 latent vector 不是预训练而是随机初始化得到的，并且是不断学习优化的。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619143513920.png" alt="image-20220619143513920" style="zoom:67%;" /></p>
<p>&emsp;&emsp;至此我们还可以发现，FM 部分和 Deep 部分是共享 embedding 的，这就解决了“<strong>学习有偏</strong>”的问题，因为高阶和低阶特征都是从同一个 embedding 层获得的。再回想 FM 部分，用 latent vector 做内积组合二阶特征的方式避免了“<strong>特征工程困难</strong>”的问题。DeepFM 的主要贡献就是在于对 Wide&amp;Deep 进行了这两方面的改进。</p>
<h1 id="DeepFM-代码"><a href="#DeepFM-代码" class="headerlink" title="DeepFM 代码"></a>DeepFM 代码</h1><p>&emsp;&emsp;开源代码见：<a href="https://github.com/datawhalechina/torch-rechub/blob/main/tutorials/DeepFM.ipynb">torch-rechub/DeepFM.ipynb at main · datawhalechina/torch-rechub (github.com)</a></p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>&emsp;&emsp;使用的是 Criteo 的一个 sample</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152139173.png" alt="image-20220619152139173"></p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152209082.png" alt="image-20220619152209082"></p>
<p>&emsp;&emsp;至此都比较好理解。</p>
<h3 id="Dense-特征"><a href="#Dense-特征" class="headerlink" title="Dense 特征"></a>Dense 特征</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152251762.png" alt="image-20220619152251762"></p>
<p>&emsp;&emsp;这里 <code>convert_numeric_feature()</code> 有点令人费解，据说是比赛中冠军队伍使用的方法，emm，EDA做得好，同时不得不佩服大佬们的创造力~ 总之经过这样的变换，将 dense 特征都转化成了新的 sparse 特征列。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152310615.png" alt="image-20220619152310615"></p>
<p>&emsp;&emsp;将 dense 特征转化成新的 sparse 特征后，dense 特征本身还要做一些归一化操作，这里使用 <code>MinMaxScaler()</code> 。</p>
<p>&emsp;&emsp;其实 Wide&amp;Deep 里做完归一化以后还做了分组，见下图：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619150451559.png" alt="image-20220619150451559" style="zoom:50%;" /></p>
<h3 id="Sparse-特征"><a href="#Sparse-特征" class="headerlink" title="Sparse 特征"></a>Sparse 特征</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619152328870.png" alt="image-20220619152328870"></p>
<p>&emsp;&emsp;直接用 <code>LabelEncoder()</code> 编码。</p>
<h2 id="定义-DataGenerator-Dataset-Dataloader"><a href="#定义-DataGenerator-Dataset-Dataloader" class="headerlink" title="定义 DataGenerator (Dataset + Dataloader)"></a>定义 DataGenerator (Dataset + Dataloader)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#重点：将每个特征定义为torch-rechub所支持的特征基类，dense特征只需指定特征名，sparse特征需指定特征名、特征取值个数(vocab_size)、embedding维度(embed_dim)</span></span><br><span class="line">dense_features = [DenseFeature(feature_name) <span class="keyword">for</span> feature_name <span class="keyword">in</span> dense_cols]</span><br><span class="line">sparse_features = [SparseFeature(feature_name, vocab_size=data[feature_name].nunique(), embed_dim=<span class="number">16</span>) <span class="keyword">for</span> feature_name <span class="keyword">in</span> sparse_cols]</span><br><span class="line">y = data[<span class="string">&quot;label&quot;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">x = data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型输入所需要的dataloader，区分验证集、测试集，指定batch大小</span></span><br><span class="line"><span class="comment">#split_ratio=[0.7,0.1] 指的是训练集占比70%，验证集占比10%，剩下的全部为测试集</span></span><br><span class="line">dg = DataGenerator(x, y) </span><br><span class="line">train_dataloader, val_dataloader, test_dataloader = dg.generate_dataloader(split_ratio=[<span class="number">0.7</span>, <span class="number">0.1</span>], batch_size=<span class="number">256</span>, num_workers=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<h2 id="定义-Model"><a href="#定义-Model" class="headerlink" title="定义 Model"></a>定义 Model</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.models.ranking <span class="keyword">import</span> DeepFM</span><br><span class="line"><span class="keyword">from</span> torch_rechub.trainers <span class="keyword">import</span> CTRTrainer</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line">model = DeepFM(</span><br><span class="line">        deep_features=dense_features+sparse_features,</span><br><span class="line">        fm_features=sparse_features,</span><br><span class="line">        mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>], <span class="string">&quot;dropout&quot;</span>: <span class="number">0.2</span>, <span class="string">&quot;activation&quot;</span>: <span class="string">&quot;relu&quot;</span>&#125;,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;我们进入 DeepFM 模型内部看一看：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619153802390.png" alt="image-20220619153802390"></p>
<p>&emsp;&emsp;从 forward() 里可以看出，在代码实现时，其实不完全是分成 deep 和 fm 两部分，而是分成 deep 、fm、linear 三部分。论文里的 fm 部分是包含一阶特征和二阶特征交叉的，代码实现的时候把一阶特征单独拿出来用 linear 实现。</p>
<p>&emsp;&emsp;LR 的实现如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619154134742.png" alt="image-20220619154134742"></p>
<p>&emsp;&emsp;FM 的实现如下（不是完全体的 FM ，这里是只计算二阶特征的 FM ）：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619154214878.png" alt="image-20220619154214878"></p>
<p>&emsp;&emsp;这里 FM 的公式是计算优化后的，可以参考下面的公式：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619134325217.png" alt="image-20220619134325217" style="zoom: 80%;" /></p>
<p>&emsp;&emsp;MLP 的实现如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619154603820.png" alt="image-20220619154603820"></p>
<p>&emsp;&emsp;我们在传参的时候：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>], <span class="string">&quot;dropout&quot;</span>: <span class="number">0.2</span>, <span class="string">&quot;activation&quot;</span>: <span class="string">&quot;relu&quot;</span>&#125;,</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里的 dims 的参数列表就表示从输入到输出，维度依次是多少，这个例子中就是： input_dim —&gt; 256 —&gt; 128 —&gt; 1 。没有指定 “output_layer” 的话，会默认再过一个 nn.linear() 让维度变成 1 ，当然 output_layer 后是不接激活函数的。 </p>
<p>另外注意这里添加了 BatchNorm 。</p>
<h2 id="定义-trainer"><a href="#定义-trainer" class="headerlink" title="定义 trainer"></a>定义 trainer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练，需要学习率、设备等一般的参数，此外我们还支持earlystoping策略，及时发现过拟合</span></span><br><span class="line">ctr_trainer = CTRTrainer(</span><br><span class="line">    model,</span><br><span class="line">    optimizer_params=&#123;<span class="string">&quot;lr&quot;</span>: <span class="number">1e-4</span>, <span class="string">&quot;weight_decay&quot;</span>: <span class="number">1e-5</span>&#125;,</span><br><span class="line">    n_epoch=<span class="number">1</span>,</span><br><span class="line">    earlystop_patience=<span class="number">3</span>,</span><br><span class="line">    device=<span class="string">&#x27;cpu&#x27;</span>, <span class="comment">#如果有gpu，可设置成cuda:0</span></span><br><span class="line">    model_path=<span class="string">&#x27;./&#x27;</span>, <span class="comment">#模型存储路径</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;不需要定义损失函数，因为 ctr 预估任务都是 BCE loss，默认评价指标是 auc 。</p>
<h2 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ctr_trainer.fit(train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看在测试集上的性能</span></span><br><span class="line">auc = ctr_trainer.evaluate(ctr_trainer.model, test_dataloader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;test auc: <span class="subst">&#123;auc&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619155836040.png" alt="image-20220619155836040"></p>
<h2 id="使用其它排序模型"><a href="#使用其它排序模型" class="headerlink" title="使用其它排序模型"></a>使用其它排序模型</h2><h3 id="调用现成模型"><a href="#调用现成模型" class="headerlink" title="调用现成模型"></a>调用现成模型</h3><p>&emsp;&emsp;调用现成的模型非常容易，只需要修改 model 参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义相应的模型，用同样的方式训练</span></span><br><span class="line">model = WideDeep(wide_features=dense_features, deep_features=sparse_features, mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>], <span class="string">&quot;dropout&quot;</span>: <span class="number">0.2</span>, <span class="string">&quot;activation&quot;</span>: <span class="string">&quot;relu&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">model = DCN(features=dense_features + sparse_features, n_cross_layers=<span class="number">3</span>, mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619163102998.png" alt="image-20220619163102998"></p>
<h1 id="DIN"><a href="#DIN" class="headerlink" title="DIN"></a>DIN</h1><p>&emsp;&emsp;DIN 还算是比较独立的存在，它在前面模型 DNN 思想的基础上加入了注意力机制。</p>
<h2 id="Base-Model"><a href="#Base-Model" class="headerlink" title="Base Model"></a>Base Model</h2><p>&emsp;&emsp;DIN 也是广告推荐场景，一般来说，模型的输入特征自然分为三部分：一部分是用户 $u$ 的特征（下图的 User Profile 和 User Behaviors），一部分是候选广告 $a$ 的特征（Candidate Ad），一部分是上下文特征（Context Features）。我们把用户的 User Behaviors 和广告的 Candidate Ad 两类特征组单独拿出来看，为什么要单独挑出来？因为它们都含有两个非常重要的特征——商品 id 和商铺 id。用户特征里的商品 id 是一个序列，代表用户曾经点击过的商品集合，商铺 id 统里；而广告特征里的商品 id  和商铺 id 就是广告对应的商品 id 和商铺 id 。下图是论文中用到的一个 base model，也是 DIN 之前绝大多数模型的做法，即给模型输入 one-hot 或 multi-hot 向量，再经过 Embedding 层转化成 $1 \times d$  的 dense embedding 向量，multi-hot 向量还得在特征组（Field）内进行 pooling 操作转化才能转化成 $1 \times d$ 的向量，论文选用最常用的 sum pooling。在得到每个特征组的向量后，对所有向量进行 concat 然后送入 DNN ，这就是最一般的做法。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619222927973.png" alt="image-20220619222927973"></p>
<p>&emsp;&emsp;这些模型在这种个性化广告点击预测任务中存在的问题就是<strong>无法表达用户广泛的兴趣</strong>，因为这些模型在得到各个特征的embedding之后，就蛮力拼接了，然后就各种交叉等。这时候根本没有考虑之前用户历史行为商品具体是什么，究竟用户历史行为中的哪个会对当前的点击预测带来积极的作用。 而实际上，对于用户点不点击当前的商品广告，很大程度上是依赖于他的历史行为的，王喆老师[1]举了个例子</p>
<blockquote>
<p>假设广告中的商品是键盘， 如果用户历史点击的商品中有化妆品， 包包，衣服， 洗面奶等商品， 那么大概率上该用户可能是对键盘不感兴趣的， 而如果用户历史行为中的商品有鼠标， 电脑，iPad，手机等， 那么大概率该用户对键盘是感兴趣的， 而如果用户历史商品中有鼠标， 化妆品， T-shirt和洗面奶， 鼠标这个商品embedding对预测“键盘”广告的点击率的重要程度应该大于后面的那三个。</p>
</blockquote>
<p>&emsp;&emsp;这里也就是说如果是之前的那些深度学习模型，是没法很好的去表达出用户这广泛多样的兴趣的，如果想表达的准确些， 那么就得加大隐向量的维度，让每个特征的信息更加丰富， 那这样带来的问题就是计算量上去了，毕竟真实情景尤其是电商广告推荐的场景，特征维度的规模是非常大的。 并且根据上面的例子， 也<strong>并不是用户所有的历史行为特征都会对某个商品广告点击预测起到作用</strong>。所以对于当前某个商品广告的点击预测任务，没必要考虑之前所有的用户历史行为。</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>&emsp;&emsp;这样， DIN的动机就出来了，在业务的角度，我们应该自适应的去捕捉用户的兴趣变化，这样才能较为准确的实施广告推荐；而放到模型的角度， 我们应该<strong>考虑到用户的历史行为商品与当前商品广告的一个关联性</strong>，如果用户历史商品中很多与当前商品关联，那么说明该商品可能符合用户的品味，就把该广告推荐给他。而一谈到关联性的话， 我们就容易想到“注意力”的思想了， 所以为了更好的从用户的历史行为中学习到与当前商品广告的关联性，学习到用户的兴趣变化， 作者把<strong>注意力</strong>引入到了模型，设计了一个”local activation unit”结构，利用候选商品和历史问题商品之间的相关性计算出权重，这个就代表了对于当前商品广告的预测，用户历史行为的各个商品的重要程度大小， 而加入了注意力权重的深度学习网络，就是这次的主角DIN， 下面具体来看下该模型。</p>
<p>&emsp;&emsp;论文原版图：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619224333801.png" alt="image-20220619224333801"></p>
<p>&emsp;&emsp;王喆老师的配图：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619224732551.png" alt="image-20220619224732551"></p>
<h2 id="注意力激活单元"><a href="#注意力激活单元" class="headerlink" title="注意力激活单元"></a>注意力激活单元</h2><p>&emsp;&emsp;注意力机制的公式可以定义如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{V}_{\mathrm{u}}=f\left(\boldsymbol{V}_{\mathrm{a}}\right)=\sum_{i=1}^{N} w_{i} \cdot \boldsymbol{V}_{i}=\sum_{i=1}^{N} g\left(\boldsymbol{V}_{i}, \boldsymbol{V}_{\mathrm{a}}\right) \cdot \boldsymbol{V}_{i}</script><p>&emsp;&emsp;其中， $\boldsymbol{V}_{\mathrm{u}}$ 是用户的Embedding向量， $\boldsymbol{V}_{\mathrm{a}}$ 是候选广告商品的 Embedding 向量， $\boldsymbol{V}_{\mathrm{i}}$ 是用户 $u$ 的第 $i$ 次行为的 Embedding 向量。 这里用户的行为就是浏览商品或店铺， 因此行为的 Embedding 向量就是那次浏览的商品或店铺的 Embedding 向量。  </p>
<p>&emsp;&emsp;因为加入了注意力机制， 所以 $\boldsymbol{V}_{\mathrm{u}}$ 从过去 $\boldsymbol{V}_{\mathrm{i}}$ 的加和变成了 $\boldsymbol{V}_{\mathrm{i}}$ 的加权和，  $\boldsymbol{V}_{\mathrm{i}}$ 的权重 $w_i$ 就由 $\boldsymbol{V}_{\mathrm{i}}$ 与 $\boldsymbol{V}_{\mathrm{a}}$ 的关系决定， 也就是公式中的$ g\left(\boldsymbol{V}_{i}, \boldsymbol{V}_{\mathrm{a}}\right)$， 即“注意力得分”。  </p>
<p>&emsp;&emsp;那么到底应该如何计算注意力得分呢，论文设计了“local activation unit”，即注意力激活单元。这个注意力激活单元本质上也是小的神经网络，看王喆老师的配图比较清晰，在图的右上角。</p>
<p>&emsp;&emsp;可以看出， 激活单元的输入层是两个 Embedding 向量， 经过元素减（element-wise minus） 操作后， 与原Embedding向量一同连接后形成全连接层的输入， 最后通过单神经元输出层生成注意力得分。  </p>
<p>&emsp;&emsp;注意商铺 id 只跟用户历史行为中的商铺 id 序列发生作用， 商品 id 只跟用户的商品 id 序列发生作用， 因为注意力的轻重更应该由同类信息的相关性决定。  </p>
<h2 id="有意思的发现"><a href="#有意思的发现" class="headerlink" title="有意思的发现"></a>有意思的发现</h2><p>&emsp;&emsp;论文作者发现用 LSTM 建模用户的历史行为，效果没有提升。作者提出，可能原因是会引入噪声。挺有意思哈哈哈哈，因为后面几年<strong>序列推荐</strong>火起来了，专门研究用户历史行为！这里作者说后续研究是不是 DIEN，还没看过论文，还不知道。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220619230308830.png" alt="image-20220619230308830"></p>
<h1 id="DIN代码"><a href="#DIN代码" class="headerlink" title="DIN代码"></a>DIN代码</h1><p>&emsp;&emsp;这里我们以Amazon-Electronics为例，原数据是json格式，我们提取所需要的信息预处理为一个仅包含user_id, item_id, cate_id, time四个特征列的CSV文件。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>&emsp;&emsp;完整的数据长这样，共有 1,689,188 条用户交互记录。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620012113828.png" alt="image-20220620012113828" style="zoom:67%;" /></p>
<p>&emsp;&emsp;在该数据集上，我们没有 dense feature，只有 sparse feature，除此之外我们还要构造 sequence feature。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620012500712.png" alt="image-20220620012500712"></p>
<p>&emsp;&emsp;这里用一个关键函数 <code>create_seq_features()</code> 构造序列特征，我们进到函数内看一看，解析见注释。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_seq_features</span>(<span class="params">data, seq_feature_col=[<span class="string">&#x27;item_id&#x27;</span>, <span class="string">&#x27;cate_id&#x27;</span>], max_len=<span class="number">50</span>, drop_short=<span class="number">3</span>, shuffle=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Build a sequence of user&#x27;s history by time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data (pd.DataFrame): must contain keys: `user_id, item_id, cate_id, time`.</span></span><br><span class="line"><span class="string">        seq_feature_col (list): specify the column name that needs to generate sequence features, and its sequence features will be generated according to userid.</span></span><br><span class="line"><span class="string">        max_len (int): the max length of a user history sequence.</span></span><br><span class="line"><span class="string">        drop_short (int): remove some inactive user who&#x27;s sequence length &lt; drop_short.</span></span><br><span class="line"><span class="string">        shuffle (bool): shuffle data if true.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        train (pd.DataFrame): target item will be each item before last two items.</span></span><br><span class="line"><span class="string">        val (pd.DataFrame): target item is the second to last item of user&#x27;s history sequence.</span></span><br><span class="line"><span class="string">        test (pd.DataFrame): target item is the last item of user&#x27;s history sequence.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> data:   <span class="comment"># 对 data 的每一列都进行 labelEncode</span></span><br><span class="line">        le = LabelEncoder()</span><br><span class="line">        data[feat] = le.fit_transform(data[feat])</span><br><span class="line">        data[feat] = data[feat].apply(<span class="keyword">lambda</span> x: x + <span class="number">1</span>)  <span class="comment"># LabelEncoder() 是从 0 开始编号的，这里留出 0 用来后续 padding</span></span><br><span class="line">    data = data.astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    n_items = data[<span class="string">&quot;item_id&quot;</span>].<span class="built_in">max</span>() <span class="comment"># labelEncode 的id都+1了，所以 物品数=最大物品id</span></span><br><span class="line"></span><br><span class="line">    item_cate_map = data[[<span class="string">&#x27;item_id&#x27;</span>, <span class="string">&#x27;cate_id&#x27;</span>]]</span><br><span class="line">    item2cate_dict = item_cate_map.set_index([<span class="string">&#x27;item_id&#x27;</span>])[<span class="string">&#x27;cate_id&#x27;</span>].to_dict()  <span class="comment"># 生成 &#123;item:category&#125; 字典</span></span><br><span class="line"></span><br><span class="line">    data = data.sort_values([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;time&#x27;</span>]).groupby(<span class="string">&#x27;user_id&#x27;</span>).agg(click_hist_list=(<span class="string">&#x27;item_id&#x27;</span>, <span class="built_in">list</span>), cate_hist_hist=(<span class="string">&#x27;cate_id&#x27;</span>, <span class="built_in">list</span>)).reset_index()</span><br><span class="line">    <span class="comment"># 按user_id,time 这样的优先级排序整个 data ，在以user_id来分组，聚合时生成“用户点击商品序列”和“点击商品类别序列”</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Sliding window to construct negative samples</span></span><br><span class="line">    train_data, val_data, test_data = [], [], []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data.itertuples():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="number">2</span>]) &lt; drop_short:   <span class="comment"># 序列长度小于阈值直接丢弃</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        user_id = item[<span class="number">1</span>]</span><br><span class="line">        click_hist_list = item[<span class="number">2</span>][:max_len] <span class="comment"># 上面排序是从小到大排的,阶段前段的maxlen是否不合理？</span></span><br><span class="line">        cate_hist_list = item[<span class="number">3</span>][:max_len]</span><br><span class="line"></span><br><span class="line">        neg_list = [neg_sample(click_hist_list, n_items) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(click_hist_list))]  <span class="comment"># 这里的负采样是全局负采样，没有剔除ground truth标签</span></span><br><span class="line">        hist_list = []</span><br><span class="line">        cate_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(click_hist_list)):    <span class="comment"># 前 n-2 条交互记录做训练集, 第 n-1 条作为验证集标签, 第 n 条作为测试集标签</span></span><br><span class="line">            hist_list.append(click_hist_list[i - <span class="number">1</span>])</span><br><span class="line">            cate_list.append(cate_hist_list[i - <span class="number">1</span>])</span><br><span class="line">            hist_list_pad = hist_list + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(hist_list))</span><br><span class="line">            cate_list_pad = cate_list + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(cate_list))</span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(click_hist_list) - <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># 用最后一位标记正负样本，1是正样本，0是负样本</span></span><br><span class="line">                test_data.append([user_id, hist_list_pad, cate_list_pad, click_hist_list[i], cate_hist_list[i], <span class="number">1</span>])</span><br><span class="line">                test_data.append([user_id, hist_list_pad, cate_list_pad, neg_list[i], item2cate_dict[neg_list[i]], <span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(click_hist_list) - <span class="number">2</span>:</span><br><span class="line">                val_data.append([user_id, hist_list_pad, cate_list_pad, click_hist_list[i], cate_hist_list[i], <span class="number">1</span>])</span><br><span class="line">                val_data.append([user_id, hist_list_pad, cate_list_pad, neg_list[i], item2cate_dict[neg_list[i]], <span class="number">0</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                train_data.append([user_id, hist_list_pad, cate_list_pad, click_hist_list[i], cate_hist_list[i], <span class="number">1</span>])</span><br><span class="line">                train_data.append([user_id, hist_list_pad, cate_list_pad, neg_list[i], item2cate_dict[neg_list[i]], <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># shuffle</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.shuffle(train_data)</span><br><span class="line">        random.shuffle(val_data)</span><br><span class="line">        random.shuffle(test_data)</span><br><span class="line"></span><br><span class="line">    col_name = [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;history_item&#x27;</span>, <span class="string">&#x27;history_cate&#x27;</span>, <span class="string">&#x27;target_item&#x27;</span>, <span class="string">&#x27;target_cate&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    train = pd.DataFrame(train_data, columns=col_name)</span><br><span class="line">    val = pd.DataFrame(val_data, columns=col_name)</span><br><span class="line">    test = pd.DataFrame(test_data, columns=col_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train, val, test</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;构建完序列特征后：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620014907354.png" alt="image-20220620014907354"></p>
<p><strong>让模型明白如何处理每一类特征</strong></p>
<p>&emsp;&emsp;在DIN模型中，我们讲使用了两种类别的特征，分别是类别特征和序列特征。对于类别特征，我们希望模型将其输入Embedding层，而对于序列特征，我们不仅希望模型将其输入Embedding层，还需要计算target-attention分数，所以需要指定DataFrame中每一列的含义，让模型能够正确处理。</p>
<p>&emsp;&emsp;在这个案例中，因为我们使用user_id,item_id和item_cate这三个类别特征，使用用户的item_id和cate的历史序列作为序列特征。在torch-rechub我们只需要调用DenseFeature, SparseFeature, SequenceFeature这三个类，就能自动正确处理每一类特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.basic.features <span class="keyword">import</span> DenseFeature, SparseFeature, SequenceFeature</span><br><span class="line"></span><br><span class="line">n_users, n_items, n_cates = data[<span class="string">&quot;user_id&quot;</span>].<span class="built_in">max</span>(), data[<span class="string">&quot;item_id&quot;</span>].<span class="built_in">max</span>(), data[<span class="string">&quot;cate_id&quot;</span>].<span class="built_in">max</span>()</span><br><span class="line"><span class="comment"># 这里指定每一列特征的处理方式，对于sparsefeature，需要输入embedding层，所以需要指定特征空间大小和输出的维度</span></span><br><span class="line">features = [SparseFeature(<span class="string">&quot;target_item&quot;</span>, vocab_size=n_items + <span class="number">2</span>, embed_dim=<span class="number">8</span>),	<span class="comment"># +2 ？？</span></span><br><span class="line">            SparseFeature(<span class="string">&quot;target_cate&quot;</span>, vocab_size=n_cates + <span class="number">2</span>, embed_dim=<span class="number">8</span>),</span><br><span class="line">            SparseFeature(<span class="string">&quot;user_id&quot;</span>, vocab_size=n_users + <span class="number">2</span>, embed_dim=<span class="number">8</span>)]</span><br><span class="line">target_features = features</span><br><span class="line"><span class="comment"># 对于序列特征，除了需要和类别特征一样处理意外，item序列和候选item应该属于同一个空间，我们希望模型共享它们的embedding，所以可以通过shared_with参数指定</span></span><br><span class="line">history_features = [</span><br><span class="line">    SequenceFeature(<span class="string">&quot;history_item&quot;</span>, vocab_size=n_items + <span class="number">2</span>, embed_dim=<span class="number">8</span>, pooling=<span class="string">&quot;concat&quot;</span>, shared_with=<span class="string">&quot;target_item&quot;</span>),</span><br><span class="line">    SequenceFeature(<span class="string">&quot;history_cate&quot;</span>, vocab_size=n_cates + <span class="number">2</span>, embed_dim=<span class="number">8</span>, pooling=<span class="string">&quot;concat&quot;</span>, shared_with=<span class="string">&quot;target_cate&quot;</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上面 + 2 的原因是：这里建立 embedding 蹭，需要按照特征表大小建立查表，用 max 获得被 labelEncoder 后的最大值， +的第一个 1 是因为把 0 作为 mask 了，+的第二个 1 是空出来以为冗余，这是编程呢个的习惯，也可以不加。</p>
<h2 id="定义数据集"><a href="#定义数据集" class="headerlink" title="定义数据集"></a>定义数据集</h2><p>&emsp;&emsp;在上述步骤中，我们制定了每一列的数据如何处理、数据维度、embed后的维度，目的就是在构建模型中，让模型知道每一层的参数。接下来我们生成训练数据，用于训练，一般情况下，我们只需要定义一个字典装入每一列特征即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.utils.data <span class="keyword">import</span> df_to_dict, DataGenerator</span><br><span class="line"><span class="comment"># 指定label，生成模型的输入，这一步是转换为字典结构</span></span><br><span class="line">train = df_to_dict(train)</span><br><span class="line">val = df_to_dict(val)</span><br><span class="line">test = df_to_dict(test)</span><br><span class="line"></span><br><span class="line">train_y, val_y, test_y = train[<span class="string">&quot;label&quot;</span>], val[<span class="string">&quot;label&quot;</span>], test[<span class="string">&quot;label&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> train[<span class="string">&quot;label&quot;</span>]</span><br><span class="line"><span class="keyword">del</span> val[<span class="string">&quot;label&quot;</span>]</span><br><span class="line"><span class="keyword">del</span> test[<span class="string">&quot;label&quot;</span>]</span><br><span class="line">train_x, val_x, test_x = train, val, test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后查看一次输入模型的数据格式</span></span><br><span class="line">train_x</span><br></pre></td></tr></table></figure>
<h2 id="定义-DataGenerator-Dataset-Dataloader-1"><a href="#定义-DataGenerator-Dataset-Dataloader-1" class="headerlink" title="定义 DataGenerator (Dataset + Dataloader)"></a>定义 DataGenerator (Dataset + Dataloader)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建dataloader，指定模型读取数据的方式，和区分验证集测试集、指定batch大小</span></span><br><span class="line">dg = DataGenerator(train_x, train_y)</span><br><span class="line">train_dataloader, val_dataloader, test_dataloader = dg.generate_dataloader(x_val=val_x, y_val=val_y, x_test=test_x, y_test=test_y, batch_size=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.models.ranking <span class="keyword">import</span> DIN</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型，模型的参数需要我们之前的feature类，用于构建模型的输入层，mlp指定模型后续DNN的结构，attention_mlp指定attention层的结构</span></span><br><span class="line">model = DIN(features=features, history_features=history_features, target_features=target_features, mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>]&#125;, attention_mlp_params=&#123;<span class="string">&quot;dims&quot;</span>: [<span class="number">256</span>, <span class="number">128</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;我们进入 DIN 模型看一看~</p>
<h2 id="定义训练器"><a href="#定义训练器" class="headerlink" title="定义训练器"></a>定义训练器</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_rechub.trainers <span class="keyword">import</span> CTRTrainer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练，需要学习率、设备等一般的参数，此外我们还支持earlystoping策略，及时发现过拟合</span></span><br><span class="line">ctr_trainer = CTRTrainer(model, optimizer_params=&#123;<span class="string">&quot;lr&quot;</span>: <span class="number">1e-3</span>, <span class="string">&quot;weight_decay&quot;</span>: <span class="number">1e-3</span>&#125;, n_epoch=<span class="number">3</span>, earlystop_patience=<span class="number">4</span>, device=<span class="string">&#x27;cpu&#x27;</span>, model_path=<span class="string">&#x27;./&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="训练和评估-1"><a href="#训练和评估-1" class="headerlink" title="训练和评估"></a>训练和评估</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">ctr_trainer.fit(train_dataloader, val_dataloader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看在测试集上的性能</span></span><br><span class="line">auc = ctr_trainer.evaluate(ctr_trainer.model, test_dataloader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;test auc: <span class="subst">&#123;auc&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;训练比较慢，只跑了一个 epoch：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220620022342747.png" alt="image-20220620022342747"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&emsp;&emsp;以 DeepFM 为切入点，学习了 FM、POLY2、FNN、PNN、Wide&amp;Deep 以及 DeepFM 模型，对这几个模型的发展脉络有了比较清晰的认识。用 Torch-RecHub 实现 DeepFM 也非常容易。DIN 是本次学习中学习的第一个序列模型，只做了 target-attention，放在现在来看 DIN 还处于序列建模比较萌芽的阶段。</p>
<p>&emsp;&emsp;Task2，以两个精排模型为线索，学到了很多！期待下一节的召回模型。</p>
<p>&emsp;&emsp;虽然忙面试，也要坚持打卡！</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1]《深度学习推荐系统》王喆</p>
<p>[2] <a href="http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf">FM (cmu.edu)</a></p>
<p>[3] <a href="https://datawhalechina.github.io/fun-rec/#/ch02/ch2.2/ch2.2.3/DeepFM">https://datawhalechina.github.io/fun-rec/#/ch02/ch2.2/ch2.2.3/DeepFM</a></p>
<p>[4] <a href="https://zhuanlan.zhihu.com/p/343174108">https://zhuanlan.zhihu.com/p/343174108</a></p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>Datawhale组队学习</tag>
        <tag>RecHub</tag>
      </tags>
  </entry>
  <entry>
    <title>Recbole避坑手册</title>
    <url>/2021/12/05/Recbole%E9%81%BF%E5%9D%91%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[<p>RecBole是个非常好的开源库，这几天做评测的时候用上了，奈何本人能力有限，遇到了非常多bug（可能是自己行为造成的），简单记录一下。可以参考这个：<a href="https://blog.csdn.net/turinger_2000/category_10624007.html">RecBole小白入门系列_Turinger_2000的博客-CSDN博客</a></p>
<p><strong>使用方法</strong>就是：<a href="https://github.com/RUCAIBox/RecBole">RUCAIBox/RecBole (github.com)</a>，下载下来unzip或者clone到设备上。然后再RecBole主目录下编写一个test.yaml文件记录一些配置，再运行run_recbole.py就可以。test.yaml大概要设置4类东西：dateset setting, model setting, train setting, evaluate setting.</p>
<p>整个项目文件如下，几个比较重要的文件夹和文件标出来了，后面会说到。</p>
<span id="more"></span>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205143321872.png" alt="image-20211205143321872"></p>
<p>接下来以一个<strong>用自己数据集跑SASRec模型</strong>的例子说明如何使用RecBole。</p>
<h2 id="配置test-yaml文件"><a href="#配置test-yaml文件" class="headerlink" title="配置test.yaml文件"></a>配置test.yaml文件</h2><h3 id="构造数据集——data-setting"><a href="#构造数据集——data-setting" class="headerlink" title="构造数据集——data setting"></a>构造数据集——data setting</h3><p>如果想跑自己的实验，那么很重要的一件事就是构造自己的数据集，recbole要求个人首先构建可以处理的原子文件，然后就可以传给模型处理了。详细见：<a href="https://recbole.io/cn/data_flow.html">数据流 | 伯乐 (recbole.io)</a></p>
<p>根据<a href="https://recbole.io/cn/atomic_files.html">原子文件 | 伯乐 (recbole.io)</a>，Sequential模型只需要.inter的原子文件，如下图：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205143831872.png" alt="image-20211205143831872"></p>
<p>虽然不知道.inter 是什么文件，但是可以看以下模型本身给的数据集以及处理好的原子文件模仿着构造。数据集保存在RecBole/dataset/ml-100k下（以ml-100k数据集为例），找到ml-100k.inter，用记事本打开格式如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205144208594.png" alt="image-20211205144208594"></p>
<p>BPR和CF等general model可能会用到rating（用户评分），timestamp。但是Sequential model一般只需要timestamp把点击行为构成对应用户的序列就行，想跑的SASRec论文附的代码里，对数据集的处理是：只保留user_id和item_id两列，按点击顺序存储。我的数据集book长这个样子：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205144708967.png" alt="image-20211205144708967"></p>
<p>只有两列特征user_id和item_id，看起来好像很接近，但是踩坑穿越回来的我可以告诉你，这里必须得有timestamp一列，recbole就是这么处理sequential model的数据集的，没有办法。那添加什么样的timestamp呢？book数据集是按点击顺序存储的（user_id已经重新从0开始标号），所以其实只要加个递增的timestamp就行了，这里用pandas简单处理下多加一列就行。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205145228872.png" alt="image-20211205145228872"></p>
<p>注意，这里用pandas处理的时候，顺便把列名改了。user_id和item_id后面加上”:token”，timestamp后面加上”:float”。RecBole要求这么做，后面也会说到。这样处理好文件后，pandas输出的一般是csv，重命名的时候要改成.inter后缀。然后在dataset下新建一个文件夹，起名为你的dataset名称xxx（可以自己起，这个很重要），然后.inter文件也要命名为xxx.inter，如图所示：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205145659369.png" alt="image-20211205145659369"></p>
<p>然后我们在RecBole主目录下新建一个test.yaml文件，在里面输入：（暂时不明白没事，抄下来就行）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dataset config</span></span><br><span class="line">field_separator: <span class="string">&quot;,&quot;</span>  <span class="comment">#指定数据集field的分隔符</span></span><br><span class="line">seq_separator: <span class="string">&quot; &quot;</span>   <span class="comment">#指定数据集中token_seq或者float_seq域里的分隔符</span></span><br><span class="line">USER_ID_FIELD: user_id <span class="comment">#指定用户id域</span></span><br><span class="line">ITEM_ID_FIELD: item_id <span class="comment">#指定物品id域</span></span><br><span class="line">TIME_FIELD: timestamp  <span class="comment">#指定时间域</span></span><br><span class="line">NEG_PREFIX: neg_   <span class="comment">#指定负采样前缀</span></span><br><span class="line"><span class="comment">#指定从什么文件里读什么列，这里就是从book.inter里面读取user_id, item_id,timestamp这四列</span></span><br><span class="line">load_col:</span><br><span class="line">  inter: [user_id, item_id, timestamp]</span><br></pre></td></tr></table></figure>
<p>需要注意前两条separator，csv文件的话默认分隔符是”,”，还有最后一行local:这里按照数据集的列指定就行，到此数据集基本构造好了。</p>
<h3 id="用Sequential-model类跑模型——model-setting"><a href="#用Sequential-model类跑模型——model-setting" class="headerlink" title="用Sequential model类跑模型——model setting"></a>用Sequential model类跑模型——model setting</h3><p>以SASRec为例，想跑一个模型，如何看这个模型需要的参数？到 RecBole/recbole/properties/model 底下找到对应模型的yaml文件，打开以后大概长这样。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205150832821.png" alt="image-20211205150832821"></p>
<p>这里包含了模型需要的参数，每次调用都到这里改很麻烦，所以recbole可以实现用test.yaml的设置覆盖具体模型的设置，所以只要在test.yaml（主目录下的那个配置文件）里改我们添加，并做一点修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_layers: <span class="number">2</span></span><br><span class="line">n_heads: <span class="number">2</span></span><br><span class="line">hidden_size: <span class="number">100</span></span><br><span class="line">inner_size: <span class="number">256</span></span><br><span class="line">hidden_dropout_prob: <span class="number">0.5</span></span><br><span class="line">attn_dropout_prob: <span class="number">0.5</span></span><br><span class="line">hidden_act: <span class="string">&#x27;gelu&#x27;</span></span><br><span class="line">layer_norm_eps: <span class="number">1e-12</span></span><br><span class="line">initializer_range: <span class="number">0.02</span></span><br><span class="line">loss_type: <span class="string">&#x27;CE&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="训练设置——train-setting"><a href="#训练设置——train-setting" class="headerlink" title="训练设置——train setting"></a>训练设置——train setting</h3><p>通用的训练设置也要写到test.yaml中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># training settings</span><br><span class="line">epochs: 500  				#训练的最大轮数</span><br><span class="line">train_batch_size: 20 #2048 	#训练的batch_size</span><br><span class="line">learner: adam 				#使用的pytorch内置优化器</span><br><span class="line">learning_rate: 0.001 		#学习率</span><br><span class="line">training_neg_sample_num: 0 	#负采样数目</span><br></pre></td></tr></table></figure>
<h3 id="评估设置——evaluate-setting"><a href="#评估设置——evaluate-setting" class="headerlink" title="评估设置——evaluate setting"></a>评估设置——evaluate setting</h3><p>通用的评估设置也要写道test.yaml中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># evalution settings</span><br><span class="line">eval_step: 1 				#每次训练后做evalaution的次数</span><br><span class="line">eval_setting: RO_RS,full 	#对数据随机重排，设置按比例划分数据集，且使用全排序</span><br><span class="line">group_by_user: True 		#是否将一个user的记录划到一个组里，当eval_setting使用RO_RS的时候该项必须是True</span><br><span class="line">split_ratio: [0.8,0.1,0.1] 	#切分比例</span><br><span class="line">metrics: [&quot;Recall&quot;, &quot;MRR&quot;,&quot;NDCG&quot;,&quot;Hit&quot;,&quot;Precision&quot;] #评测标准</span><br><span class="line">topk: [10] #评测标准使用topk，设置成10评测标准就是[&quot;Recall@10&quot;, &quot;MRR@10&quot;, &quot;NDCG@10&quot;, &quot;Hit@10&quot;, &quot;Precision@10&quot;]</span><br><span class="line">valid_metric: MRR@10 		#选取哪个评测标准作为作为提前停止训练的标准</span><br><span class="line">stopping_step: 10 			#控制训练收敛的步骤数，在该步骤数内若选取的评测标准没有什么变化，就可以提前停止了</span><br><span class="line">eval_batch_size: 4096 		#评测的batch_size</span><br></pre></td></tr></table></figure>
<p>recbole实现了earlystopping早停策略，可以设置控制收敛的步骤数。</p>
<p>eval_setting，可以设置不同的数据切分方式，具体可见<a href="https://recbole.io/cn/evaluation.html">评测 | 伯乐 (recbole.io)</a>，大致如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205163312199.png" alt="image-20211205163312199"></p>
<h2 id="run！跑起来吧，baseline！"><a href="#run！跑起来吧，baseline！" class="headerlink" title="run！跑起来吧，baseline！"></a>run！跑起来吧，baseline！</h2><p>我们cd到RecBole目录下，此时数据集已经准备好，test.yaml文件也已经写好，可以开始跑实验了，用以下指令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python run_recbole.py --model=SASRec --dataset=book ----config_files=test.yaml</span><br></pre></td></tr></table></figure>
<p>如果看不懂参数的话，点进run_recbole.py看一下就明白了！</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>RecBole是个集成度很高，也比较方便用于复现一些基本推荐模型的开源库（在此致敬中国人民大学AI BOX小组！），需要用recbole跑自己的baseline时，只需要三步：</p>
<ol>
<li>构造满足原子文件的数据集</li>
<li>写好.yaml格式的配置文件</li>
<li>run！</li>
</ol>
<h2 id="记录一些遇到的坑"><a href="#记录一些遇到的坑" class="headerlink" title="记录一些遇到的坑"></a>记录一些遇到的坑</h2><h3 id="1-neg-sampling-不能使用-‘CE’-loss"><a href="#1-neg-sampling-不能使用-‘CE’-loss" class="headerlink" title="1. neg_sampling 不能使用 ‘CE’ loss"></a>1. neg_sampling 不能使用 ‘CE’ loss</h3><p>在配置文件中加上一行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">neg_sampling: (注意这里冒号后面有个空格)</span><br></pre></td></tr></table></figure>
<h3 id="2-使用不上gpu"><a href="#2-使用不上gpu" class="headerlink" title="2.使用不上gpu"></a>2.使用不上gpu</h3><p><strong>问题描述：</strong>程序可以跑起来，但是nvtop看不到它在gpu上运行。并且无论如何修改配置文件都没有用。非常奇怪的问题。检查torch.cuda.is_available()的时候发现，居然输出False，所以原因是用不了CUDA。</p>
<p><strong>问题原因</strong>：用不了CUDA。</p>
<p>用conda list检查安装的库时发现，默认安装的torch是cpu的</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205140943532.png" alt="image-20211205140943532"></p>
<blockquote>
<p>左边是默认安装的，想了一宿也没想明白为什么默认安装cpu版本</p>
</blockquote>
<p>卸载cpu版本再安装cuda版本有点麻烦，所以我直接新建了一个环境，并且自带torch=1.10，然后再根据依赖安装，使用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install -r requirement.txt</span><br></pre></td></tr></table></figure>
<p>这里不用担心torch会被覆盖，因为torch版本大于1.17就会自动跳过了。</p>
<p>然后总可以跑了吧，运行下面指令（用RecBole需要这样输）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python run_recbole.py --model=BERT4Rec --dataset=book ----config_files=bert4rec.yaml</span><br></pre></td></tr></table></figure>
<p>按理说此时torch是建环境时我安装的cuda版本，一定不是cpu版本，然而每次运行仍然是device=cpu。实在让人崩溃！</p>
<p><strong>解决方案</strong>：</p>
<p>最后debug多轮，寻找到的解决方案是：</p>
<ul>
<li>在run_recbole.py里import torch，并且打印torch.cuda.is_available()</li>
</ul>
<p>这样做合理的<strong>可能的原因</strong>：</p>
<p>可能项目某个地方import torch，import进来的torch是cpu版本的，所以提前import可以解决。</p>
]]></content>
      <categories>
        <category>代码阅读</category>
      </categories>
      <tags>
        <tag>RecBole</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Self-Attentive Sequential Recommendation》</title>
    <url>/2021/11/03/SASRec/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117230533153.png" alt="image-20211117230533153"></p>
<hr>
<p>原paper：<a href="https://ieeexplore.ieee.org/document/8594844">https://ieeexplore.ieee.org/document/8594844</a></p>
<p>源码解读：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec</a></p>
<hr>
<p>中译：自注意序列推荐</p>
<p>总结：比较早使用self-attention的序列推荐模型</p>
<hr>
<span id="more"></span>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>question作者想解决什么问题？  </li>
</ul>
<p>序列动态是许多当代推荐系统的一个重要特征，它试图根据用户最近执行的操作来捕捉用户活动的“上下文“。RNN模型可以在稠密数据集上捕捉长期语义。（马尔科夫链）MC模型可以在稀疏数据集上仅根据最近几次action做出预测。本文想平衡这两个目标：在稀疏和稠密数据集上做到捕捉长期语义、依赖较少的action做预测。</p>
<ul>
<li>method作者通过什么理论/模型来解决这个问题？</li>
</ul>
<p>本文提出了一个基于self-attention的序列模型（SASRec），在每个时间步寻找与用户历史最相关的物品作为next item的预测。</p>
<ul>
<li>answer作者给出的答案是什么？</li>
</ul>
<p>在稀疏和稠密数据集上，与MC/CNN/RNN方法相比都取得了SOTA效果。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>why作者为什么研究这个课题？    </li>
</ul>
<p>MC方法模型简单，但因为它的强假设（当前预测仅取决于最近n次）使得它在稀疏数据上表现好，但是不能捕捉更复杂的动态转换。RNN方法需要稠密数据，并且计算复杂。最近出现新的序列模型Transformer，它是基于self-attention的，效率高并且可以捕获句子中单词的句法和语义模式。受self-attention方法启发，应用到序列推荐上。</p>
<ul>
<li>how当前研究到了哪一阶段？ </li>
</ul>
<p>第一个将transformer里的self-attention应用到了序列推荐上。</p>
<ul>
<li>what作者基于什么样的假设（看不懂最后去查）？</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>优点 <ul>
<li>SASRec模型建模了整个序列，自适应地考虑items来预测</li>
<li>在dense和sparse的数据集上效果都很好</li>
<li>比CNN/RNN方法快了一个数量级</li>
</ul>
</li>
<li>缺点</li>
<li>展望<ul>
<li>引进更多上下文信息，比如等待时间、行为类型、位置、设备等。</li>
<li>探索处理超长序列（如clicks）的方法</li>
</ul>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 （都开源）<ul>
<li>Amazon</li>
<li>Steam 作者爬的，开源了</li>
<li>Movielens</li>
</ul>
</li>
<li>重要指标 <ul>
<li>Hit@10</li>
<li>NDCG@10</li>
</ul>
</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>这部分主要参考了知乎<a href="[推荐算法炼丹笔记：序列化推荐算法SASRec - 知乎 (zhihu.com">[1]</a>](<a href="https://zhuanlan.zhihu.com/p/277660092?utm_source=qq">https://zhuanlan.zhihu.com/p/277660092?utm_source=qq</a>))</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211115114043810.png" alt="image-20211115114043810"></p>
<p><strong>1.Embedding层</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-6127cd6bfcdc00f007ba287f11c1f55f_720w.jpg" alt="img"></p>
<p><strong>A. Positional Embedding</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-558fcc53330d91271fc2850a3998e704_720w.jpg" alt="img"></p>
<p><strong>2.Self-Attention Block</strong></p>
<p><strong>A.Self-Attention Layer</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-4ad7a98ce285113021eade4349199c5f_720w.jpg" alt="img"></p>
<p><strong>C.Point-Wise Feed-Forward Network</strong>: 尽管self-attention能将之前item的emebdding使用自适应的权重进行集成，但仍然是一个先线性模型,为了加入非线性能力, 我们使用两层的DDN,</p>
<p><img src="https://pic2.zhimg.com/80/v2-bdfa1cac41b3f4aa676e81d54a72671d_720w.jpg" alt="img"></p>
<p><strong>3.Stacking Self-Attention Blocks</strong></p>
<p>在第一个self-attention block之后,学习item的迁移可以学习更加复杂的item迁移,所以我们对self-attention block进行stacking,第b(b&gt;1)的block可以用下面的形式进行定义：</p>
<p><img src="https://pic3.zhimg.com/80/v2-cdc40ee5705587460d39e19649625942_720w.jpg" alt="img"></p>
<p><strong>4.Prediction Layer</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-873157dd4336dcbbd818227c7ced3f25_720w.jpg" alt="img"></p>
<p>使用同质(homogeneous)商品embedding的一个潜在问题是，它们的内部产品不能代表不对称的商品转换。然而，我们的模型没有这个问题，因为它学习了一个非线性变换。例如，前馈网络可以很容易地实现同项嵌入的不对称性,<strong>经验上使用共享的商品embedding也可以大大提升模型的效果;</strong></p>
<p><strong>显示的用户建模</strong>：为了提供个性化的推荐,现有的方法常用两种方法,(1).学习显示的用户embedding表示用户的喜好;(2).考虑之前的行为并且引入隐式的用户embedding。此处使用并没有带来提升。</p>
<p><strong>5.网络训练</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-684099a2a86837c0b3ad701ea2169710_720w.jpg" alt="img"></p>
<p><strong>6.方案复杂度分析</strong></p>
<p><strong>a. 空间复杂度</strong></p>
<p>模型中学习的参数来自于self-attention.ffn以及layer normalization的参数,总的参数为:</p>
<p><img src="https://pic4.zhimg.com/80/v2-3d4d8db1c48964728a0c6830ecc4a71b_720w.jpg" alt="img"></p>
<p><strong>b. 时间复杂度</strong></p>
<p>我们模型的计算复杂度主要在于self-attention layer和FFN网络,</p>
<p><img src="https://pic1.zhimg.com/80/v2-1cd0b2b09e9bc3fba57281ab76f2d478_720w.jpg" alt="img"></p>
<p>里面最耗时间的还是self-attention layer, 不过这个可以进行并行化。</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><p>该次实验主要为了验证下面的四个问题：</p>
<ol>
<li>是否SASRec比现有最好的模型(CNN/RNN)要好？</li>
<li>在SASRec框架中不同的成份的影响怎么样？</li>
<li>SASRec的训练效率和可扩展性怎么样？</li>
<li>attention的权重是否可以学习得到关于位置和商品属性的有意义的模式?</li>
</ol>
<p><strong>1. 推荐效果比较</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-e789c62c7c2e998f0713341ebc43155f_720w.jpg" alt="img"></p>
<ul>
<li>SASRec在稀疏的和dense的数据集合熵比所有的baseline都要好, 获得了6.9%的Hit Rate提升以及9.6%的NDCG提升；</li>
</ul>
<p><strong>2. SASRec框架中不同成份的影响</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-155ea54d12922a3d1aafcece005b5731_720w.jpg" alt="img"></p>
<ul>
<li>删除PE: 删除位置embedding ,在稀疏的数据集上,删除PE效果变好,但是在稠密的数据集上,删除PE的效果变差了。</li>
<li>不共享IE(Item Embedding): 使用共享的item embedding比不使用要好很多;</li>
<li>删除RC(Residual Connection):不实用残差连接,性能会变差非常多;</li>
<li>删除Dropout: dropout可以帮助模型,尤其是在稀疏的数据集上,Dropout的作用更加明显;</li>
<li>blocks的个数：没有block的时候,效果最差,在dense数据集上,相比稀疏数据多一些block的效果好一些;</li>
<li>Multi-head:在我们数据集上,single-head效果最好.</li>
</ul>
<p><strong>3. SASRec的训练效率和可扩展性</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-b4a0692c6cf9b0a335dae79eba2ed723_720w.jpg" alt="img"></p>
<p><img src="https://pic1.zhimg.com/80/v2-1a7f5f0f47c7ada0e2ccd22b23078584_720w.jpg" alt="img"></p>
<ul>
<li>SASRec是最快的;</li>
<li>序列长度可以扩展至500左右.</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/277660092?utm_source=qq">[1]推荐算法炼丹笔记：序列化推荐算法SASRec</a></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>序列推荐</tag>
        <tag>SASRec</tag>
        <tag>自注意力</tag>
      </tags>
  </entry>
  <entry>
    <title>SASRec代码笔记</title>
    <url>/2021/11/06/SASRec%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<hr>
<p>完整的代码注释：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec</a></p>
<p>论文笔记：<a href="https://guadzilla.github.io/2021/11/03/SASRec/">https://guadzilla.github.io/2021/11/03/SASRec/</a></p>
<hr>
<h2 id="collections-defaultdict-list"><a href="#collections-defaultdict-list" class="headerlink" title="collections.defaultdict(list)"></a>collections.defaultdict(list)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">collections</span>.<span class="title">defaultdict</span>(<span class="params">default_factory=<span class="literal">None</span>, /[, ...]</span>)</span></span><br></pre></td></tr></table></figure>
<p>返回一个新的类似字典的对象。<code>defaultdict</code> 是内置 <code>dict</code>类的子类。 它重载了一个方法并添加了一个可写的实例变量。</p>
<p>本对象包含一个名为 <code>default_factory</code> 的属性，构造时，第一个参数用于为该属性提供初始值，默认为 None。所有其他参数（包括关键字参数）都相当于传递给 dict 的构造函数。</p>
<p>使用<code>defulydict(list)</code>实例化对象时， <code>default_factory=list</code>，可以很轻松地<strong>将（键-值对组成的）序列转换为（键-列表组成的）字典</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = [(<span class="string">&#x27;yellow&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;blue&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;yellow&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;blue&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;red&#x27;</span>, <span class="number">1</span>)]</span><br><span class="line">d = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> s:</span><br><span class="line">    d[k].append(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>(d.items())</span><br><span class="line"><span class="comment"># 输出：[(&#x27;blue&#x27;, [2, 4]), (&#x27;red&#x27;, [1]), (&#x27;yellow&#x27;, [1, 3])]</span></span><br></pre></td></tr></table></figure>
<p>当字典中没有的键第一次出现时，python自动为其返回一个空列表，list.append()会将值添加进新列表；再次遇到相同的键时，list.append()将其它值再添加进该列表。</p>
<span id="more"></span>
<h2 id="Python自定义多线程"><a href="#Python自定义多线程" class="headerlink" title="Python自定义多线程"></a>Python自定义多线程</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_neq</span>(<span class="params">l, r, s</span>):</span></span><br><span class="line">    t = np.random.randint(l, r)</span><br><span class="line">    <span class="keyword">while</span> t <span class="keyword">in</span> s:</span><br><span class="line">        t = np.random.randint(l, r)</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_function</span>(<span class="params">user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>():</span></span><br><span class="line"></span><br><span class="line">        user = np.random.randint(<span class="number">1</span>, usernum + <span class="number">1</span>)    <span class="comment"># 随机采样user id，注意是从1开始的</span></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(user_train[user]) &lt;= <span class="number">1</span>: user = np.random.randint(<span class="number">1</span>, usernum + <span class="number">1</span>)  <span class="comment"># 长度小于1的训练集不要</span></span><br><span class="line"></span><br><span class="line">        seq = np.zeros([maxlen], dtype=np.int32)    <span class="comment"># seq序列，长度固定为maxlen，用0在前面padding补上长度，例：[0,0,...,0,23,15,2,6]</span></span><br><span class="line">        pos = np.zeros([maxlen], dtype=np.int32)</span><br><span class="line">        neg = np.zeros([maxlen], dtype=np.int32)</span><br><span class="line">        nxt = user_train[user][-<span class="number">1</span>]  <span class="comment"># user_train的最后一个item取为nxt</span></span><br><span class="line">        idx = maxlen - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        ts = <span class="built_in">set</span>(user_train[user])  <span class="comment"># ts为序列的item集合</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(user_train[user][:-<span class="number">1</span>]):   <span class="comment"># 从后往前遍历user_train,idx为当前要填充的下标</span></span><br><span class="line">            seq[idx] = i</span><br><span class="line">            pos[idx] = nxt</span><br><span class="line">            <span class="keyword">if</span> nxt != <span class="number">0</span>: neg[idx] = random_neq(<span class="number">1</span>, itemnum + <span class="number">1</span>, ts)  <span class="comment"># 生成的负样本不能取该序列item集合里的item</span></span><br><span class="line">            nxt = i</span><br><span class="line">            idx -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> idx == -<span class="number">1</span>: <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (user, seq, pos, neg)    <span class="comment"># 返回一次采样，(用户id,训练序列，label序列，负样本序列)</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(SEED)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:     <span class="comment"># 采样一个batch_size大小的数据样本，打包成一个batch，放到线程队列里</span></span><br><span class="line">        one_batch = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            one_batch.append(sample())</span><br><span class="line"></span><br><span class="line">        result_queue.put(<span class="built_in">zip</span>(*one_batch))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WarpSampler</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, User, usernum, itemnum, batch_size=<span class="number">64</span>, maxlen=<span class="number">10</span>, n_workers=<span class="number">1</span></span>):</span></span><br><span class="line">        self.result_queue = Queue(maxsize=n_workers * <span class="number">10</span>)   <span class="comment"># 长度为10的线程队列</span></span><br><span class="line">        self.processors = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_workers):</span><br><span class="line">            self.processors.append(     <span class="comment"># Process()进程的类, target：要调用的对象即sampler_function，args：调用该对象要接受的参数</span></span><br><span class="line">                Process(target=sample_function, args=(User,</span><br><span class="line">                                                      usernum,</span><br><span class="line">                                                      itemnum,</span><br><span class="line">                                                      batch_size,</span><br><span class="line">                                                      maxlen,</span><br><span class="line">                                                      self.result_queue,</span><br><span class="line">                                                      np.random.randint(<span class="number">2e9</span>)</span><br><span class="line">                                                      )))</span><br><span class="line">            self.processors[-<span class="number">1</span>].daemon = <span class="literal">True</span></span><br><span class="line">            self.processors[-<span class="number">1</span>].start()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_batch</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.result_queue.get()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.processors:</span><br><span class="line">            p.terminate()</span><br><span class="line">            p.join()</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line"><span class="comment"># sampler是WarpSampler对象的实例，每次调用sampler.next_batch(),就返回一个batch的样本。</span></span><br><span class="line"><span class="comment"># 进一步解释：每次调用sampler.next_batch()就call其线程队列里的一个线程，每个线程用于返回一个batch的数据。</span></span><br><span class="line">sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen, n_workers=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="torch-tril"><a href="#torch-tril" class="headerlink" title="torch.tril()"></a>torch.tril()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.tril(<span class="built_in">input</span>, diagonal=<span class="number">0</span>, *, out=<span class="literal">None</span>) → Tensor</span><br><span class="line"><span class="comment"># 功能：返回下三角矩阵其余部分用out填充（默认为0）</span></span><br><span class="line"><span class="comment"># input：输入矩阵，二维tensor</span></span><br><span class="line"><span class="comment"># diagonal：表示对角线位置，diagonal=0为主对角线，diagonal=-1为主对角线往下1格，diagonal=1为主对角线往上1格</span></span><br><span class="line"><span class="comment"># out：表示填充，默认用out=None即0填充</span></span><br></pre></td></tr></table></figure>
<p>例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[-<span class="number">1.0813</span>, -<span class="number">0.8619</span>,  <span class="number">0.7105</span>],</span><br><span class="line">        [ <span class="number">0.0935</span>,  <span class="number">0.1380</span>,  <span class="number">2.2112</span>],</span><br><span class="line">        [-<span class="number">0.3409</span>, -<span class="number">0.9828</span>,  <span class="number">0.0289</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tril(a)</span><br><span class="line">tensor([[-<span class="number">1.0813</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0935</span>,  <span class="number">0.1380</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [-<span class="number">0.3409</span>, -<span class="number">0.9828</span>,  <span class="number">0.0289</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.randn(<span class="number">4</span>, <span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">tensor([[ <span class="number">1.2219</span>,  <span class="number">0.5653</span>, -<span class="number">0.2521</span>, -<span class="number">0.2345</span>,  <span class="number">1.2544</span>,  <span class="number">0.3461</span>],</span><br><span class="line">        [ <span class="number">0.4785</span>, -<span class="number">0.4477</span>,  <span class="number">0.6049</span>,  <span class="number">0.6368</span>,  <span class="number">0.8775</span>,  <span class="number">0.7145</span>],</span><br><span class="line">        [ <span class="number">1.1502</span>,  <span class="number">3.2716</span>, -<span class="number">1.1243</span>, -<span class="number">0.5413</span>,  <span class="number">0.3615</span>,  <span class="number">0.6864</span>],</span><br><span class="line">        [-<span class="number">0.0614</span>, -<span class="number">0.7344</span>, -<span class="number">1.3164</span>, -<span class="number">0.7648</span>, -<span class="number">1.4024</span>,  <span class="number">0.0978</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tril(b, diagonal=<span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">1.2219</span>,  <span class="number">0.5653</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.4785</span>, -<span class="number">0.4477</span>,  <span class="number">0.6049</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">1.1502</span>,  <span class="number">3.2716</span>, -<span class="number">1.1243</span>, -<span class="number">0.5413</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [-<span class="number">0.0614</span>, -<span class="number">0.7344</span>, -<span class="number">1.3164</span>, -<span class="number">0.7648</span>, -<span class="number">1.4024</span>,  <span class="number">0.0000</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tril(b, diagonal=-<span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.4785</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">1.1502</span>,  <span class="number">3.2716</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [-<span class="number">0.0614</span>, -<span class="number">0.7344</span>, -<span class="number">1.3164</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="Python中的-波浪线运算符"><a href="#Python中的-波浪线运算符" class="headerlink" title="Python中的 ~ 波浪线运算符"></a>Python中的 ~ 波浪线运算符</h2><p>~，用法只有一个那就是按位取反</p>
<p><a href="https://blog.csdn.net/lanchunhui/article/details/51746477"> Python 波浪线与补码_https://space.bilibili.com/59807853-CSDN博客_python 波浪线</a></p>
<h2 id="torch-nn-MultiAttention"><a href="#torch-nn-MultiAttention" class="headerlink" title="torch.nn.MultiAttention"></a>torch.nn.MultiAttention</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=<span class="number">0.0</span>, bias=<span class="literal">True</span>, add_bias_kv=<span class="literal">False</span>, add_zero_attn=<span class="literal">False</span>, kdim=<span class="literal">None</span>, vdim=<span class="literal">None</span>, batch_first=<span class="literal">False</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>):</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>对应公式：</p>
<script type="math/tex; mode=display">
Multihead(Q,K,V) = Concat(head_1,...,head_h)W^O    \\
where \quad head_i= Attention(QW^Q_i,KW^K_i,VW^V_i)</script><p>计算公式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">forward(query, key, value, key_padding_mask=<span class="literal">None</span>, need_weights=<span class="literal">True</span>, attn_mask=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>QKV比较常规，需要注意的是</p>
<ol>
<li>key_padding_mask参数，大小为（N，S），指定key中的哪些元素不做attention计算，即看作padding。注意，为True的位置不计算attention（是padding的地方不计算）</li>
<li>attn_mask参数，</li>
</ol>
<h2 id="torch-nn-BCEWithLogitsLoss"><a href="#torch-nn-BCEWithLogitsLoss" class="headerlink" title="torch.nn.BCEWithLogitsLoss()"></a>torch.nn.BCEWithLogitsLoss()</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">forward(self, input: Tensor, target: Tensor) -&gt; Tensor</span><br></pre></td></tr></table></figure>
<p>参数说明：</p>
<ul>
<li>input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits).</li>
<li>target: Tensor of the same shape as input with values between 0 and 1</li>
</ul>
<p>input：$x$        output：$y$</p>
<script type="math/tex; mode=display">
ℓ(x,y)=L={l_1,…,l_N}^T</script><script type="math/tex; mode=display">
l_n=−w_n[y_n·log\sigma(x_n)+(1−y_n)·log(1−\sigma(x_n))]</script><p>当 $y=1$ 时，$l_n=−log\sigma(x_n)$  ；当 $y=0$ 时，$l_n=−log(1-\sigma(x_n))$     。</p>
<p>论文里使用了一个全1的矩阵pos_labels，和一个全0的矩阵neg_labels。正例标签值都为1（正确的item，ground truth应该是概率为1），负例标签值都为0（错误的item，ground truth应该是概率为0）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), \</span><br><span class="line">torch.zeros(neg_logits.shape, device=args.device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(&quot;\neye ball check raw_logits:&quot;); print(pos_logits); print(neg_logits)</span></span><br><span class="line"><span class="comment"># check pos_logits &gt; 0, neg_logits &lt; 0</span></span><br><span class="line">adam_optimizer.zero_grad()</span><br><span class="line">indices = np.where(pos != <span class="number">0</span>)    <span class="comment"># 返回一个二维数组array， array[0]=[横坐标], array[1]=[纵坐标]</span></span><br><span class="line">loss = bce_criterion(pos_logits[indices], pos_labels[indices])  <span class="comment"># 使正例的得分尽量</span></span><br><span class="line">loss += bce_criterion(neg_logits[indices], neg_labels[indices])</span><br></pre></td></tr></table></figure>
<h2 id="torch-argsort"><a href="#torch-argsort" class="headerlink" title="torch.argsort()"></a>torch.argsort()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.argsort(<span class="built_in">input</span>, dim=-<span class="number">1</span>, descending=<span class="literal">False</span>) → LongTensor</span><br></pre></td></tr></table></figure>
<p>沿着指定dim从小到大（默认）排序元素，然后返回这些元素原来的下标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;t = torch.randint(<span class="number">1</span>,<span class="number">10</span>,(<span class="number">1</span>,<span class="number">5</span>))</span><br><span class="line">&gt;&gt;&gt;t</span><br><span class="line">tensor([[<span class="number">7</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">3</span>]])</span><br><span class="line">&gt;&gt;&gt;t.argsort()</span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">&gt;&gt;&gt;t.argsort().argsort()</span><br><span class="line">tensor([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两次argsort()可以返回每个元素的rank排名</span></span><br><span class="line"><span class="comment"># 解释：</span></span><br><span class="line"><span class="comment"># 把商品0,1,2,3,4按顺序摆好，他们的得分分别为[7,9,5,6,3]</span></span><br><span class="line"><span class="comment"># 对所有商品的得分从小到大排序（argsort()操作）</span></span><br><span class="line"><span class="comment"># 得到积分排名是[3,5,6,7,9]，积分排名对应的商品id是[4,2,3,0,1]（第一次argsort()的结果），每个商品id对应的下标就是他们的得分名次</span></span><br><span class="line"><span class="comment"># 例如商品4得分最高排在第一位，商品1得分最低排最后一位</span></span><br><span class="line"><span class="comment"># 然后我们想得到0,1,2,3,4顺序下的结果</span></span><br><span class="line"><span class="comment"># 所以对商品id排序，使得商品摆放顺序由[4,2,3,0,1]变为[0,1,2,3,4]，这里也是argsort()操作，因为0~4天然有顺序关系</span></span><br><span class="line"><span class="comment"># [4,2,3,0,1]变为[0,1,2,3,4]的同时，排名情况[0,1,2,3,4]也变成了[3,4,1,2,0]（第二次argsort()的结果）</span></span><br><span class="line"><span class="comment"># 即求得每个商品在原来顺序下的得分名次</span></span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/traditional/p/13702904.html">numpy中的argmax、argmin、argwhere、argsort、argpartition函数 - 古明地盆 - 博客园 (cnblogs.com)</a></p>
<h2 id="评价指标Hit-Ratio、NDCG-1"><a href="#评价指标Hit-Ratio、NDCG-1" class="headerlink" title="评价指标Hit Ratio、NDCG[1]"></a>评价指标Hit Ratio、NDCG<a href="https://dl.acm.org/doi/10.1145/2806416.2806504">[1]</a></h2><h3 id="Hit-Ratio"><a href="#Hit-Ratio" class="headerlink" title="Hit Ratio"></a>Hit Ratio</h3><p>Evaluation Metrics. Given a user, each algorithm produces a ranked list of items. To assess the ranked list with the ground-truth item set (GT), we adopt Hit Ratio (HR), which has been commonly used in top-N evaluation . If a test item appears in the recommended list, it is deemed a hit. HR is calculated as:</p>
<script type="math/tex; mode=display">
HR@K=\frac{Number\ of \  Hits@K}{|GT|}</script><h3 id="NDCG"><a href="#NDCG" class="headerlink" title="NDCG"></a>NDCG</h3><p>As the HR is recall-based metric, it does not reflect the accuracy of getting top ranks correct, which is crucial in many real-world applications. To address this, we also adopt Normalized Discounted Cumulative Gain (NDCG), which assigns higher importance to results at top ranks, scoring successively lower ranks with marginal fractional utility:</p>
<script type="math/tex; mode=display">
NDCG@K=Z_K\sum^K_{i=1}\frac{2^{r_i}-1}{log_2{(i+1)}}</script><p>where ZK is the normalizer to ensure the perfect ranking has a value of 1; ri is the graded relevance of item at position i. We use the simple binary relevance for our work: ri = 1 if the item is in the test set, and 0 otherwise. For both metrics, larger values indicate better performance. In the evaluation, we calculate both metrics for each user in the test set, and report the average score.</p>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># evaluate on test set</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">model, dataset, args</span>):</span></span><br><span class="line">    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)  <span class="comment"># deepcopy一份用于valid和test</span></span><br><span class="line"></span><br><span class="line">    NDCG = <span class="number">0.0</span></span><br><span class="line">    HT = <span class="number">0.0</span></span><br><span class="line">    valid_user = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> usernum &gt; <span class="number">10000</span>:  <span class="comment"># 用户数量大于10000就随机采10000</span></span><br><span class="line">        users = random.sample(<span class="built_in">range</span>(<span class="number">1</span>, usernum + <span class="number">1</span>), <span class="number">10000</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        users = <span class="built_in">range</span>(<span class="number">1</span>, usernum + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> users:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(train[u]) &lt; <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">len</span>(test[u]) &lt; <span class="number">1</span>: <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        seq = np.zeros([args.maxlen], dtype=np.int32)</span><br><span class="line">        idx = args.maxlen - <span class="number">1</span></span><br><span class="line">        <span class="comment"># 假设原始序列为[1,2,3,4,5,6,7]    6用于valid；7用于test</span></span><br><span class="line">        seq[idx] = valid[u][<span class="number">0</span>]  <span class="comment"># seq: [0,0,0,...,0,0,0,6]</span></span><br><span class="line">        idx -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(train[u]):  <span class="comment"># seq: [0,0,0,...,0,1,2,3,4,5,6]  只剩test里的[7]用于预测</span></span><br><span class="line">            seq[idx] = i</span><br><span class="line">            idx -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> idx == -<span class="number">1</span>: <span class="keyword">break</span></span><br><span class="line">        rated = <span class="built_in">set</span>(train[u])  <span class="comment"># 序列物品集合</span></span><br><span class="line">        rated.add(<span class="number">0</span>)</span><br><span class="line">        item_idx = [test[u][<span class="number">0</span>]]  <span class="comment"># 取出ground truth label</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># item_idx: [label,random,random,...,random] 1+100个随机物品，看得分在top10的情况</span></span><br><span class="line">            t = np.random.randint(<span class="number">1</span>, itemnum + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">while</span> t <span class="keyword">in</span> rated: t = np.random.randint(<span class="number">1</span>, itemnum + <span class="number">1</span>)</span><br><span class="line">            item_idx.append(t)</span><br><span class="line"></span><br><span class="line">        predictions = -model.predict(*[np.array(l) <span class="keyword">for</span> l <span class="keyword">in</span> [[u], [seq], item_idx]])</span><br><span class="line">        predictions = predictions[<span class="number">0</span>]  <span class="comment"># (1,101) -&gt; 101 (squeeze)</span></span><br><span class="line"></span><br><span class="line">        rank = predictions.argsort().argsort()[<span class="number">0</span>].item()  <span class="comment"># 做两次argsort()，可以得到每个位置的rank排名</span></span><br><span class="line"></span><br><span class="line">        valid_user += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rank &lt; <span class="number">10</span>:  <span class="comment"># TOP10才记录，这里真实rank = rank + 1 ，因为argsort()索引包含0</span></span><br><span class="line">            NDCG += <span class="number">1</span> / np.log2(rank + <span class="number">2</span>)</span><br><span class="line">            HT += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> valid_user % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;.&#x27;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">            <span class="comment"># sys.stdout.flush()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> NDCG / valid_user, HT / valid_user</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p><a href="https://dl.acm.org/doi/10.1145/2806416.2806504">[1]He X, Chen T, Kan M Y, et al. Trirank: Review-aware explainable recommendation by modeling aspects</a></p>
]]></content>
      <categories>
        <category>代码阅读</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>SASRec</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL组队学习01：环境搭建，初识数据库</title>
    <url>/2022/07/12/SQL%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A001%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%8C%E5%88%9D%E8%AF%86%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
    <content><![CDATA[<h1 id="Task01：环境搭建，初识数据库"><a href="#Task01：环境搭建，初识数据库" class="headerlink" title="Task01：环境搭建，初识数据库"></a>Task01：环境搭建，初识数据库</h1><h1 id="SQL-环境搭建"><a href="#SQL-环境搭建" class="headerlink" title="SQL 环境搭建"></a>SQL 环境搭建</h1><p>主要 follw 教程：<a href="https://github.com/datawhalechina/wonderful-sql">https://github.com/datawhalechina/wonderful-sql</a></p>
<span id="more"></span>
<h2 id="1-安装-MySQL"><a href="#1-安装-MySQL" class="headerlink" title="1 安装 MySQL"></a>1 安装 MySQL</h2><p>系统为 MacOS ，根据教程一步步来，很简单。</p>
<h2 id="2-DataGrip-的安装和连接MySQL"><a href="#2-DataGrip-的安装和连接MySQL" class="headerlink" title="2 DataGrip 的安装和连接MySQL"></a>2 DataGrip 的安装和连接MySQL</h2><p>安装好 MySQL 以后，使用 DataGrip 连接。DataGrip 教程：<a href="https://www.jetbrains.com/help/datagrip/quick-start-with-datagrip.html">https://www.jetbrains.com/help/datagrip/quick-start-with-datagrip.html</a></p>
<h2 id="3-创建学习用的数据库"><a href="#3-创建学习用的数据库" class="headerlink" title="3 创建学习用的数据库"></a>3 创建学习用的数据库</h2><p>根据《SQL基础教程》提供的MySQL版本的数据库,数据表的创建以及数据导入的代码, 经过一些修改, 创建了一份 sql 脚本, 运行该脚本可以一步到位地创建本文档所需的数据库shop及其中所有的表,并插入本教程所需要的所有数据。</p>
<p>下述SQL脚本可用于创建本教程所使用的示例数据库 <code>shop</code> 以及数据库中表的创建和数据的插入。</p>
<p>详见 <code>./materials/shop.sql</code></p>
<p>将<code>shop.sql</code>复制到在 DataGrip 里并运行</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220710185610866.png" alt="image-20220710185610866"></p>
<h3 id="3-1-执行-SQL-查询"><a href="#3-1-执行-SQL-查询" class="headerlink" title="3.1 执行 SQL 查询"></a>3.1 执行 SQL 查询</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> shopproduct <span class="keyword">where</span> quantity <span class="operator">&gt;</span> <span class="number">100</span>;</span><br></pre></td></tr></table></figure>
<p>查询结果</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220710190234782.png" alt="image-20220710190234782"></p>
<h1 id="SQL-初识数据库"><a href="#SQL-初识数据库" class="headerlink" title="SQL 初识数据库"></a>SQL 初识数据库</h1><h2 id="1-1-初识数据库"><a href="#1-1-初识数据库" class="headerlink" title="1.1 初识数据库"></a>1.1 初识数据库</h2><p>数据库（Database，DB）：将大量数据保存起来，通过计算机加工而成的可以进行高效访问的数据集合</p>
<p>数据库管理系统（Database Management System，DBMS）：用来管理数据库的计算机系统</p>
<h3 id="1-1-1-DBMS的种类"><a href="#1-1-1-DBMS的种类" class="headerlink" title="1.1.1 DBMS的种类"></a>1.1.1 DBMS的种类</h3><p>DBMS 主要通过数据的保存格式（数据库的种类）来进行分类，现阶段主要有以下 5 种类型：</p>
<ul>
<li><p>层次数据库（Hierarchical Database，HDB）</p>
</li>
<li><p>关系数据库（Relational Database，RDB）</p>
<ul>
<li>Oracle Database：甲骨文公司的RDBMS</li>
<li>SQL Server：微软公司的RDBMS</li>
<li>DB2：IBM公司的RDBMS</li>
<li>PostgreSQL：开源的RDBMS</li>
<li>MySQL：开源的RDBMS</li>
</ul>
<p>如上是5种具有代表性的RDBMS，其特点是由行和列组成的二维表来管理数据，这种类型的 DBMS 称为<strong>关系数据库管理系统</strong>（Relational Database Management System，RDBMS）。</p>
</li>
<li><p>面向对象数据库（Object Oriented Database，OODB）</p>
</li>
<li><p>XML数据库（XML Database，XMLDB）</p>
</li>
<li><p>键值存储系统（Key-Value Store，KVS），举例：MongoDB</p>
</li>
</ul>
<h3 id="1-1-2-RDBMS的常见系统结构"><a href="#1-1-2-RDBMS的常见系统结构" class="headerlink" title="1.1.2 RDBMS的常见系统结构"></a>1.1.2 RDBMS的常见系统结构</h3><p>客户端 / 服务器类型（C/S类型）</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712000449258.png" alt="image-20220712000449258"></p>
<h2 id="1-2-初识SQL"><a href="#1-2-初识SQL" class="headerlink" title="1.2 初识SQL"></a>1.2 初识SQL</h2><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712000537847.png" alt="image-20220712000537847"></p>
<p>数据库存储的表结构类似与excel的行和列，每一行称为一条<strong>记录</strong>，每一列称为一个<strong>字段</strong>。</p>
<p>SQL语句可以分为以下三类：</p>
<ul>
<li><strong>DDL</strong> ：DDL（Data Definition Language，数据定义语言） 用来创建或者删除存储数据用的数据库以及数据库中的表等对象。DDL 包含以下几种指令。<ul>
<li>CREATE ： 创建数据库和表等对象</li>
<li>DROP ： 删除数据库和表等对象</li>
<li>ALTER ： 修改数据库和表等对象的结构</li>
</ul>
</li>
<li><strong>DML</strong> :DML（Data Manipulation Language，数据操纵语言） 用来查询或者变更表中的记录。DML 包含以下几种指令。（<strong>增删改查</strong>）<ul>
<li>SELECT ：查询表中的数据</li>
<li>INSERT ：向表中插入新数据</li>
<li>UPDATE ：更新表中的数据</li>
<li>DELETE ：删除表中的数据</li>
</ul>
</li>
<li><strong>DCL</strong> ：DCL（Data Control Language，数据控制语言） 用来确认或者取消对数据库中的数据进行的变更。除此之外，还可以对 RDBMS 的用户是否有权限操作数据库中的对象（数据库表等）进行设定。DCL 包含以下几种指令。<ul>
<li>COMMIT ： 确认对数据库中的数据进行的变更</li>
<li>ROLLBACK ： 取消对数据库中的数据进行的变更</li>
<li>GRANT ： 赋予用户操作权限</li>
<li>REVOKE ： 取消用户的操作权限</li>
</ul>
</li>
</ul>
<p>实际使用的 SQL 语句当中有 90% 属于 DML，本书同样会以 DML 为中心进行讲解。</p>
<h2 id="1-2-SQL语句"><a href="#1-2-SQL语句" class="headerlink" title="1.2 SQL语句"></a>1.2 SQL语句</h2><h3 id="1-2-1-SQL的基本书写规则"><a href="#1-2-1-SQL的基本书写规则" class="headerlink" title="1.2.1 SQL的基本书写规则"></a>1.2.1 SQL的基本书写规则</h3><p><strong>SQL语法规范总得的原则是，清楚、易读并且层次清晰。</strong>实际场景中常常动辄几百上千行的SQL语句，如果不写清楚，事后review或者别人接手的时候，会让人怀疑人生。</p>
<p>当然这些规范都是笔者根据实际经验总结归纳的，并不是金科铁律，但是强烈建议新手按以下规范入门编写SQL语句。</p>
<p><strong>常见注意事项如下：</strong></p>
<ol>
<li>MySQL本身不区分大小写，但<strong>强烈要求关键字大写</strong>，表名、列名用小写；</li>
<li>创建表时，使用统一的、描述性强的字段命名规则保证字段名是独一无二且不是保留字的，不要使用连续的下划线，不用下划线结尾；最好以字母开头</li>
<li>关键字右对齐，且不同层级的用空格或缩进控制，使其区分开，见样例二；</li>
<li>列名少的时候写在一行里无伤大雅；多的时候以及涉及到CASE WHEN 或者聚合计算的时候，建议分行写；个人习惯是逗号在列名前面，方便之后删除某些列，放列名后亦可；</li>
<li>表别名和列别名尽量用有具体含义的词组，不要用a b c，不然以后review的时候会非常痛苦；</li>
<li>运算符前后都加一个空格；</li>
<li>当用到多个表时，请在所有列名前写上引用的表别名，不要嫌麻烦；</li>
<li>每条命令用分号结尾；</li>
<li>养成随手写注释的习惯，注释方法：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">单行注释 #注释文字</span><br><span class="line">单行注释 -- 注释文字</span><br><span class="line">多行注释：/* 注释文字 */</span><br></pre></td></tr></table></figure>
<p>P.S. 养成良好的书写习惯，可以避免以后发生流血事件  →_→</p>
<h3 id="1-2-2-数据库的创建（CREATE-DATABASE-语句）"><a href="#1-2-2-数据库的创建（CREATE-DATABASE-语句）" class="headerlink" title="1.2.2 数据库的创建（CREATE DATABASE 语句）"></a>1.2.2 数据库的创建（CREATE DATABASE 语句）</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE <span class="operator">&lt;</span> 数据库名称 <span class="operator">&gt;</span> ;</span><br></pre></td></tr></table></figure>
<p>创建本课程使用的数据库；因为之前已经用脚本创建过 DATABASE shop 了，这里数据库名改为 shop_test：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE shop_test;</span><br></pre></td></tr></table></figure>
<p>如图，创建成功空数据库</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712001902049.png" alt="image-20220712001902049"></p>
<h3 id="1-2-3-表的创建（-CREATE-TABLE-语句）"><a href="#1-2-3-表的创建（-CREATE-TABLE-语句）" class="headerlink" title="1.2.3 表的创建（ CREATE TABLE 语句）"></a>1.2.3 表的创建（ CREATE TABLE 语句）</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="operator">&lt;</span> 表名 <span class="operator">&gt;</span></span><br><span class="line">( <span class="operator">&lt;</span> 列名 <span class="number">1</span><span class="operator">&gt;</span> <span class="operator">&lt;</span> 数据类型 <span class="operator">&gt;</span> <span class="operator">&lt;</span> 该列所需约束 <span class="operator">&gt;</span> ,</span><br><span class="line">  <span class="operator">&lt;</span> 列名 <span class="number">2</span><span class="operator">&gt;</span> <span class="operator">&lt;</span> 数据类型 <span class="operator">&gt;</span> <span class="operator">&lt;</span> 该列所需约束 <span class="operator">&gt;</span> ,</span><br><span class="line">  <span class="operator">&lt;</span> 列名 <span class="number">3</span><span class="operator">&gt;</span> <span class="operator">&lt;</span> 数据类型 <span class="operator">&gt;</span> <span class="operator">&lt;</span> 该列所需约束 <span class="operator">&gt;</span> ,</span><br><span class="line">  <span class="operator">&lt;</span> 列名 <span class="number">4</span><span class="operator">&gt;</span> <span class="operator">&lt;</span> 数据类型 <span class="operator">&gt;</span> <span class="operator">&lt;</span> 该列所需约束 <span class="operator">&gt;</span> ,</span><br><span class="line">  .</span><br><span class="line">  .</span><br><span class="line">  .</span><br><span class="line">  <span class="operator">&lt;</span> 该表的约束 <span class="number">1</span><span class="operator">&gt;</span> , <span class="operator">&lt;</span> 该表的约束 <span class="number">2</span><span class="operator">&gt;</span> ,……);</span><br></pre></td></tr></table></figure>
<p>创建本课程用到的商品表，照着敲</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> product</span><br><span class="line">(product_id <span class="type">CHAR</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line"> product_name <span class="type">VARCHAR</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line"> product_type <span class="type">VARCHAR</span>(<span class="number">32</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line"> sale_price <span class="type">INTEGER</span> ,</span><br><span class="line"> purchase_price <span class="type">INTEGER</span> ,</span><br><span class="line"> regist_date <span class="type">DATE</span> ,</span><br><span class="line"> <span class="keyword">PRIMARY</span> KEY (product_id));</span><br></pre></td></tr></table></figure>
<p>创建成功如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712002519329.png" alt="image-20220712002519329"></p>
<p>其中 product 列名称的解释：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712002054990.png" alt="image-20220712002054990"></p>
<h3 id="1-2-4-数据类型"><a href="#1-2-4-数据类型" class="headerlink" title="1.2.4 数据类型"></a>1.2.4 数据类型</h3><p>数据库创建的表，所有的列都必须指定数据类型，每一列都不能存储与该列数据类型不符的数据。</p>
<p>四种最基本的数据类型</p>
<ul>
<li>INTEGER 型</li>
</ul>
<p>用来指定存储整数的列的数据类型（数字型），不能存储小数。</p>
<ul>
<li>CHAR 型</li>
</ul>
<p>用来存储<strong>定长字符串</strong>，当列中存储的字符串长度达不到最大长度的时候，使用半角空格进行补足，由于会浪费存储空间，所以一般不使用。</p>
<ul>
<li>VARCHAR 型</li>
</ul>
<p>用来存储<strong>可变长度字符串</strong>，定长字符串在字符数未达到最大长度时会用半角空格补足，但可变长字符串不同，即使字符数未达到最大长度，也不会用半角空格补足。</p>
<ul>
<li>DATE 型</li>
</ul>
<p>用来指定存储日期（年月日）的列的数据类型（日期型）。</p>
<blockquote>
<p>CHAR 是定长字符串，VARCHAR 是变长字符串</p>
</blockquote>
<h3 id="1-2-5-约束的设置"><a href="#1-2-5-约束的设置" class="headerlink" title="1.2.5 约束的设置"></a>1.2.5 约束的设置</h3><p>约束是除了数据类型之外，对列中存储的数据进行限制或者追加条件的功能。</p>
<p><code>NOT NULL</code>是非空约束，即该列必须输入数据。</p>
<p><code>PRIMARY KEY</code>是主键约束，代表该列是唯一值，可以通过该列取出特定的行的数据。</p>
<h3 id="1-2-6-表的删除和更新"><a href="#1-2-6-表的删除和更新" class="headerlink" title="1.2.6 表的删除和更新"></a>1.2.6 表的删除和更新</h3><ul>
<li>删除表的语法：</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="operator">&lt;</span> 表名 <span class="operator">&gt;</span> ;</span><br></pre></td></tr></table></figure>
<ul>
<li>删除 product 表</li>
</ul>
<p>需要特别注意的是，<strong>删除的表是无法恢复的</strong>，只能重新插入，请执行删除操作时要特别谨慎。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> product;</span><br></pre></td></tr></table></figure>
<p>表已删除：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712224235900.png" alt="image-20220712224235900"></p>
<ul>
<li>添加列的 ALTER TABLE 语句</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="operator">&lt;</span> 表名 <span class="operator">&gt;</span> <span class="keyword">ADD</span> <span class="keyword">COLUMN</span> <span class="operator">&lt;</span> 列的定义 <span class="operator">&gt;</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>添加一列可以存储100位的可变长字符串的 product_name_pinyin 列</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> product <span class="keyword">ADD</span> <span class="keyword">COLUMN</span> product_name_pinyin <span class="type">VARCHAR</span>(<span class="number">100</span>);</span><br></pre></td></tr></table></figure>
<p>重新建表并添加列：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712224503279.png" alt="image-20220712224503279" style="zoom:50%;" /><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712224537910.png" alt="image-20220712224537910" style="zoom:50%;" /></p>
<ul>
<li>删除列的 ALTER TABLE 语句</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="operator">&lt;</span> 表名 <span class="operator">&gt;</span> <span class="keyword">DROP</span> <span class="keyword">COLUMN</span> <span class="operator">&lt;</span> 列名 <span class="operator">&gt;</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>删除 product_name_pinyin 列</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> product <span class="keyword">DROP</span> <span class="keyword">COLUMN</span> product_name_pinyin;</span><br></pre></td></tr></table></figure>
<p>删除后恢复到添加列之前：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712224626807.png" alt="image-20220712224626807"></p>
<ul>
<li>删除表中特定的<strong>行</strong>（语法）</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 一定注意添加 WHERE 条件，否则将会删除所有的数据</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> product <span class="keyword">WHERE</span> COLUMN_NAME<span class="operator">=</span><span class="string">&#x27;XXX&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>ALTER TABLE 语句和 DROP TABLE 语句一样，执行之后<strong>无法恢复</strong>。误添加的列可以通过 ALTER TABLE 语句删除，或者将表全部删除之后重新再创建。 【扩展内容】</p>
<ul>
<li>清空表内容</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> TABLE_NAME;</span><br></pre></td></tr></table></figure>
<p>优点：相比<code>drop / delete</code>，<code>truncate</code>用来清除数据时，<strong>速度最快</strong>。</p>
<ul>
<li>数据的更新</li>
</ul>
<p>基本语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">UPDATE <span class="operator">&lt;</span>表名<span class="operator">&gt;</span></span><br><span class="line">   <span class="keyword">SET</span> <span class="operator">&lt;</span>列名<span class="operator">&gt;</span> <span class="operator">=</span> <span class="operator">&lt;</span>表达式<span class="operator">&gt;</span> [, <span class="operator">&lt;</span>列名<span class="number">2</span><span class="operator">&gt;=</span><span class="operator">&lt;</span>表达式<span class="number">2</span><span class="operator">&gt;</span>...]  </span><br><span class="line"> <span class="keyword">WHERE</span> <span class="operator">&lt;</span>条件<span class="operator">&gt;</span>  <span class="comment">-- 可选，非常重要</span></span><br><span class="line"> <span class="keyword">ORDER</span> <span class="keyword">BY</span> 子句  <span class="comment">--可选</span></span><br><span class="line"> LIMIT 子句; <span class="comment">--可选</span></span><br></pre></td></tr></table></figure>
<p><strong>使用 update 时要注意添加 where 条件，否则将会将所有的行按照语句修改</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 修改所有的注册时间</span></span><br><span class="line">UPDATE product</span><br><span class="line">   <span class="keyword">SET</span> regist_date <span class="operator">=</span> <span class="string">&#x27;2009-10-10&#x27;</span>;  </span><br><span class="line"><span class="comment">-- 仅修改部分商品的单价</span></span><br><span class="line">UPDATE product</span><br><span class="line">   <span class="keyword">SET</span> sale_price <span class="operator">=</span> sale_price <span class="operator">*</span> <span class="number">10</span></span><br><span class="line"> <span class="keyword">WHERE</span> product_type <span class="operator">=</span> <span class="string">&#x27;厨房用具&#x27;</span>;  </span><br></pre></td></tr></table></figure>
<p>使用 UPDATE 也可以将列更新为 NULL（该更新俗称为NULL清空）。此时只需要将赋值表达式右边的值直接写为 NULL 即可。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 将商品编号为0008的数据（圆珠笔）的登记日期更新为NULL  </span></span><br><span class="line">UPDATE product</span><br><span class="line">   <span class="keyword">SET</span> regist_date <span class="operator">=</span> <span class="keyword">NULL</span></span><br><span class="line"> <span class="keyword">WHERE</span> product_id <span class="operator">=</span> <span class="string">&#x27;0008&#x27;</span>;  </span><br></pre></td></tr></table></figure>
<p>和 INSERT 语句一样， UPDATE 语句也可以将 NULL 作为一个值来使用。 <strong>但是，只有未设置 NOT NULL 约束和主键约束的列才可以清空为NULL。</strong>如果将设置了上述约束的列更新为 NULL，就会出错，这点与INSERT 语句相同。</p>
<p>多列更新</p>
<p>UPDATE 语句的 SET 子句支持同时将多个列作为更新对象。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 基础写法，一条UPDATE语句只更新一列</span></span><br><span class="line">UPDATE product</span><br><span class="line">   <span class="keyword">SET</span> sale_price <span class="operator">=</span> sale_price <span class="operator">*</span> <span class="number">10</span></span><br><span class="line"> <span class="keyword">WHERE</span> product_type <span class="operator">=</span> <span class="string">&#x27;厨房用具&#x27;</span>;</span><br><span class="line">UPDATE product</span><br><span class="line">   <span class="keyword">SET</span> purchase_price <span class="operator">=</span> purchase_price <span class="operator">/</span> <span class="number">2</span></span><br><span class="line"> <span class="keyword">WHERE</span> product_type <span class="operator">=</span> <span class="string">&#x27;厨房用具&#x27;</span>;  </span><br></pre></td></tr></table></figure>
<p>该写法可以得到正确结果，但是代码较为繁琐。可以采用合并的方法来简化代码。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 合并后的写法</span></span><br><span class="line">UPDATE product</span><br><span class="line">   <span class="keyword">SET</span> sale_price <span class="operator">=</span> sale_price <span class="operator">*</span> <span class="number">10</span>,</span><br><span class="line">       purchase_price <span class="operator">=</span> purchase_price <span class="operator">/</span> <span class="number">2</span></span><br><span class="line"> <span class="keyword">WHERE</span> product_type <span class="operator">=</span> <span class="string">&#x27;厨房用具&#x27;</span>;  </span><br></pre></td></tr></table></figure>
<p>需要明确的是，SET 子句中的列不仅可以是两列，还可以是三列或者更多。</p>
<h3 id="1-2-7-向-product-表中插入数据"><a href="#1-2-7-向-product-表中插入数据" class="headerlink" title="1.2.7 向 product 表中插入数据"></a>1.2.7 向 product 表中插入数据</h3><p>为了学习 <code>INSERT</code> 语句用法，我们首先创建一个名为 productins 的表，建表语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> productins</span><br><span class="line">(product_id    <span class="type">CHAR</span>(<span class="number">4</span>)      <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">product_name   <span class="type">VARCHAR</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">product_type   <span class="type">VARCHAR</span>(<span class="number">32</span>)  <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">sale_price     <span class="type">INTEGER</span>      <span class="keyword">DEFAULT</span> <span class="number">0</span>,</span><br><span class="line">purchase_price <span class="type">INTEGER</span> ,</span><br><span class="line">regist_date    <span class="type">DATE</span> ,</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (product_id)); </span><br></pre></td></tr></table></figure>
<p>建表如下，product_id 为主键：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712225215297.png" alt="image-20220712225215297"></p>
<p>插入语句的基本语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="operator">&lt;</span>表名<span class="operator">&gt;</span> (列<span class="number">1</span>, 列<span class="number">2</span>, 列<span class="number">3</span>, ……) <span class="keyword">VALUES</span> (值<span class="number">1</span>, 值<span class="number">2</span>, 值<span class="number">3</span>, ……);  </span><br></pre></td></tr></table></figure>
<p>对表进行<strong>全列 INSERT 时</strong>，可以省略表名后的列清单。这时 VALUES子句的值会默认按照从左到右的顺序赋给每一列。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 包含列清单</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> productins (product_id, product_name, product_type, sale_price, purchase_price, regist_date) <span class="keyword">VALUES</span> (<span class="string">&#x27;0006&#x27;</span>, <span class="string">&#x27;电饭煲&#x27;</span>, <span class="string">&#x27;厨房用具&#x27;</span>, <span class="number">1500</span>, <span class="number">1000</span>, <span class="string">&#x27;2022-07-12&#x27;</span>);</span><br><span class="line"><span class="comment">-- 省略列清单</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> productins <span class="keyword">VALUES</span> (<span class="string">&#x27;0005&#x27;</span>, <span class="string">&#x27;高压锅&#x27;</span>, <span class="string">&#x27;厨房用具&#x27;</span>, <span class="number">6800</span>, <span class="number">5000</span>, <span class="string">&#x27;2009-01-15&#x27;</span>);  </span><br></pre></td></tr></table></figure>
<p>都成功插入一条记录：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712225543281.png" alt="image-20220712225543281"></p>
<p>原则上，执行一次 INSERT 语句会插入一行数据。插入多行时，通常需要循环执行相应次数的 INSERT 语句。其实很多 RDBMS 都支持一次插入多行数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 多行INSERT （ DB2、SQL、SQL Server、 PostgreSQL 和 MySQL多行插入）</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> productins <span class="keyword">VALUES</span> (<span class="string">&#x27;0002&#x27;</span>, <span class="string">&#x27;打孔器&#x27;</span>, <span class="string">&#x27;办公用品&#x27;</span>, <span class="number">500</span>, <span class="number">320</span>, <span class="string">&#x27;2009-09-11&#x27;</span>),</span><br><span class="line">                              (<span class="string">&#x27;0003&#x27;</span>, <span class="string">&#x27;运动T恤&#x27;</span>, <span class="string">&#x27;衣服&#x27;</span>, <span class="number">4000</span>, <span class="number">2800</span>, <span class="keyword">NULL</span>),</span><br><span class="line">                              (<span class="string">&#x27;0004&#x27;</span>, <span class="string">&#x27;菜刀&#x27;</span>, <span class="string">&#x27;厨房用具&#x27;</span>, <span class="number">3000</span>, <span class="number">2800</span>, <span class="string">&#x27;2009-09-20&#x27;</span>);   </span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712225751354.png" alt="image-20220712225751354"></p>
<h3 id="1-2-7-从其他表插入"><a href="#1-2-7-从其他表插入" class="headerlink" title="1.2.7* 从其他表插入"></a>1.2.7* 从其他表插入</h3><p>可以使用INSERT … SELECT 语句从其他表复制数据。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 将productins表中的数据复制到product表中</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product (product_id, product_name, product_type, sale_price, purchase_price, regist_date)</span><br><span class="line"><span class="keyword">SELECT</span> product_id, product_name, product_type, sale_price, purchase_price, regist_date</span><br><span class="line">  <span class="keyword">FROM</span> productins;  </span><br></pre></td></tr></table></figure>
<p>把productins表的内容复制到了product中：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712230108663.png" alt="image-20220712230108663"></p>
<h3 id="1-2-8-索引"><a href="#1-2-8-索引" class="headerlink" title="1.2.8 索引"></a>1.2.8 索引</h3><ul>
<li>索引的作用</li>
</ul>
<p>MySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。</p>
<p>拿汉语字典的目录页（索引）打比方，我们可以按拼音、笔画、偏旁部首等排序的目录（索引）快速查找到需要的字。</p>
<p>索引创建了一种有序的数据结构，采用二分法搜索数据时，其复杂度为 O(logN) ，1000多万的数据只要搜索23次，其效率是非常高效的。</p>
<ul>
<li>如何创建索引（两种方式）</li>
</ul>
<p>创建表时可以直接创建索引，语法如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建表时就建立索引</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mytable(  </span><br><span class="line"> </span><br><span class="line">ID <span class="type">INT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,   </span><br><span class="line"> </span><br><span class="line">username <span class="type">VARCHAR</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,  </span><br><span class="line"> </span><br><span class="line">INDEX [indexName] (username(length))  </span><br><span class="line"> </span><br><span class="line">);  </span><br></pre></td></tr></table></figure>
<p>也可以使用如下语句创建：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法1</span></span><br><span class="line"><span class="keyword">CREATE</span> INDEX indexName <span class="keyword">ON</span> table_name (column_name)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 方法2</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">table</span> tableName <span class="keyword">ADD</span> INDEX indexName(columnName)</span><br></pre></td></tr></table></figure>
<h1 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h1><h2 id="1-1"><a href="#1-1" class="headerlink" title="1.1"></a>1.1</h2><p>编写一条 CREATE TABLE 语句，用来创建一个包含表 1-A 中所列各项的表 Addressbook （地址簿），并为 regist_no （注册编号）列设置主键约束</p>
<p>表1-A　表 Addressbook （地址簿）中的列</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712230844459.png" alt="image-20220712230844459"></p>
<p>答：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Addressbook(</span><br><span class="line">    regist_no <span class="type">INTEGER</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span> ,</span><br><span class="line">    name <span class="type">VARCHAR</span>(<span class="number">128</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> ,</span><br><span class="line">    tel_no <span class="type">CHAR</span>(<span class="number">10</span>) ,</span><br><span class="line">    mail_address <span class="type">CHAR</span>(<span class="number">20</span>),</span><br><span class="line">    <span class="keyword">PRIMARY</span> KEY (regist_no)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712231249778.png" alt="image-20220712231249778"></p>
<h2 id="1-2"><a href="#1-2" class="headerlink" title="1.2"></a>1.2</h2><p>假设在创建练习1.1中的 Addressbook 表时忘记添加如下一列 postal_code （邮政编码）了，请编写 SQL 把此列添加到 Addressbook 表中。</p>
<p>列名 ： postal_code</p>
<p>数据类型 ：定长字符串类型（长度为 8）</p>
<p>约束 ：不能为 NULL</p>
<p>答：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> Addressbook <span class="keyword">ADD</span> <span class="keyword">COLUMN</span> postal_code <span class="type">CHAR</span>(<span class="number">8</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> ;</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220712231436444.png" alt="image-20220712231436444"></p>
<h2 id="1-3-填空题"><a href="#1-3-填空题" class="headerlink" title="1.3 填空题"></a>1.3 填空题</h2><p>请补充如下 SQL 语句来删除 Addressbook 表。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(  DROP  ) table Addressbook;</span><br></pre></td></tr></table></figure>
<h2 id="1-4-判断题"><a href="#1-4-判断题" class="headerlink" title="1.4 判断题"></a>1.4 判断题</h2><p>是否可以编写 SQL 语句来恢复删除掉的 Addressbook 表？</p>
<p>答：不可以。ALTER TABLE 语句和 DROP TABLE 语句一样，执行之后<strong>无法恢复</strong>。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>wonderful-sql 教程：<a href="https://github.com/datawhalechina/wonderful-sql/blob/main/ch01:%20%E5%88%9D%E8%AF%86%E6%95%B0%E6%8D%AE%E5%BA%93.md">https://github.com/datawhalechina/wonderful-sql/blob/main/ch01:%20%E5%88%9D%E8%AF%86%E6%95%B0%E6%8D%AE%E5%BA%93.md</a></p>
<p>SQL语法规范：<a href="https://github.com/datawhalechina/wonderful-sql/blob/main/materials/%E9%99%84%E5%BD%951%EF%BC%9ASQL%E8%AF%AD%E6%B3%95%E8%A7%84%E8%8C%83.md">https://github.com/datawhalechina/wonderful-sql/blob/main/materials/%E9%99%84%E5%BD%951%EF%BC%9ASQL%E8%AF%AD%E6%B3%95%E8%A7%84%E8%8C%83.md</a></p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Datawhale组队学习</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL组队学习02：基础查询与排序</title>
    <url>/2022/07/15/SQL%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A002%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%9F%A5%E8%AF%A2%E4%B8%8E%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="Task02：基础查询与排序"><a href="#Task02：基础查询与排序" class="headerlink" title="Task02：基础查询与排序"></a>Task02：基础查询与排序</h1><span id="more"></span>
<h2 id="2-1-SELECT-语句基础"><a href="#2-1-SELECT-语句基础" class="headerlink" title="2.1 SELECT 语句基础"></a>2.1 SELECT 语句基础</h2><h3 id="2-1-1-从表中选取数据"><a href="#2-1-1-从表中选取数据" class="headerlink" title="2.1.1 从表中选取数据"></a>2.1.1 从表中选取数据</h3><h4 id="SELECT语句"><a href="#SELECT语句" class="headerlink" title="SELECT语句"></a>SELECT语句</h4><p>从表中选取数据时需要使用SELECT语句，也就是<strong>只从表中选出（SELECT）必要数据</strong>的意思。通过SELECT语句查询并选取出必要数据的过程称为匹配查询或<strong>查询</strong>（query）。</p>
<p>基本SELECT语句包含了SELECT和FROM两个子句（clause）。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>列名<span class="operator">&gt;</span>, </span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>表名<span class="operator">&gt;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-1-2-从表中选取符合条件的数据"><a href="#2-1-2-从表中选取符合条件的数据" class="headerlink" title="2.1.2 从表中选取符合条件的数据"></a>2.1.2 从表中选取符合条件的数据</h3><h4 id="WHERE语句"><a href="#WHERE语句" class="headerlink" title="WHERE语句"></a>WHERE语句</h4><p>当不需要取出全部数据，而是选取出满足“商品种类为衣服”“销售单价在1000日元以上”等某些条件的数据时，使用WHERE语句。</p>
<p>SELECT 语句通过WHERE子句来指定查询数据的条件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT &lt;列名&gt;, ……</span><br><span class="line">  FROM &lt;表名&gt;</span><br><span class="line"> WHERE &lt;条件表达式&gt;;</span><br></pre></td></tr></table></figure>
<p>比较下面两者输出结果的不同：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 用来选取product type列为衣服的记录的SELECT语句</span></span><br><span class="line"><span class="keyword">SELECT</span> product_name, product_type</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">WHERE</span> product_type <span class="operator">=</span> <span class="string">&#x27;衣服&#x27;</span>;</span><br><span class="line"><span class="comment">-- 也可以选取出不是查询条件的列（条件列与输出列不同）</span></span><br><span class="line"><span class="keyword">SELECT</span> product_name</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">WHERE</span> product_type <span class="operator">=</span> <span class="string">&#x27;衣服&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-1-3-相关法则"><a href="#2-1-3-相关法则" class="headerlink" title="2.1.3 相关法则"></a>2.1.3 相关法则</h3><ul>
<li>星号（*）代表全部列的意思。</li>
<li>SQL中可以随意使用换行符，不影响语句执行（<strong>但不可插入空行</strong>）。</li>
<li>设定汉语别名时需要使用双引号（”）括起来。</li>
<li>在SELECT语句中使用DISTINCT可以删除重复行。</li>
<li>注释是SQL语句中用来标识说明或者注意事项的部分。分为1行注释”— “和多行注释两种”/<em> </em>/“。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">	<span class="comment">-- 想要查询出全部列时，可以使用代表所有列的星号（*）。</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>表名<span class="operator">&gt;</span>；</span><br><span class="line"><span class="comment">-- SQL语句可以使用AS关键字为列设定别名（用中文时需要双引号（“”））。</span></span><br><span class="line"><span class="keyword">SELECT</span> product_id     <span class="keyword">As</span> id,</span><br><span class="line">       product_name   <span class="keyword">As</span> name,</span><br><span class="line">       purchase_price <span class="keyword">AS</span> &quot;进货单价&quot;</span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br><span class="line"><span class="comment">-- 使用DISTINCT删除product_type列中重复的数据</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> product_type</span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br></pre></td></tr></table></figure>
<h3 id="2-1-4-关于DISTINCT"><a href="#2-1-4-关于DISTINCT" class="headerlink" title="2.1.4 关于DISTINCT"></a>2.1.4 关于DISTINCT</h3><p>DISTINCT 是跟在 SELECT 后面的，表示对查询出来的结果去重；而不是跟在某个字段之前。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span> product_type,</span><br><span class="line">                product_id</span><br><span class="line"><span class="keyword">from</span> product;</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714223600736.png" alt="image-20220714223600736"></p>
<p>本例中product_type仍然有重复，因为去重的条件是两个SELECT的字段都相同，而本例中它们的product_id不同。</p>
<h2 id="2-2-算术运算符和比较运算符"><a href="#2-2-算术运算符和比较运算符" class="headerlink" title="2.2 算术运算符和比较运算符"></a>2.2 算术运算符和比较运算符</h2><h3 id="2-2-1-算术运算符"><a href="#2-2-1-算术运算符" class="headerlink" title="2.2.1 算术运算符"></a>2.2.1 算术运算符</h3><p>SQL语句中可以使用四则运算符：“+”，“-”，“*”，“/”</p>
<h3 id="2-2-2-比较运算符"><a href="#2-2-2-比较运算符" class="headerlink" title="2.2.2 比较运算符"></a>2.2.2 比较运算符</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-- 选取出sale_price列为500的记录</span><br><span class="line">SELECT product_name, product_type</span><br><span class="line">  FROM product</span><br><span class="line"> WHERE sale_price = 500;</span><br></pre></td></tr></table></figure>
<p>SQL常见比较运算符如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>运算符</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>=</td>
<td>和 ~ 相等</td>
</tr>
<tr>
<td>&lt;&gt; / !=</td>
<td>和 ~ 不相等</td>
</tr>
<tr>
<td>&gt;=</td>
<td>大于等于 ~</td>
</tr>
<tr>
<td>&gt;</td>
<td>大于 ~</td>
</tr>
<tr>
<td>&lt;=</td>
<td>小于等于 ~</td>
</tr>
<tr>
<td>&lt;</td>
<td>小于 ~</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-2-3-常用法则"><a href="#2-2-3-常用法则" class="headerlink" title="2.2.3 常用法则"></a>2.2.3 常用法则</h3><ul>
<li>SELECT子句中可以使用<strong>常数或者表达式</strong>。</li>
<li>使用比较运算符时一定要注意不等号和等号的位置。</li>
<li>字符串类型的数据原则上按照字典顺序进行排序，不能与数字的大小顺序混淆。</li>
<li>希望选取NULL记录时，需要在条件表达式中使用IS NULL运算符。希望选取不是NULL的记录时，需要在条件表达式中使用IS NOT NULL运算符。</li>
</ul>
<p>相关代码如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 1、SQL语句中也可以使用运算表达式</span></span><br><span class="line"><span class="keyword">SELECT</span> product_name, sale_price, sale_price <span class="operator">*</span> <span class="number">2</span> <span class="keyword">AS</span> &quot;sale_price x2&quot;</span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br></pre></td></tr></table></figure>
<p>SELECT 了三列，第三列为算术运算结果 sale_price * 2</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714230044491.png" alt="image-20220714230044491"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 2、WHERE子句的条件表达式中也可以使用计算表达式</span></span><br><span class="line"><span class="keyword">SELECT</span> product_name, sale_price, purchase_price</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">WHERE</span> sale_price <span class="operator">-</span> purchase_price <span class="operator">&gt;=</span> <span class="number">500</span>;</span><br></pre></td></tr></table></figure>
<p>WHERE 相当于在查询的结果上再进行筛选，选出满足条件的</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714230151001.png" alt="image-20220714230151001"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 3、选取出大于&#x27;2&#x27;的数据的SELECT语句(&#x27;2&#x27;为字符串)</span></span><br><span class="line"><span class="keyword">SELECT</span> chr</span><br><span class="line">  <span class="keyword">FROM</span> chars</span><br><span class="line"> <span class="keyword">WHERE</span> chr <span class="operator">&gt;</span> <span class="string">&#x27;2&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>字符串也可以直接比较大小</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714230615130.png" alt="image-20220714230615130"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 4、选取NULL的记录</span></span><br><span class="line"><span class="keyword">SELECT</span> product_name, purchase_price</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">WHERE</span> purchase_price <span class="keyword">IS</span> <span class="keyword">NULL</span>;</span><br><span class="line"><span class="comment">-- 5、选取不为NULL的记录</span></span><br><span class="line"><span class="keyword">SELECT</span> product_name, purchase_price</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">WHERE</span> purchase_price <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure>
<p>等于 NULL 的等于要用 IS，不等于要用 IS NOT</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714230651887.png" alt="image-20220714230651887"></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714230815626.png" alt="image-20220714230815626"></p>
<h2 id="2-3-逻辑运算符"><a href="#2-3-逻辑运算符" class="headerlink" title="2.3 逻辑运算符"></a>2.3 逻辑运算符</h2><h3 id="2-3-1-AND、OR、NOT"><a href="#2-3-1-AND、OR、NOT" class="headerlink" title="2.3.1 AND、OR、NOT"></a>2.3.1 AND、OR、NOT</h3><p>AND、OR、NOT，与或非，和逻辑里的概念一样。注意 AND 运算优先级高于 OR。</p>
<h3 id="2-3-2-真值表"><a href="#2-3-2-真值表" class="headerlink" title="2.3.2 真值表"></a>2.3.2 真值表</h3><p>复杂运算时该怎样理解？</p>
<p>当碰到条件较复杂的语句时，理解语句含义并不容易，这时可以采用<strong>真值表</strong>来梳理逻辑关系。</p>
<p>什么是真值？</p>
<p>本节介绍的三个运算符 NOT、AND 和 OR 称为逻辑运算符。这里所说的逻辑就是对真值进行操作的意思。<strong>真值</strong>就是值为真（TRUE）或假 （FALSE）其中之一的值。</p>
<p>例如，对于 sale_price &gt;= 3000 这个查询条件来说，由于 product_name 列为 ‘运动 T 恤’ 的记录的 sale_price 列的值是 2800，因此会返回假（FALSE），而 product_name 列为 ‘高压锅’ 的记录的sale_price 列的值是 5000，所以返回真（TRUE）。</p>
<p><strong>AND 运算符</strong>两侧的真值都为真时返回真，除此之外都返回假。</p>
<p><strong>OR 运算符</strong>两侧的真值只要有一个不为假就返回真，只有当其两侧的真值都为假时才返回假。</p>
<p><strong>NOT运算符</strong>只是单纯的将真转换为假，将假转换为真。</p>
<p>真值表</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.03true.png"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.03true.png" alt="图片"></a></p>
<p>查询条件为P AND（Q OR R）的真值表</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.04true2.png"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.04true2.png" alt="图片"></a></p>
<h4 id="含有NULL时的真值"><a href="#含有NULL时的真值" class="headerlink" title="含有NULL时的真值"></a>含有NULL时的真值</h4><p>NULL的真值结果既不为真，也不为假，因为并不知道这样一个值。</p>
<p>那该如何表示呢？</p>
<p>这时真值是除真假之外的第三种值——<strong>不确定</strong>（UNKNOWN）。一般的逻辑运算并不存在这第三种值。SQL 之外的语言也基本上只使用真和假这两种真值。与通常的逻辑运算被称为二值逻辑相对，只有 SQL 中的逻辑运算被称为<strong>三值逻辑</strong>。</p>
<p>三值逻辑下的AND和OR真值表为：</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.05true3.png"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.05true3.png" alt="图片"></a></p>
<h2 id="练习题-第一部分（请给出代码、包含代码及代码执行结果的截图）"><a href="#练习题-第一部分（请给出代码、包含代码及代码执行结果的截图）" class="headerlink" title="练习题-第一部分（请给出代码、包含代码及代码执行结果的截图）"></a>练习题-第一部分（请给出代码、包含代码及代码执行结果的截图）</h2><h3 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h3><p>编写一条SQL语句，从 <code>product</code>(商品) 表中选取出“登记日期(<code>regist_date</code>)在2009年4月28日之后”的商品，查询结果要包含 <code>product name</code> 和 <code>regist_date</code> 两列。</p>
<p>答：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> product_name,</span><br><span class="line">       regist_date</span><br><span class="line"><span class="keyword">from</span> product</span><br><span class="line"><span class="keyword">where</span> regist_date<span class="operator">&gt;</span><span class="string">&#x27;2009-04-28&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714231802643.png" alt="image-20220714231802643"></p>
<h3 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h3><p>请说出对product 表执行如下3条SELECT语句时的返回结果。</p>
<p>①空</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">WHERE</span> purchase_price <span class="operator">=</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure>
<p>②空</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">WHERE</span> purchase_price <span class="operator">&lt;&gt;</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure>
<p>③空</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">WHERE</span> product_name <span class="operator">&gt;</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure>
<p>判断某字段是否为空，必须用 IS/IS NOT NULL，而不能用比较运算符。</p>
<h3 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h3><p><code>2.2.3</code> 章节中的SELECT语句能够从 <code>product</code> 表中取出“销售单价（<code>sale_price</code>）比进货单价（<code>purchase_price</code>）高出500日元以上”的商品。请写出两条可以得到相同结果的SELECT语句。执行结果如下所示：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">product_name <span class="operator">|</span> sale_price <span class="operator">|</span> purchase_price </span><br><span class="line"><span class="comment">-------------+------------+------------</span></span><br><span class="line">T恤衫        <span class="operator">|</span> 　 <span class="number">1000</span>    <span class="operator">|</span> <span class="number">500</span></span><br><span class="line">运动T恤      <span class="operator">|</span>    <span class="number">4000</span>    <span class="operator">|</span> <span class="number">2800</span></span><br><span class="line">高压锅       <span class="operator">|</span>    <span class="number">6800</span>    <span class="operator">|</span> <span class="number">5000</span></span><br></pre></td></tr></table></figure>
<p>答：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 1.</span></span><br><span class="line"><span class="keyword">select</span> product_name,</span><br><span class="line">       sale_price,</span><br><span class="line">       purchase_price</span><br><span class="line"><span class="keyword">from</span> product</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">NOT</span> sale_price<span class="operator">-</span>purchase_price<span class="operator">&lt;</span><span class="number">500</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2.</span></span><br><span class="line"><span class="keyword">select</span> product_name,</span><br><span class="line">       sale_price,</span><br><span class="line">       purchase_price</span><br><span class="line"><span class="keyword">from</span> product</span><br><span class="line"><span class="keyword">where</span> sale_price<span class="operator">-</span>purchase_price<span class="operator">&gt;=</span><span class="number">500</span> <span class="keyword">is</span> <span class="literal">TRUE</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-4"><a href="#2-4" class="headerlink" title="2.4"></a>2.4</h3><p>请写出一条SELECT语句，从 <code>product</code> 表中选取出满足“销售单价打九折之后利润高于 <code>100</code> 日元的办公用品和厨房用具”条件的记录。查询结果要包括 <code>product_name</code>列、<code>product_type</code> 列以及销售单价打九折之后的利润（别名设定为 <code>profit</code>）。</p>
<p>提示：销售单价打九折，可以通过 <code>sale_price</code> 列的值乘以0.9获得，利润可以通过该值减去 <code>purchase_price</code> 列的值获得。</p>
<p>答：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span>  product_name,</span><br><span class="line">        product_type,</span><br><span class="line">        sale_price<span class="operator">*</span><span class="number">0.9</span><span class="operator">-</span>purchase_price <span class="keyword">as</span> profit</span><br><span class="line"><span class="keyword">from</span> product</span><br><span class="line"><span class="keyword">where</span> sale_price<span class="operator">*</span><span class="number">0.9</span><span class="operator">-</span>purchase_price<span class="operator">&gt;</span><span class="number">100</span>;</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714233054704.png" alt="image-20220714233054704"></p>
<h2 id="2-4-对表进行聚合查询"><a href="#2-4-对表进行聚合查询" class="headerlink" title="2.4 对表进行聚合查询"></a>2.4 对表进行聚合查询</h2><h3 id="2-4-1-聚合函数"><a href="#2-4-1-聚合函数" class="headerlink" title="2.4.1 聚合函数"></a>2.4.1 聚合函数</h3><p>SQL中用于汇总的函数叫做聚合函数。以下五个是最常用的聚合函数：</p>
<ul>
<li>SUM：计算表中某数值列中的合计值</li>
<li>AVG：计算表中某数值列中的平均值</li>
<li>MAX：计算表中任意列中数据的最大值，包括文本类型和数字类型</li>
<li>MIN：计算表中任意列中数据的最小值，包括文本类型和数字类型</li>
<li>COUNT：计算表中的记录条数（行数）</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 1、计算销售单价和进货单价的合计值</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">SUM</span>(sale_price), <span class="built_in">SUM</span>(purchase_price) </span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br></pre></td></tr></table></figure>
<p>对所有sale_price和purchase_price求和，作为两列：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714233246149.png" alt="image-20220714233246149"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 2、计算销售单价和进货单价的平均值</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">AVG</span>(sale_price), <span class="built_in">AVG</span>(purchase_price)</span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br></pre></td></tr></table></figure>
<p>对所有sale_price和purchase_price求均值，作为两列：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714233328811.png" alt="image-20220714233328811"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 3、计算销售单价的最大值和最小值</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">MAX</span>(sale_price), <span class="built_in">MIN</span>(sale_price)</span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br></pre></td></tr></table></figure>
<p>求出所有sale_price的最大值和purchase_price的最小值，作为两列：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714233451884.png" alt="image-20220714233451884"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 4、MAX和MIN也可用于非数值型数据</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">MAX</span>(regist_date), <span class="built_in">MIN</span>(regist_date)</span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br></pre></td></tr></table></figure>
<p>求出所有注册日期中的最大值和最小值，作为两列：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714233542435.png" alt="image-20220714233542435"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 5、计算全部数据的行数（包含 NULL 所在行）</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br></pre></td></tr></table></figure>
<p>计算product表中的记录行数：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714233643496.png" alt="image-20220714233643496"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 计算 NULL 以外数据的行数</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(purchase_price)</span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br></pre></td></tr></table></figure>
<p>NULL 不计数：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714233734002.png" alt="image-20220714233734002"></p>
<h4 id="DISTINCT-和聚合运算一起使用"><a href="#DISTINCT-和聚合运算一起使用" class="headerlink" title="DISTINCT 和聚合运算一起使用"></a>DISTINCT 和聚合运算一起使用</h4><p>当对整表进行聚合运算时，表中可能存在多行相同的数据，比如商品类型（product_type 列）。</p>
<p>在某些场景下，就不能直接使用聚合函数进行聚合运算了，必须搭配 <code>DISTINCT</code> 函数使用。</p>
<p>比如：要计算总共有几种咖啡类型在售，该怎么计算呢？</p>
<p>如前所述，<code>DISTINCT</code> 函数用于删除重复数据，应用 COUNT 聚合函数之前，加上 <code>DISTINCT</code> 关键字就可以实现需求。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> product_type)</span><br><span class="line">  <span class="keyword">FROM</span> product;</span><br></pre></td></tr></table></figure>
<p>求不同的类别数：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714234318339.png" alt="image-20220714234318339"></p>
<h3 id="2-4-2-聚合函数应用法则"><a href="#2-4-2-聚合函数应用法则" class="headerlink" title="2.4.2 聚合函数应用法则"></a>2.4.2 聚合函数应用法则</h3><ul>
<li>COUNT 聚合函数运算结果与参数有关，COUNT(*) / COUNT(1) 得到包含 NULL 值的所有行，COUNT(&lt;列名&gt;) 得到不包含 NULL 值的所有行。</li>
<li>聚合函数<strong>不处理</strong>包含 NULL 值的行，但是 COUNT(*) 除外。</li>
<li>MAX / MIN 函数适用于文本类型和数字类型的列，而 SUM / AVG 函数仅适用于数字类型的列。</li>
<li>在聚合函数的参数中使用 DISTINCT 关键字，可以得到删除重复值的聚合结果。</li>
</ul>
<h2 id="2-5-对表进行分组"><a href="#2-5-对表进行分组" class="headerlink" title="2.5 对表进行分组"></a>2.5 对表进行分组</h2><h3 id="2-5-1-GROUP-BY语句"><a href="#2-5-1-GROUP-BY语句" class="headerlink" title="2.5.1 GROUP BY语句"></a>2.5.1 GROUP BY语句</h3><p>之前使用聚合函数都是会将整个表的数据进行处理，当你想将进行<strong>分组汇总</strong>时（即：<strong>将现有的数据按照某列来汇总统计</strong>），GROUP BY可以帮助你：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>列名<span class="number">1</span><span class="operator">&gt;</span>,<span class="operator">&lt;</span>列名<span class="number">2</span><span class="operator">&gt;</span>, <span class="operator">&lt;</span>列名<span class="number">3</span><span class="operator">&gt;</span>, ……</span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>表名<span class="operator">&gt;</span></span><br><span class="line"> <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="operator">&lt;</span>列名<span class="number">1</span><span class="operator">&gt;</span>, <span class="operator">&lt;</span>列名<span class="number">2</span><span class="operator">&gt;</span>, <span class="operator">&lt;</span>列名<span class="number">3</span><span class="operator">&gt;</span>, ……;</span><br></pre></td></tr></table></figure>
<p>看一看是否使用GROUP BY语句的差异：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 按照商品种类统计数据行数</span></span><br><span class="line"><span class="keyword">SELECT</span> product_type, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">GROUP</span> <span class="keyword">BY</span> product_type;</span><br><span class="line"> <span class="comment">-- 不含GROUP BY</span></span><br><span class="line"><span class="keyword">SELECT</span> product_type, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714234547139.png" alt="image-20220714234547139" style="zoom:50%;" /></p>
<p>报错原因：在不使用group by 子句的聚合查询中，Select列表中的第一个表达式包含了非聚合的列‘shop.product.product_type’；当sql_mode为only_full_group_by时，是不能出现这种情况的。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714234808432.png" alt="image-20220714234808432"></p>
<p>按照商品种类对表进行切分</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.06cut.png"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.06cut.png" alt="图片"></a></p>
<p>这样，GROUP BY 子句就像切蛋糕那样将表进行了分组。在 GROUP BY 子句中指定的列称为<strong>聚合键</strong>或者<strong>分组列</strong>。</p>
<h4 id="聚合键中包含NULL时"><a href="#聚合键中包含NULL时" class="headerlink" title="聚合键中包含NULL时"></a>聚合键中包含NULL时</h4><p>将进货单价（purchase_price）作为聚合键举例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT purchase_price, COUNT(*)</span><br><span class="line">  FROM product</span><br><span class="line"> GROUP BY purchase_price;</span><br></pre></td></tr></table></figure>
<p>此时会将NULL<strong>作为一组特殊数据</strong>进行聚合运算</p>
<h4 id="GROUP-BY书写位置"><a href="#GROUP-BY书写位置" class="headerlink" title="GROUP BY书写位置"></a>GROUP BY书写位置</h4><p>GROUP BY的子句书写顺序有严格要求，不按要求会导致SQL无法正常执行，目前出现过的子句顺序为：</p>
<ol>
<li>SELECT ➡️ 2. FROM ➡️ 3. WHERE ➡️ 4. GROUP BY</li>
</ol>
<blockquote>
<p>GROUP BY 一定在 WHERE 后面！！</p>
</blockquote>
<p>其中前三项用于筛选数据，GROUP BY对筛选出的数据进行处理</p>
<h4 id="在WHERE子句中使用GROUP-BY"><a href="#在WHERE子句中使用GROUP-BY" class="headerlink" title="在WHERE子句中使用GROUP BY"></a>在WHERE子句中使用GROUP BY</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> purchase_price, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">WHERE</span> product_type <span class="operator">=</span> <span class="string">&#x27;衣服&#x27;</span></span><br><span class="line"> <span class="keyword">GROUP</span> <span class="keyword">BY</span> purchase_price;</span><br></pre></td></tr></table></figure>
<p>查询每种价格的衣服的销量</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714235313436.png" alt="image-20220714235313436"></p>
<h3 id="2-5-2-常见错误"><a href="#2-5-2-常见错误" class="headerlink" title="2.5.2 常见错误"></a>2.5.2 常见错误</h3><p>在使用聚合函数及GROUP BY子句时，经常出现的错误有：</p>
<ol>
<li>在聚合函数的SELECT子句中写了聚合键以外的列使用COUNT等聚合函数时，<strong>SELECT子句中如果出现列名，只能是GROUP BY子句中指定的列名（也就是聚合键）</strong>。</li>
<li>在GROUP BY子句中使用列的别名SELECT子句中可以通过AS来指定别名，但在GROUP BY中不能使用别名。因为在DBMS中 ,SELECT子句在GROUP BY子句后执行。</li>
<li>（？？？）在WHERE中使用聚合函数原因是聚合函数的使用前提是结果集已经确定，而WHERE还处于确定结果集的过程中，所以相互矛盾会引发错误。 如果想指定条件，可以在SELECT，HAVING（下面马上会讲）以及ORDER BY子句中使用聚合函数。</li>
</ol>
<h2 id="2-6-为聚合结果指定条件"><a href="#2-6-为聚合结果指定条件" class="headerlink" title="2.6 为聚合结果指定条件"></a>2.6 为聚合结果指定条件</h2><h3 id="2-6-1-用-HAVING-得到特定分组"><a href="#2-6-1-用-HAVING-得到特定分组" class="headerlink" title="2.6.1 用 HAVING 得到特定分组"></a>2.6.1 用 HAVING 得到特定分组</h3><p>前面学习了如何得到分组聚合结果，现在大家思考一下，如何得到分组聚合结果的部分结果呢？</p>
<p>将表使用 GROUP BY 分组后，怎样才能只取出其中两组？</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220714235742889.png" alt="image-20220714235742889"></p>
<p>这里 WHERE 不可行，因为，WHERE子句只能指定记录（行）的条件，而不能用来指定组的条件（例如，“数据行数为 2 行”或者“平均值为 500”等）。</p>
<p>可以在 GROUP BY 后使用 HAVING 子句。</p>
<p>HAVING 的用法类似 WHERE。</p>
<p>值得注意的是：HAVING 子句必须与 GROUP BY 子句配合使用，且限定的是分组聚合结果，WHERE 子句是限定数据行（包括分组列），二者各司其职，不要混淆。</p>
<blockquote>
<p>WHERE子句 指定记录（行level）的条件</p>
<p>HAVING子句 制定分组聚合（组level）的结果</p>
</blockquote>
<h3 id="2-6-2-HAVING特点"><a href="#2-6-2-HAVING特点" class="headerlink" title="2.6.2 HAVING特点"></a>2.6.2 HAVING特点</h3><p>HAVING子句用于对分组进行过滤，可以使用常数、聚合函数和GROUP BY中指定的列名（聚合键）。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 常数</span></span><br><span class="line"><span class="keyword">SELECT</span> product_type, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">GROUP</span> <span class="keyword">BY</span> product_type</span><br><span class="line"><span class="keyword">HAVING</span> <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="operator">=</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<p>查询类别下个数为2的商品类别</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220715000314859.png" alt="image-20220715000314859"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 错误形式（因为product_name不包含在GROUP BY聚合键中）</span></span><br><span class="line"><span class="keyword">SELECT</span> product_type, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">GROUP</span> <span class="keyword">BY</span> product_type</span><br><span class="line"><span class="keyword">HAVING</span> product_name <span class="operator">=</span> <span class="string">&#x27;圆珠笔&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h2 id="2-7-对查询结果进行排序"><a href="#2-7-对查询结果进行排序" class="headerlink" title="2.7 对查询结果进行排序"></a>2.7 对查询结果进行排序</h2><h3 id="2-7-1-ORDER-BY"><a href="#2-7-1-ORDER-BY" class="headerlink" title="2.7.1 ORDER BY"></a>2.7.1 ORDER BY</h3><p>SQL 语句执行结果<strong>默认随机排列</strong>，想要按照顺序排序，需使用 <strong>ORDER BY</strong> 子句。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>列名<span class="number">1</span><span class="operator">&gt;</span>, <span class="operator">&lt;</span>列名<span class="number">2</span><span class="operator">&gt;</span>, <span class="operator">&lt;</span>列名<span class="number">3</span><span class="operator">&gt;</span>, ……</span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>表名<span class="operator">&gt;</span></span><br><span class="line"> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="operator">&lt;</span>排序基准列<span class="number">1</span><span class="operator">&gt;</span> [<span class="keyword">ASC</span>, <span class="keyword">DESC</span>], <span class="operator">&lt;</span>排序基准列<span class="number">2</span><span class="operator">&gt;</span> [<span class="keyword">ASC</span>, <span class="keyword">DESC</span>], ……</span><br></pre></td></tr></table></figure>
<p>其中，参数 ASC 表示升序排列，DESC 表示降序排列，<strong>默认为升序</strong>。</p>
<p>如下代码将得到按照销售价格倒序排列的查询结果：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 降序排列</span></span><br><span class="line"><span class="keyword">SELECT</span> product_id, product_name, sale_price, purchase_price</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">ORDER</span> <span class="keyword">BY</span> sale_price <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220715000503572.png" alt="image-20220715000503572"></p>
<p>如果有多列排序需求，只需在 ORDER BY 子句中<strong>依次书写排序列</strong> + 排序参数即可，详见如下代码：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 多个排序键</span></span><br><span class="line"><span class="keyword">SELECT</span> product_id, product_name, sale_price, purchase_price</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">ORDER</span> <span class="keyword">BY</span> sale_price, product_id;</span><br></pre></td></tr></table></figure>
<p>先按照 sale_price 排序，如果 sale_price 相同，再按照 product_id 排序，都默认升序。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220715000602322.png" alt="image-20220715000602322"></p>
<p>需要特别说明的是：由于 NULL 无法使用比较运算符进行比较，也就是说，无法与文本类型，数字类型，日期类型等进行比较，当排序列存在 NULL 值时，NULL 结果会展示在查询结果的开头或者末尾。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 当用于排序的列名中含有NULL时，NULL会在开头或末尾进行汇总。</span></span><br><span class="line"><span class="keyword">SELECT</span> product_id, product_name, sale_price, purchase_price</span><br><span class="line">  <span class="keyword">FROM</span> product</span><br><span class="line"> <span class="keyword">ORDER</span> <span class="keyword">BY</span> purchase_price;</span><br></pre></td></tr></table></figure>
<p>在MySQL中，<code>NULL</code> 值被认为比任何 <code>非NULL</code> 值低，所以默认升序时排在开头。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220715000730767.png" alt="image-20220715000730767"></p>
<h3 id="2-7-2-ORDER-BY-子句中使用别名"><a href="#2-7-2-ORDER-BY-子句中使用别名" class="headerlink" title="2.7.2 ORDER BY 子句中使用别名"></a>2.7.2 ORDER BY 子句中使用别名</h3><p>前文讲GROUP BY中提到，GROUP BY 子句中不能使用SELECT 子句中定义的别名，但是在 ORDER BY 子句中却可以使用别名。为什么在GROUP BY中不可以而在ORDER BY中可以呢？</p>
<p>这是因为 SQL 在使用 HAVING 子句时 SELECT 语句的执行顺序为：</p>
<blockquote>
<p><strong>FROM → WHERE → GROUP BY → SELECT → HAVING → ORDER BY</strong></p>
</blockquote>
<p>其中 SELECT 的执行顺序在 GROUP BY 子句之后，ORDER BY 子句之前。</p>
<p>当在 ORDER BY 子句中使用别名时，已经知道了 SELECT 子句设置的别名，但是在 GROUP BY 子句执行时还不知道别名的存在，所以在 ORDER BY 子句中可以使用别名，但是在GROUP BY中不能使用别名。</p>
<h3 id="2-7-3-ORDER-BY-排序列中存在-NULL-时，指定其出现在首行或者末行的方式"><a href="#2-7-3-ORDER-BY-排序列中存在-NULL-时，指定其出现在首行或者末行的方式" class="headerlink" title="2.7.3 ORDER BY 排序列中存在 NULL 时，指定其出现在首行或者末行的方式"></a>2.7.3 ORDER BY 排序列中存在 NULL 时，指定其出现在首行或者末行的方式</h3><p>在MySQL中，<code>NULL</code> 值被认为比任何 <code>非NULL</code> 值低，因此，当顺序为 ASC（升序）时，<code>NULL</code> 值出现在第一位，而当顺序为 DESC（降序）时，则排序在最后。</p>
<p>如果想指定存在 <code>NULL</code> 的行出现在首行或者末行，需要特殊处理。</p>
<p>使用如下代码构建示例表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">user</span> (</span><br><span class="line">    id <span class="type">INT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span> AUTO_INCREMENT,</span><br><span class="line">    name <span class="type">VARCHAR</span>(<span class="number">5</span>),</span><br><span class="line">    date_login <span class="type">DATE</span>,</span><br><span class="line">    <span class="keyword">PRIMARY</span> KEY (id)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">user</span>(name, date_login) <span class="keyword">VALUES</span></span><br><span class="line">(<span class="keyword">NULL</span>,    <span class="string">&#x27;2017-03-12&#x27;</span>), </span><br><span class="line">(<span class="string">&#x27;john&#x27;</span>,   <span class="keyword">NULL</span>), </span><br><span class="line">(<span class="string">&#x27;david&#x27;</span>, <span class="string">&#x27;2016-12-24&#x27;</span>), </span><br><span class="line">(<span class="string">&#x27;zayne&#x27;</span>, <span class="string">&#x27;2017-03-02&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p>既然排序时，<code>NULL</code> 的值比 <code>非NULL</code> 值低（可以理解为 <code>0</code> 或者 <code>-∞</code>），那么我们在排序时就要对这个默认情况进行特殊处理以达到想要的效果。</p>
<p>一般有如下两种需求：</p>
<ul>
<li>将 <code>NULL</code> 值排在末行，同时将所有 <code>非NULL</code> 值按升序排列。</li>
</ul>
<p>对于数字或者日期类型，可以在排序字段前添加一个负号（minus）来得到反向排序。（<code>-1、-2、-3....-∞</code>）</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.07_null_last1.jpg"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.07_null_last1.jpg" alt="图片"></a></p>
<p>对于字符型或者字符型数字，此方法不一定能得到期望的排序结果，可以使用 <code>IS NULL</code> 比较运算符。另外 <code>ISNULL( )</code> 函数等同于使用 <code>IS NULL</code> 比较运算符。</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.07_null_last2.jpg"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.07_null_last2.jpg" alt="图片"></a></p>
<p>还可以使用 <code>COALESCE</code> 函数实现需求</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.07_null_last4.jpg"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.07_null_last4.jpg" alt="图片"></a></p>
<ul>
<li>将 <code>NULL</code> 值排在首行，同时将所有 <code>非NULL</code> 值按倒序排列。</li>
</ul>
<p>对于数字或者日期类型，可以在排序字段前添加一个负号（minus）来实现。（<code>-∞...-3、-2、-1</code>）</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.07_null_first1.jpg"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.07_null_first1.jpg" alt="图片"></a></p>
<p>对于字符型或者字符型数字，此方法不一定能得到期望的排序结果，可以使用 <code>IS NOT NULL</code> 比较运算符。另外 <code>!ISNULL( )</code> 函数等同于使用 <code>IS NOT NULL</code> 比较运算符。</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.07_null_first2.jpg"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.07_null_first2.jpg" alt="图片"></a> <a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.07_null_first2.jpg"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.07_null_first2.jpg" alt="图片"></a></p>
<p>还可以使用 <code>COALESCE</code> 函数实现需求</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.07_null_first4.jpg"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.07_null_first4.jpg" alt="图片"></a></p>
<h2 id="练习题-第二部分（请给出代码、包含代码及代码执行结果的截图）"><a href="#练习题-第二部分（请给出代码、包含代码及代码执行结果的截图）" class="headerlink" title="练习题-第二部分（请给出代码、包含代码及代码执行结果的截图）"></a>练习题-第二部分（请给出代码、包含代码及代码执行结果的截图）</h2><h3 id="2-5"><a href="#2-5" class="headerlink" title="2.5"></a>2.5</h3><p>请指出下述SELECT语句中所有的语法错误。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> product_id, <span class="built_in">SUM</span>(product_name)</span><br><span class="line"><span class="comment">--本SELECT语句中存在错误。</span></span><br><span class="line">  <span class="keyword">FROM</span> product </span><br><span class="line"> <span class="keyword">GROUP</span> <span class="keyword">BY</span> product_type </span><br><span class="line"> <span class="keyword">WHERE</span> regist_date <span class="operator">&gt;</span> <span class="string">&#x27;2009-09-01&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>答：1. GROUP BY 应该在 WHERE 后面；2. SELECT 只能选择 GROUP BY 的聚合键（这几个关键词的执行顺序是：FROM—WHERE—GROUP BY—SELECT）</p>
<h3 id="2-6"><a href="#2-6" class="headerlink" title="2.6"></a>2.6</h3><p>请编写一条SELECT语句，求出销售单价（ <code>sale_price</code> 列）合计值大于进货单价（ <code>purchase_price</code> 列）合计值1.5倍的商品种类。执行结果如下所示。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">product_type | sum  | sum </span><br><span class="line">-------------+------+------</span><br><span class="line">衣服         | 5000 | 3300</span><br><span class="line">办公用品      |  600 | 320</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.08test26.png"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.08test26.png" alt="图片"></a></p>
<p>答： 注意要用 having ，因为是对分组后的结果进行筛选。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> product_type,</span><br><span class="line">       <span class="built_in">sum</span>(sale_price) <span class="keyword">as</span> sum1,</span><br><span class="line">       <span class="built_in">sum</span>(purchase_price) <span class="keyword">as</span> sum2</span><br><span class="line"><span class="keyword">from</span> product</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> product_type</span><br><span class="line"><span class="keyword">having</span> sum1<span class="operator">&gt;</span>sum2<span class="operator">*</span><span class="number">1.5</span>;</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220715002257639.png" alt="image-20220715002257639"></p>
<h3 id="2-7"><a href="#2-7" class="headerlink" title="2.7"></a>2.7</h3><p>此前我们曾经使用SELECT语句选取出了product（商品）表中的全部记录。当时我们使用了 <code>ORDER BY</code> 子句来指定排列顺序，但现在已经无法记起当时如何指定的了。请根据下列执行结果，思考 <code>ORDER BY</code> 子句的内容。</p>
<p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/img/ch02/ch02.09test27.png"><img src="https://github.com/datawhalechina/wonderful-sql/raw/main/img/ch02/ch02.09test27.png" alt="图片"></a></p>
<p>答：观察可以看到是根据 regist_date, purchase_price 的优先级排列的。register_date降序排列，但是null排在了第一个；相同 register_date 按照 purchase_price 升序排列。如果直接对 register_date 降序排列，null会排在最后（null小于所有非null值，可以理解成负无穷）。所以用一些<code>2.7.3</code>的小trick。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> product</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">coalesce</span>(regist_date,<span class="string">&#x27;9999-12-31&#x27;</span>) <span class="keyword">desc</span>,</span><br><span class="line">         purchase_price;</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220715004704161.png" alt="image-20220715004704161"></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://github.com/datawhalechina/wonderful-sql/blob/main/ch02:%20%E5%9F%BA%E7%A1%80%E6%9F%A5%E8%AF%A2%E4%B8%8E%E6%8E%92%E5%BA%8F.md#22">https://github.com/datawhalechina/wonderful-sql/blob/main/ch02:%20%E5%9F%BA%E7%A1%80%E6%9F%A5%E8%AF%A2%E4%B8%8E%E6%8E%92%E5%BA%8F.md#22</a></p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Datawhale组队学习</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Streaming Session-based Recommendation》</title>
    <url>/2022/03/07/SSRM/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220307185213298.png" alt="image-20220307185213298"></p>
<hr>
<p>原paper：<a href="https://dl.acm.org/doi/10.1145/3292500.3330839">Streaming Session-based Recommendation | Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</a></p>
<p>源码解读：未开源</p>
<hr>
<p>中译：流会话推荐</p>
<p>总结：第一篇结合了流推荐和会话推荐的论文（准备开坑）。主要解决两个问题，MF attention + GRU  解决用户行为的不确定性；存储技术+主动采样策略 解决了更贴近实时场景的“高速、海量、连续的流数据”的需求。个人认为可以进一步做的地方：session encoder部分，用新模型；储存技术；采样技术。</p>
<hr>
<span id="more"></span>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>question作者想解决什么问题？ </p>
<p>1）用户行为的不确定性。</p>
<p>2）会话推荐在实际场景中会以会话流的形式出现，即连续不断的、海量的、高速的会话数据，但现有会话推荐模型都是离线模型，没有模型可以解决这个问题。</p>
</li>
<li><p>method作者通过什么理论/模型来解决这个问题？</p>
<p>作者提出SSRM(Streaming Session-based Recommendation Machine)模型，其中</p>
<p>1）为了解决用户行为的不确定性，作者利用历史交互，提出基于矩阵分解的注意力模型。</p>
<p>2）为了解决流会话数据“海量”、“高速”的挑战，作者基于存储提出使用主动采样策略的流模型。</p>
</li>
<li><p>answer作者给出的答案是什么？</p>
<p>在LastFM和Gowalla数据集上证明了SOTA。</p>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>第一篇提出流会话推荐模型的论文。</p>
<ul>
<li><p>why作者为什么研究这个课题？    </p>
<p>“流会话”(Streaming session)的设定更贴近实际。</p>
</li>
<li><p>how当前研究到了哪一阶段？</p>
<p>第一篇提出流会话推荐模型的论文。</p>
</li>
<li><p>what作者基于什么样的假设（看不懂最后去查）？</p>
<p>1）用户的历史交互信息是可获得的</p>
<p>2）背景是流会话的设定，即会话数据海量、连续不断、迅速地迭代。</p>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 </li>
</ul>
<p>LastFM：<a href="http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz">http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz</a></p>
<p>Gowalla：<a href="https://snap.stanford.edu/data/loc-gowalla.html">https://snap.stanford.edu/data/loc-gowalla.html</a></p>
<ul>
<li>数据划分</li>
</ul>
<p>给数据集 $D$ 中的会话按时间排序，分成前60%作为训练集，和后40%作为候选集。为了模拟线上的流数据输入，将候选集再划分成5个等长切片作为测试机。第一个测试机和10%的训练集作为验证集。实验中，若要预测第 $i$ 个测试集的序列行为，那么 $i$ 之前的测试集切片都用作在线训练。</p>
<ul>
<li>重要指标 </li>
</ul>
<p>MRR@20、Recall@20</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li><h3 id="SSRM模型框架"><a href="#SSRM模型框架" class="headerlink" title="SSRM模型框架"></a>SSRM模型框架</h3></li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220307191209899.png" alt="image-20220307191209899"></p>
<p>SSRM的主要工作是，建立一个基于注意力的会话推荐系统（离线模型），再将其拓展到流会话的设定。</p>
<ul>
<li><h3 id="离线模型：基于历史行为的注意力编码器"><a href="#离线模型：基于历史行为的注意力编码器" class="headerlink" title="离线模型：基于历史行为的注意力编码器"></a>离线模型：基于历史行为的注意力编码器</h3></li>
</ul>
<p>离线模型 = 建模序列 + 矩阵分解</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220307193225545.png" alt="image-20220307193225545"></p>
<p>其中，建模“序列” = 基础会话编码器 + 基于矩阵分解的注意力会话编码器。这两部分如下1.2.：</p>
<ol>
<li><strong>基础会话编码器(Basic Session Encoder)</strong></li>
</ol>
<p>主要为了建模当前会话的表示。采用基本的GRU模型（重置门、更新门、候选状态），将最后一个隐藏状态作为当前会话 $i$ 的基本表示： $c_i = h_t$ 。</p>
<ol>
<li><strong>基于矩阵分解的注意力会话编码器(MF-based Attentive Session Encoder)</strong></li>
</ol>
<p>MF分解得到每个用户的隐含表示 $p_u \in \R ^ {1 \times D} $ 和每个物品的隐含表示 $q_t \in \R ^ {1 \times D} $ 。MF既用来单独作为一个模块，还通过引入MF（利用了历史交互信息）来加强上面的基础模型。</p>
<p>$p_u $ 和 $q_t$ 的内积  $\hat y_{u,t}^R = <p_u,q_t>=p_u \cdot q_t$ 用来表示用户 $u$ 有多“喜欢”物品 $i$ 。这里的 $q_t$ 和上面的 $h_t$ 是物品 $t$ 的两种表示，作用、含义不同： $h_t$ ，即GRU在物品 $t$ 处的隐含表示，是总结了序列行为(1,2,…,t)的物品表示；而 $q_t$ 作为实际的物品表示，可用于和用户隐含表示做内积，表明用户是否喜欢这个物品。</p>
<p>为了降低随机性、捕捉用户 $u$ 的主要意图，这里使用了注意力机制编码会话表示 $c_i$：</p>
<script type="math/tex; mode=display">
c_i = [\ \sum^t_{j=1}\alpha_{u,j}\cdot h_j \ ; h_t \ ]</script><p>其中权重 $\alpha_u = softmax(\hat y_u^R)$ ，即对用户 $u$ 对序列中物品的“喜欢”程度的打分进行softmax，得到和为1的权重。求到加权的表示后，再与GRU得到的会话表示 $h_t$ 拼接，作为最终的会话表示（这里 $h_t$ 用到了两次，一次作为最后一个物品表示，一次作为会话表示）。</p>
<ol>
<li><strong>混合注意力推荐系统</strong></li>
</ol>
<p>SSRM再进一步将注意力编码器的输出和MF的输出结合，使得模型不仅考虑当前会话的序列行为，还考虑了用户的长期兴趣和共现行为。</p>
<p>会话编码器最后输出，每个会话对每个物品的打分： $\hat y_{i,t}^S = q_t B c_i^T $  ，表示第 $i$ 个会话对物品 $t$ 的i打分，其中 $q_t \in \R ^ {1 \times D} $ ， $c_i \in \R ^ {1 \times H} $ ，变换矩阵 $B \in \R ^ {D \times H} $ 。</p>
<p>由MF得到的 $p_u$ 和 $q_t$  的内积  $\hat y_{u,t}^R =p_u \cdot q_t$ 。</p>
<p>两者使用一个可调参数 $w$ 加权求和  $\hat y_{i,t} = w \hat y_{i,t}^R + (1-w)\hat y_{i,t}^S $ 。</p>
<p>最后，把物品排序任务当作分类任务，使用CE loss损失函数训练模型。 $r_u$ 是物品的真实标签分布， $\hat y_u$ 是预测的概率分布。</p>
<script type="math/tex; mode=display">
L(r_u,\hat y_u) = - \sum^n_{i=1} r_{u,i} \cdot log(\hat y_{u,i})</script><ul>
<li><h3 id="在线训练：基于存储的、使用主动采样策略的流模型"><a href="#在线训练：基于存储的、使用主动采样策略的流模型" class="headerlink" title="在线训练：基于存储的、使用主动采样策略的流模型"></a>在线训练：基于存储的、使用主动采样策略的流模型</h3></li>
</ul>
<p>建立完基于注意力的会话推荐系统（离线模型）后，再将其拓展到流会话的设定。建立存储的目的是准确地概括总结历史行为。</p>
<p><strong>1.传统模型的更新方法</strong></p>
<p>随机采样技术。令 $C$ 为保存会话序列的存储库，令 $t$ 为下一个时刻即将到达的数据实例。当 $t&gt;|C|$ 时，存储库会以 $\frac{|C|}{t} $ 的概率存储这个数据实例，同时随机替换掉存储库 $C$ 里的数据实例。这样得到的存储库可以证明相当于当前数据集的随机采样结果，同时也证明可以保留模型的长期记忆[1]。但是 $\frac{|C|}{t} $ 是随时间递减的，这样模型就会倾向于忽略最近的数据，因为越近的数据被选入存储库的概率越低。而实际场景中是存在用户兴趣漂移的现象的，换句话说，使用这种随机采样技术难以实现“捕捉最新产生的数据中的行为模式”。</p>
<p><strong>2.主动采样策略</strong></p>
<p>虽然更新时同时使用整个存储库和所有新到达数据能产生更好结果，但是由于高速的流数据和有限的计算资源，这种方式通常会导致可利用的数据非常少（这个问题也被称为系统过载）。所以需要一个明智的样本选择策略。</p>
<p>本文提出的主动采样策略的思想和一些主动学习（Active learning）类似，即“选择对系统贡献最大的样本的最小集合”，供用户评估。具体来说，为了使模型在有限时间窗口内尽可能地多学习，采样策略每次应该选择 $C^{new}\cup C$ 中<strong>信息量最大</strong>的会话实例。</p>
<p>计算会话的信息量，使用会话中每个物品得分的均值，其中物品得分 $r_{u,k}$ 表示用户 $u$ 在当前模型下对物品 $k$ 的预测能力，使用用户隐含表示和物品隐含表示做内积得到 $r_{u,k}=p_u q_k$。$r_{u,k}$ 值越小， $r_{s_i}$ 值越小，说明模型越难以预测该会话中的物品，说明该会话信息量越大，越能修正模型，越该对它进行采样。</p>
<script type="math/tex; mode=display">
r_{s_i}=\frac{\sum^t_{k=1}r_{u,k}}{t}</script><p>以 $r_{s_i}$ 值对这些会话降序排列，再根据排名计算每个会话的权重因子 $w_{s_i}$，最后得到每个会话的采样概率 $p(s_i)$ ：</p>
<script type="math/tex; mode=display">
w_{s_i}=exp(\frac{rank_{s_i}}{C\cup C^{new}})
;\\ 
p(s_i)=\frac{w_{s_i}}{\sum_{s_i \in C \cup C^{new}}w_{s_i}}</script><p>其中，信息量越大的会话排名越靠后，权重因子越高，采样概率也越大。</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><ul>
<li><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3></li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308193608578.png" alt="image-20220308193608578"></p>
<ul>
<li><h3 id="和SBR的经典方法比较"><a href="#和SBR的经典方法比较" class="headerlink" title="和SBR的经典方法比较"></a>和SBR的经典方法比较</h3></li>
</ul>
<p>SOTA。LastFM的提升率比Gowalla高的主要原因是，Gowalla更系数，每个用户的点击很少，并且会话数据只有寥寥几个，从而导致训练不充分。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308193302601.png" alt="image-20220308193302601"></p>
<ul>
<li><h3 id="验证矩阵分解注意力机制的有效性（MF-Attention）"><a href="#验证矩阵分解注意力机制的有效性（MF-Attention）" class="headerlink" title="验证矩阵分解注意力机制的有效性（MF-Attention）"></a>验证矩阵分解注意力机制的有效性（MF-Attention）</h3></li>
</ul>
<p>Baseline：只有基础会话编码器（Basic session encoder），好像就是GRU4Rec。三个模型采用相同的streaming技术，以消除线上更新方法带来的影响。SSRM效果最好。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308193902198.png" alt="image-20220308193902198"></p>
<ul>
<li><h3 id="不同流策略的影响"><a href="#不同流策略的影响" class="headerlink" title="不同流策略的影响"></a>不同流策略的影响</h3></li>
</ul>
<p>S1：没有存储库，且仅用新来的数据更新模型。S2：有存储库，采用从 $C$ 中随机采样训练的传统方法。S3：S2的基础上，从 $C\cup C^{new}$ 中采样。S1、S3优于S2，说明相比于历史long-term memory，模型更能从用户最近行为中受益。而SSRM模型优于其它所有，说明本文提出的主动采样策略是有效的。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308215603572.png" alt="image-20220308215603572"></p>
<ul>
<li><h3 id="不同过载设定的影响"><a href="#不同过载设定的影响" class="headerlink" title="不同过载设定的影响"></a>不同过载设定的影响</h3></li>
</ul>
<p>$W$ ：固定时间窗口，即一次能从 $C\cup C^{new}$ 中采样的样本数。 $W$ 越小说明工作负载越重，只训练到有限的序列。横轴看，窗口越大，过载越轻，模型表现越好；反之窗口越大，过载越重，模型表现越差。纵向看不同测试集，因为这些测试集是按时间顺序分的，越往后新用户、新物品越多。但是可以发现，从test2到test5，它们之间的gap越来越小。SSRM能够快速减小这种gap，证明其处理新数据的能力。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308215631714.png" alt="image-20220308215631714"></p>
<ul>
<li><h3 id="存储库大小的影响"><a href="#存储库大小的影响" class="headerlink" title="存储库大小的影响"></a>存储库大小的影响</h3><p>$|C|$ 的大小决定了保留多少历史信息，它保留的观测行为越多，就越可能从历史行为中采样，就会有更多的样本距离current behavior越久远，故模型就会用更少的当前会话信息来更新。换句话说，对于过去没有出现的用户和项目，模型会学习得更少，这将导致性能相对较差。Figure 6 里也能得出这个结论，最近的行为更重要。但是因为存在long-memory problem，如果只用当前session来训练模型的话，结果会变低，总的来说，历史行为和当前行为两者得结合起来。</p>
</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308194605932.png" alt="image-20220308194605932"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1]Ernesto Diaz-Aviles, Lucas Drumond, Lars Schmidt-Thieme, and Wolfgang Nejdl. 2012. Real-time top-n recommendation in social streams. RecSys (2012)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>流会话推荐</tag>
        <tag>SSRM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Time Interval Aware Self-Attention for Sequential Recommendation》</title>
    <url>/2021/11/18/TiSASRec/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117164643993.png" alt=""></p>
<hr>
<p>原paper：<a href="https://dl.acm.org/doi/10.1145/3336191.3371786">https://dl.acm.org/doi/10.1145/3336191.3371786</a></p>
<p>源码解读：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec</a></p>
<hr>
<p>中译：时间间隔感知的自注意力序列推荐</p>
<p>总结：是SASRec工作的延续，在self-attention的基础上加了绝对位置信息和相对时间间隔信息（加在Q和K里）取得了更好的performamce。发现Beauty数据集序列模式不明显。</p>
<span id="more"></span>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><strong>question作者想解决什么问题？</strong></li>
</ul>
<p>MC模型和RNN模型都只将用户交互作为有序序列（一种强假设），却没有考虑交互与交互之间的时间间隔。</p>
<ul>
<li><strong>method作者通过什么理论/模型来解决这个问题？</strong></li>
</ul>
<p>在序列模型的结构中显式建模交互的时间戳（timestamps），并且探索不同时间间隔对next item推荐的影响。提出TiSASRec模型，模型建模了item在序列中的绝对位置以及交互之间的时间间隔。</p>
<ul>
<li><strong>answer作者给出的答案是什么？</strong></li>
</ul>
<p>展示了不同设定下TiSASRec的特点，比较了不同位置编码下自注意力模块的表现。在dense和sparse数据集都取得了SOTA。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><strong>why作者为什么研究这个课题？</strong></li>
</ul>
<p>Temporal recommendation（实时推荐）主要建模“绝对时间”来捕获用户与物品的实时动态，即挖掘实时模式、依据时间建模。Sequential recommendation（序列推荐）主要依据交互的顺序挖掘序列模式。序列推荐只用timestamps来决定item顺序，其实假设了所有交互之间是等间隔的。但一天之内产生的序列和一个月内产生的序列显然对next item的影响区别很大。</p>
<ul>
<li><strong>how当前研究到了哪一阶段</strong></li>
</ul>
<p>目前的序列推荐只挖掘序列模式，即假设交互之间是等间隔的，不合理。有模型使用自注意力+相对位置编码[1]，受到启发。</p>
<ul>
<li><strong>what作者基于什么样的假设（看不懂最后去查）</strong></li>
</ul>
<p>交互序列应该被建模为包含时间间隔的序列。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li><p><strong>优点</strong></p>
<ul>
<li>结合了<strong>绝对位置编码</strong>和<strong>相对时间间隔</strong>编码的优点。</li>
<li>证明了使用相对时间间隔的有效性。</li>
</ul>
</li>
<li><p><strong>缺点</strong></p>
</li>
<li><p><strong>展望</strong></p>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li><p><strong>数据来源</strong></p>
<ul>
<li>MovieLens-1m</li>
<li>Amazon CDs&amp;Vinyl/ Movies&amp;TV/ Beauty/ Games</li>
<li>Steam</li>
</ul>
</li>
<li><p><strong>重要指标</strong></p>
<ul>
<li>Hit@10、NDCG@10</li>
</ul>
</li>
</ul>
<h2 id="Method-amp-Table"><a href="#Method-amp-Table" class="headerlink" title="Method &amp; Table"></a>Method &amp; Table</h2><ul>
<li>模型架构</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211116223143274.png" alt=""></p>
<ul>
<li>参数说明</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211116223530787.png" alt="image-20211116223530787"></p>
<p><strong>1.个性化的时间间隔（time interval）</strong></p>
<p>规定了序列$S$的maxlen（n），长度小于n的序列用一个特殊标记的padding item来padding。时间序列$T$用第一个item的timestamps来padding（到这里还只是时间戳）。</p>
<p>对每个用户制定个性化的时间间隔，个性化指的其实就是下面介绍的缩放操作。对用户$u$来说，时间戳序列$t=(t_1,t_2,…,t_n)$，用任意两个物品的时间戳之差表示物品之间的时间间隔，作为任意两个物品之间的关系（relation）$r_{ij}$，于是得到时间间隔集合$R^u$。规定一个缩放系数$r^u_{min}=min(R^u)$，即序列里的最小时间间隔，再对所有时间间隔缩放$r^u_{ij}=\lfloor\frac{|r_i-r_j|}{r^u_{min}}\rfloor$，得到时间间隔矩阵$M^u\in N^{n\times n}$。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211116225152019.png" alt="image-20211116225152019"></p>
<p>另外，论文还规定了每个$r^u_{ij}$的阈值，对大于阈值的做了一个clip操作得到$M^u_{clipped}$。</p>
<p><strong>2.Embedding层</strong></p>
<ul>
<li>item的表示：padding item用$\vec0$表示，其它每个item用d维向量表示，构成$M^I\in R^{|I|\times d}$的item embedding矩阵，则前n个item的表示为$E^I\in R^{n\times d}$。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117222616679.png" alt="image-20211117222616679"></p>
<ul>
<li>position 的表示：即位置编码，用两个K、V矩阵$M^P_K\in R^{n\times d}$和$M^P_V\in R^{n\times d}$，表示每个位置（序列最大长度为n）的Key和Value向量，分别为$E^P_K\in R^{n\times d}$和$E^P_V\in R^{n\times d}$。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117081419839.png" alt="image-20211117081419839"></p>
<ul>
<li>relative time interval的表示：和positional embedding相似，用两个K、V矩阵$E^P_K\in R^{k\times d}$和$E^P_V\in R^{k\times d}$，表示每个位置（序列最大长度为n）的Key和Value向量，其中k表示一共有k种相对时间间隔。于是clipped后的$M^u_{clipped}$，把对应的$r_{ij}$替换成对应的K、V向量，就得到了$E^R_K\in R^{n\times n\times d}$和$E^R_V\in R^{n\times n\times d}$。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117081413089.png" alt="image-20211117081413089"></p>
<p><strong>3.时间间隔感知的自注意力机制</strong></p>
<p>仅有item和对应的时间戳也不能把序列确定下来，还要加入item在序列中的位置。</p>
<ul>
<li><strong>时间间隔感知的自注意力层Time Interval-Aware Self-attention Layer</strong></li>
</ul>
<p>传统的自注意力层为QKV模式，可以定义成$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$。用item embedding乘以$W^Q、W^K、W^V$投影到其对应的$Querry、Key、Value$空间上。</p>
<p>这里本质上也是这么做的，但对K和V做了一点改变。</p>
<p>作者首先将$E^I=(m_{s_1},m_{s_2},…,m_{s_n})$表示的item序列变换新序列$Z=(z_1,z_2,…,z_n)$，$z_i\in R^d$。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117222727853.png" alt="image-20211117222727853"></p>
<p>其中$(m_{s_j}W^V+r_{ij}^v+p_j^v)$<strong>对应QKV模式里的$Value$</strong>，其中$m_{s_j}W^Q$是将item embedding投影到$Value$空间，不过在此基础上还加上了realation embedding（相对时间间隔）和position embedding（位置编码）的value表示。</p>
<p>找到$Value$以后，公式就可以写成：$z_i=\sum^n_{j=1}\alpha_{ij}\ Value_j$。</p>
<p>系数$\alpha_{ij}$是其实就是$softmax(\frac{QK^T}{\sqrt{d_k}})$部分。$softmax()$在论文中体现在$\alpha_{ij}=\frac{exp\ e_{ij}}{\sum^n_{k=1}exp\ e_{ik}}$，那么可以猜测$\frac{QK^T}{\sqrt{d_k}}$就对应论文中的$e_{ij}$了。事实正如此，$e_{ij}$被定义为：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117090739720.png" alt="image-20211117090739720"></p>
<p>其中$m_{s_j}W^Q$是将item embedding投影到$Querry$空间，<strong>对应QKV模式里的$Querry$</strong>。</p>
<p>$(m_{s_j}W^K+r^k_{ij}+p^k_j)$，<strong>对应QKV模式里的$Key$</strong>，其中$m_{s_j}W^K$是将item embedding投影到$Key$空间，不过在此基础上还加上了realation embedding（相对时间间隔）和position embedding（位置编码）的key表示，另外除以的$\sqrt{d}$是缩放系数。</p>
<ul>
<li><strong>因果关系Causality</strong></li>
</ul>
<p>序列本身就有因果关系，因为我们在预测第t+1个物品时，只知道前t个物品的信息。但是在做self-attention时，每个物品都能感知到所有物品（因为Q对所有K做了查询），破坏了因果关系。所以我们必须规定，在做self-attention时，规定每个$Q_i$只能查询$K_j$，其中$j&lt;i$，即每个Q只能查询在其之前（previous）的K，满足了因果关系，代码里可以用mask实现。</p>
<ul>
<li><strong>前馈层Point-wise Feef-Forward Network</strong></li>
</ul>
<p>FFN为模型加入非线性性。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117093429227.png" alt="image-20211117093429227"></p>
<p>Residual connection和dropout正则化。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117093655881.png" alt="image-20211117093655881"></p>
<p>Layer Norm正则化。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117093809122.png" alt="image-20211117093809122"></p>
<p><strong>4.预测层</strong></p>
<p>常规的点积计算每个物品的得分。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117094112160.png" alt="image-20211117094112160"></p>
<p><strong>5.模型训练</strong></p>
<p>取物品序列$\widetilde{S_{|S^u|}}=(S^u_1,S^u_2,…,S^u_{|S^u|})$和对应的时间序列$\widetilde{T_{|T^u|}}=(T^u_1,T^u_2,…,T^u_{|T^u|})$的前$|S^u|-1$项，即$\widetilde{S_{|S^u|-1}}=(S^=u_1,S^u_2,…,S^u_{|S^u|-1})$和$\widetilde{T_{|T^u|-1}}=(T^u_1,T^u_2,…,T^u_{|T^u-1|})$。通过裁剪和补长各自化成成相同长度n的两个序列$s=(s_1,s_2,…,s_n)$，和$t=(t_1,t_2,…,t_n)$。给定这两个序列，再规定对应的输出序列$o=(o_1,o_2,…,o_n)$，其中$o_i$定义为：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117095415283.png" alt="image-20211117095415283"></p>
<p>简而言之，padding项的输出为\<pad>；$s$最后一项之前的输出（预测）为下一项；$s$最后一项的输出（预测）为$S^u_{|S^u|}$，注意$S^u_{|S^u|}$不在$s$里，因为一开始就把最后一项拿出来了。</p>
<p>loss采用进行负采样的binary cross entropy，加入了F正则项：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117100703091.png" alt="image-20211117100703091"></p>
<p>padding项也计算了loss，但是没有意义，所以实际计算时把padding项的loss mask掉。</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><p><strong>1.模型表现</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117101056812.png" alt="image-20211117101056812"></p>
<ul>
<li>TiSASRec在6个数据集上达到了SOTA</li>
</ul>
<p><strong>2.Ablation Study</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117103132511.png" alt="image-20211117103132511"></p>
<ul>
<li>TiSASRec-R去掉了在K和V里去掉了position embedding（绝对位置）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117103534252.png" alt="image-20211117103534252"></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117103553274.png" alt="image-20211117103553274"></p>
<ul>
<li>SASRec去掉了relative time interval（relation，相对时间间隔）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117114708312.png" alt="image-20211117114708312"></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117114729528.png" alt="image-20211117114729528"></p>
<ul>
<li>结果表明保留绝对位置和相对时间间隔时model performance最好</li>
</ul>
<p><strong>3.超参数实验</strong></p>
<p><strong>A.隐向量维度d</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117104027269.png" alt="image-20211117104027269"></p>
<ul>
<li>不同模型在不同数据集（<em>除了Games和Steam，why？</em>）上选择d={10，20，30，40，50}</li>
<li>基本上所给模型在所给数据集上都是d越大越好</li>
<li>在Beauty数据集比较特殊，MARank、Caser、TransRec的表现随着d增大在变差</li>
<li>所以最后选d=50</li>
</ul>
<p><strong>B. 序列最大长度n</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117104705615.png" alt="image-20211117104705615"></p>
<ul>
<li>n越大效果越好，并且在这两个数据集上SASRec表现比TiSASRec差且更快收敛。</li>
<li>所以最后选n=50</li>
<li><em>疑问：只选了MovieLens和Amazon CD&amp;Vinyl做实验，why？SASRec论文里选MovieLens-1m做实验的时候maxlen选的可是200，且performance比TiSASRec选50时好….这篇论文maxlen选的最大才50，why？</em></li>
</ul>
<p><strong>C.最大时间间隔k</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117105425551.png" alt="image-20211117105425551"></p>
<ul>
<li>k越大意味着要训练的参数越多</li>
<li>TiSASRec整体上更稳定，TiSASRec-R当k取合适时表现最好，但当k更大时表现变差。</li>
<li><em>疑问：ml-1m上比较稳定且permformance在提升，到最大值2048。但CD&amp;Vinyl上最好表现是k=256，但论文最后选的k=512</em></li>
</ul>
<p><strong>4.个性化时间间隔实验</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117110513024.png" alt="image-20211117110513024"></p>
<ul>
<li>Method（1）直接用时间戳作为特征，Method（2）使用没缩放的时间间隔，Method（3）使用个性化（根据每个用户最小时间间隔缩放后的）的时间间隔，即论文方法。</li>
<li>注意前两个方法没有使用时间戳裁剪 timestamps clip</li>
<li>Method（3）的performence最好</li>
</ul>
<p><strong>5.可视化</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117111955529.png" alt="image-20211117111955529"></p>
<ul>
<li>Figure 7 表明预测时使用时间间隔产生的推荐不一样，而且好像更准确</li>
<li>Figure 8 是不同时间间隔的权重可视化<ul>
<li>小时间间隔的权重更大，说明更短期交互的物品对预测结果影响更大</li>
<li>(a)MovieLens是dense数据集，(b) CDs&amp;Vinyl是sparse数据集。左边绿的区域更大，说明dense数据集上预测需要更大范围的物品。</li>
<li>Amazon Beauty数据集没有明显的黄绿区域，说明这个数据集没有明显的序列模式，这也说明了为什么有些序列模型在该数据集上效果不是很好。</li>
</ul>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://aclanthology.org/N18-2074/">[1]Self-Attention with Relative Position Representations</a>)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>序列推荐</tag>
        <tag>自注意力</tag>
        <tag>TiSASRec</tag>
      </tags>
  </entry>
  <entry>
    <title>TiSASRec代码笔记</title>
    <url>/2021/11/22/TiSASRec%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<hr>
<p>完整的代码注释：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec</a></p>
<p>论文笔记：<a href="https://guadzilla.github.io/2021/11/18/TiSASRec/">https://guadzilla.github.io/2021/11/18/TiSASRec/</a></p>
<hr>
<h2 id="squeeze-unsqueeze-repeat-expand"><a href="#squeeze-unsqueeze-repeat-expand" class="headerlink" title="squeeze, unsqueeze, repeat ,expand"></a>squeeze, unsqueeze, repeat ,expand</h2><p><strong>torch.squeeze(input,dim,*,out) —&gt;Tensor</strong></p>
<blockquote>
<p><em>squeeze：挤压，捏</em></p>
</blockquote>
<p>与unsqueeze操作相反，在指定dim处加入一维，如果dim未指定，则所有为1的维度去掉。</p>
<p><strong>torch.unsqueeze(input,dim) —&gt; Tensor</strong></p>
<blockquote>
<p><em>unsqueeze：挤压的反义词，膨胀</em></p>
</blockquote>
<p>与squeeze操作相反，返回一个新张量，在原来张量的指定dim处加入一维。</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],		<span class="comment"># x.shape=(2,4) ,有三处可以插入维度 _,1,_,4,_</span></span><br><span class="line">                  [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]]) 	</span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">0</span>).shape			<span class="comment"># (_,2,_,4,_),在第0维度（最左边）插入1维 = (1,2,4)</span></span><br><span class="line"><span class="comment"># torch.Size([1, 2, 4])</span></span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">1</span>).shape			<span class="comment"># (_,2,_,4,_),在第1维度（中间的）插入1维 = (2,1,4)</span></span><br><span class="line"><span class="comment"># torch.Size([2, 1, 4])</span></span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">2</span>).shape			<span class="comment"># (_,2,_,4,_),在第2维度（最右边）插入1维 = (2,1,4)</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 1])</span></span><br><span class="line"></span><br><span class="line">y = torch.unsqueeze(x, -<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)		<span class="comment"># 在最后填两个为1的维度</span></span><br><span class="line">y.shape</span><br><span class="line"><span class="comment"># torch.Size([2, 4, 1, 1])		</span></span><br><span class="line">y.squeeze().shape					<span class="comment"># squeeze不指定dim，会去掉所有size=1的维度</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4])</span></span><br></pre></td></tr></table></figure>
<p><strong>torch.repeat(*size)</strong></p>
<p>沿着指定的维度重复这个张量。类似numpy.tile()，地板铺（把tensor当成一块地板，按形状铺）。</p>
<p><strong>torch.expand(*sizes)</strong></p>
<p>将单个维度<strong>拓展</strong>成更大维度，和repeat不一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">x</span><br><span class="line"><span class="comment"># tensor([1, 2, 3])</span></span><br><span class="line">x.repeat(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],		# x作为地板，被重复铺了(2,3)次</span></span><br><span class="line"><span class="comment">#         [1, 2, 3, 1, 2, 3, 1, 2, 3]])</span></span><br><span class="line">x.expand(<span class="number">2</span>,<span class="number">3</span>)		</span><br><span class="line"><span class="comment"># tensor([[1, 2, 3],						# x被拓展成(2,3)</span></span><br><span class="line"><span class="comment">#         [1, 2, 3]])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>实际代码：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time_mask = time_mask.unsqueeze(-<span class="number">1</span>).repeat(self.head_num, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 1.unsqueeze()：time_mask.shape=(batch_size,maxlen) ——&gt; (batch_size,maxlen,1)，最后一个维度填1</span></span><br><span class="line"><span class="comment"># 2.repeat():(batch_size,maxlen,1) ——&gt; (self.head_num*batch_size,maxlen,1),第一个维度乘倍数</span></span><br><span class="line">time_mask = time_mask.expand(-<span class="number">1</span>, -<span class="number">1</span>, attn_weights.shape[-<span class="number">1</span>])	<span class="comment"># 这里attn_weights.shape[-1]=maxlen</span></span><br><span class="line"><span class="comment"># 3.(self.head_num*batch_size,maxlen,1) ——&gt;(self.head_num*batch_size,maxlen,maxlen)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">attn_mask = attn_mask.unsqueeze(<span class="number">0</span>).expand(attn_weights.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"><span class="comment"># (maxlen,maxlen) ——&gt; (1,maxlen,maxlen) ——&gt; (batch_size,maxlen,maxlen)</span></span><br></pre></td></tr></table></figure>
<h2 id="手动多头注意力"><a href="#手动多头注意力" class="headerlink" title="手动多头注意力"></a>手动多头注意力</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeAwareMultiHeadAttention</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># required homebrewed mha layer for Ti/SASRec experiments</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size, head_num, dropout_rate, dev</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TimeAwareMultiHeadAttention, self).__init__()</span><br><span class="line">        self.Q_w = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.K_w = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.V_w = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = torch.nn.Dropout(p=dropout_rate)</span><br><span class="line">        self.softmax = torch.nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.head_num = head_num</span><br><span class="line">        self.head_size = hidden_size // head_num</span><br><span class="line">        self.dropout_rate = dropout_rate</span><br><span class="line">        self.dev = dev</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, time_mask, attn_mask, time_matrix_K, time_matrix_V, abs_pos_K, abs_pos_V</span>):</span></span><br><span class="line">        <span class="comment"># time_mask: padding item的mask,     attn_mask: 为了causality的mask,下三角</span></span><br><span class="line">        Q, K, V = self.Q_w(queries), self.K_w(keys), self.V_w(keys)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># head dim * batch dim for parallelization (h*N, T, C/h)</span></span><br><span class="line">        <span class="comment"># 即(batch_size, maxlen, hidden_units) ----&gt; (batch_size*3, maxlen, hidden_units/3)</span></span><br><span class="line">        <span class="comment">#   (batch_size, maxlen, maxlen, hidden_units) ----&gt; (batch_size*3, maxlen, maxlen, hidden_units/3)</span></span><br><span class="line">        Q_ = torch.cat(torch.split(Q, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">        K_ = torch.cat(torch.split(K, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">        V_ = torch.cat(torch.split(V, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        time_matrix_K_ = torch.cat(torch.split(time_matrix_K, self.head_size, dim=<span class="number">3</span>), dim=<span class="number">0</span>)</span><br><span class="line">        time_matrix_V_ = torch.cat(torch.split(time_matrix_V, self.head_size, dim=<span class="number">3</span>), dim=<span class="number">0</span>)</span><br><span class="line">        abs_pos_K_ = torch.cat(torch.split(abs_pos_K, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">        abs_pos_V_ = torch.cat(torch.split(abs_pos_V, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># batched channel wise matmul to gen attention weights  ---公式（8）</span></span><br><span class="line">        attn_weights = Q_.matmul(torch.transpose(K_, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        attn_weights += Q_.matmul(torch.transpose(abs_pos_K_, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        attn_weights += time_matrix_K_.matmul(Q_.unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># seq length adaptive scaling   ---公式（8）</span></span><br><span class="line">        attn_weights = attn_weights / (K_.shape[-<span class="number">1</span>] ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># key masking, -2^32 lead to leaking, inf lead to nan</span></span><br><span class="line">        <span class="comment"># 0 * inf = nan, then reduce_sum([nan,...]) = nan</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># time_mask = time_mask.unsqueeze(-1).expand(attn_weights.shape[0], -1, attn_weights.shape[-1])</span></span><br><span class="line">        <span class="comment"># 会报错，必须按下面的1.2.3.</span></span><br><span class="line">        time_mask = time_mask.unsqueeze(-<span class="number">1</span>).repeat(self.head_num, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1.unsqueeze()：time_mask.shape=(batch_size,maxlen) ——&gt; (batch_size,maxlen,1),最后一个维度填1</span></span><br><span class="line">        <span class="comment"># 2.repeat():(batch_size,maxlen,1) ——&gt; (self.head_num*batch_size,maxlen,1),第一个维度乘倍数</span></span><br><span class="line">        time_mask = time_mask.expand(-<span class="number">1</span>, -<span class="number">1</span>, attn_weights.shape[-<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 3.(self.head_num*batch_size,maxlen,1) ——&gt;(self.head_num*batch_size,maxlen,maxlen)</span></span><br><span class="line">        <span class="comment"># tips：attn_weights= (B,maxlen,maxlen),每个batch中size=(maxlen,maxlen)，每行表示某个item对其它所有item的atten矩阵</span></span><br><span class="line">        <span class="comment">#      time_mask是对padding的item做mask,本来是(B,maxlen,1),每个batch中size=(maxlen,1)</span></span><br><span class="line">        <span class="comment">#      expand成(B,maxlen,maxlen)才能把attn里padding的物品，即对应行都mask掉</span></span><br><span class="line"></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">0</span>).expand(attn_weights.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (maxlen,maxlen) ——&gt; (1,maxlen,maxlen) ——&gt; (batch_size,maxlen,maxlen)</span></span><br><span class="line">        <span class="comment"># padding取负无穷是因为底下要用softmax，以e为底的负无穷接近0</span></span><br><span class="line">        paddings = torch.ones(attn_weights.shape) *  (-<span class="number">2</span>**<span class="number">32</span>+<span class="number">1</span>) <span class="comment"># -1e23 # float(&#x27;-inf&#x27;),</span></span><br><span class="line">        paddings = paddings.to(self.dev)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这两步一起为了mask掉不用的attention计算，第一步是mask掉padding的items，第二是为了因果关系mask掉afterwards的items</span></span><br><span class="line">        attn_weights = torch.where(time_mask, paddings, attn_weights) <span class="comment"># True:pick padding</span></span><br><span class="line">        attn_weights = torch.where(attn_mask, paddings, attn_weights) <span class="comment"># enforcing causality</span></span><br><span class="line"></span><br><span class="line">        attn_weights = self.softmax(attn_weights)   <span class="comment"># ---公式（7）</span></span><br><span class="line">        attn_weights = self.dropout(attn_weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---公式（6），把alpha放进去乘了</span></span><br><span class="line">        outputs = attn_weights.matmul(V_)</span><br><span class="line">        outputs += attn_weights.matmul(abs_pos_V_)</span><br><span class="line">        outputs += attn_weights.unsqueeze(<span class="number">2</span>).matmul(time_matrix_V_).reshape(outputs.shape)<span class="comment">#.squeeze(2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (num_head * N, T, C / num_head) -&gt; (N, T, C)</span></span><br><span class="line">        outputs = torch.cat(torch.split(outputs, Q.shape[<span class="number">0</span>], dim=<span class="number">0</span>), dim=<span class="number">2</span>) <span class="comment"># div batch_size</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>代码阅读</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>TiSASRec</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习01：西瓜书概览</title>
    <url>/2022/12/14/%5B%E5%90%83%E7%93%9C%E6%95%99%E7%A8%8B%5D%20Task01%20%E8%A5%BF%E7%93%9C%E4%B9%A6%E6%A6%82%E8%A7%88/</url>
    <content><![CDATA[<h1 id="Task01-西瓜书概览"><a href="#Task01-西瓜书概览" class="headerlink" title="Task01 西瓜书概览"></a>Task01 西瓜书概览</h1><p>主要 follw 教程：<a href="https://datawhale.feishu.cn/docs/doccndJC2sbSfdziNcahCYCx70W#">https://datawhale.feishu.cn/docs/doccndJC2sbSfdziNcahCYCx70W#</a></p>
<p>只记录一些印象深刻的基本概念，以及自己认为重要的点</p>
<span id="more"></span>
<p>第一章：</p>
<p>1、“样本空间”：属性（特征）张成的空间，也叫样本空间。所有特征取值组合的集合。</p>
<p>2、“标记空间”：样本 label 的集合。</p>
<p>3、学习任务可大致分为两大类：监督学习和无监督学习，分类和回归是前者的代表，聚类是后者的代表。</p>
<p>4、通常对样本的假设是：样本空间中的全体样本服从某个未知分布 $\mathcal{D} $ ，我们获得的每个样本都是独立地从这个分布上采样获得的，即独立同分布。</p>
<p>5、归纳（induction）：从特殊到一般的泛化（generalization）过程。</p>
<p>6、演绎（deduction）：从特殊到一般的特化（specialization）过程。</p>
<p>7、我们可以把学习过程看作一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到与训练集”匹配“（<strong>fit</strong>）的假设。</p>
<p><code>model.fit(train)</code>，原来是根据这样的定义来的。</p>
<p>8、归纳偏好（inductive bias）：机器学习算法在学习过程中对某种类型假设的偏好。归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或“价值观”。例如“奥卡姆剃刀”。归纳偏好对应了学习算法本身所作出的关于“什么样的模型更好”的假设。</p>
<p>9、NFL定理的理解：<a href="https://zhuanlan.zhihu.com/p/11312671">https://zhuanlan.zhihu.com/p/11312671</a></p>
<p>第二章：</p>
<p>1、精度（accuracy，acc）=分类正确的样本数占样本总数的比例；误差（error）=实际预测输出与样本的真实输出之间的差异。</p>
<p>2、从采样（sampling）的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为“分层采样”（stratified sampling）。</p>
<p>3、p次k折交叉验证法。p次：减少了划分k折时带来的误差，k折：充分利用训练集。</p>
<p>4、留一法（Leave-One-Out）是交叉验证法的特例，测试集数量为1。</p>
<p>5、自助法（bootstrapping），即有放回采样，在数据集较小时很有用，也适合集成学习，但是改变了初始数据集的分布，会引入估计偏差，所以当数据量足够时，建议交叉验证法。</p>
<p>6、给定包含m个样本的数据集D，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型。因此在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集D重新训练模型。这个模型在训练过程中使用了所有m个样本，这才是我们最终提交给用户的模型。</p>
<p>k折交叉验证每个单独模型训练都没有使用全量数据，只用来寻找模型的最佳超参数，确定完超参数以后再全量训练一次得到最终模型。</p>
<p><img src="https://img-blog.csdnimg.cn/3b939929716f487cbe524234494bcfa4.png" alt="二分类结果混淆矩阵" style="zoom:50%;" /></p>
<p>7、查准率（precision）：检索出来的有多少是用户感兴趣的；查全率（recall）：用户感兴趣的有多少检索出来；</p>
<p><strong>查准率（precision）和精度（accuracy）有区别</strong>，精度：分类正确的样本数占样本总数的比例</p>
<script type="math/tex; mode=display">
precision = \frac{TP}{TP+FP} \\
recall = \frac{TP}{TP+FN} \\
accuracy = \frac{TP+TN}{TP+FP+FN+TN}</script><p>8、关于P-R曲线</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221213222318781.png" alt="image-20221213222318781" style="zoom:50%;" /></p>
<p><strong>可以用来比较模型优劣</strong></p>
<p>在进行比较时，若一个学习器的P-R曲线被另一个学习器曲线完全“包住”，则可断言后者的性能优于前者。</p>
<p>以A、C为例，A“包住”C，则A的性能高于C，因为<strong>在相同查准率或查全率的条件下都是A优于C</strong>，可以做水平线和垂线更直观。</p>
<p>9、macro-F1 和 micro-F1</p>
<p>多分类场景下：</p>
<p>macro 指标是先计算各混淆矩阵对应的 P、R、F1，对这些指标取平均得到 macro-P，macro-R，macro-F1</p>
<p>micro 指标是对各混淆矩阵的 TP、FP、TN、FN 取完平均后，计算 P、R、F1，得到 micro-P、micro-R、micro-F1</p>
<p>附上之前用过的 sklearn.metrics.classification_report()<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=precision_recall">[2]</a>，形如：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221213223442654.png" alt="image-20221213223442654" style="zoom:33%;" /></p>
<p>10、ROC曲线与AUC</p>
<p>AUC值衡量的是<strong>模型是否把正例都排在了负例前面</strong>。</p>
<p>结合混淆矩阵看</p>
<p><img src="https://img-blog.csdnimg.cn/3b939929716f487cbe524234494bcfa4.png" alt="二分类结果混淆矩阵" style="zoom: 33%;" /></p>
<p>ROC曲线的纵坐标是“真正例率”（TPR），横坐标是“假正例率”（FPR）</p>
<script type="math/tex; mode=display">
TPR=\frac{TP}{TP+FN} \\
FPR=\frac{FP}{TN+FP}</script><p>取这两个指标做横、纵坐标的原因是：ROC绘制时假设依次每个样本为<strong>正例</strong>，所以方便计算当前阈值下的<strong>真正例率</strong>和<strong>假正例率</strong>。</p>
<p>ROC曲线的绘制过程如下：</p>
<ul>
<li>假设有 m+ 个正例，m- 个负例，对模型输出的预测概率按从高到低排序</li>
<li>然后依次将每个样本的预测值作为阈值（<strong>即将该样本作为正例</strong>），假设前一个坐标为$(x,y)$，若当前为真正例，对应标记点为$(x,y + \frac{1}{m^+})$，若当前为假正例，则对应标记点为$(x,y + \frac{1}{m^-})$</li>
<li>将所有点相连即可得到 ROC 曲线</li>
</ul>
<p>而AUC值就是ROC曲线下的面积，这里<a href="https://www.scholat.com/teamwork/teamwork/showPostMessage.html?id=9229">[3]</a>有AUC计算过程.</p>
<p>看个例子：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221213232512000.png" alt="image-20221213232512000" style="zoom:50%;" /></p>
<p>模型有两个预测错误的样本，分别是样本3和样本8，此时AUC=0.8，具体计算过程略过</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; from sklearn.metrics import roc_auc_score</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; scores=[0.9,0.8,0.7,0.6,0.51,0.4,0.3,0.2,0.1,0.01]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; lables=[1,1,0,1,1,0,0,1,0,0]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; roc_auc_score(lables,scores)</span></span><br><span class="line">0.8</span><br></pre></td></tr></table></figure>
<p>现在我们直接看最理想的情况下的AUC，帮助理解AUC的含义</p>
<p>最理想情况下，模型根据预测值给样本排序的结果是：$f(1),f(2),f(4),f(5),f(8)  &gt; f(3),f(6),f(7),f(9),f(10)$ ，即把真实标签为正的都排在负的前面了。此时的AUC值一定为1，不信可以描点看看：</p>
<ul>
<li>依次将每个样本的预测值作为阈值（<strong>即将该样本作为正例</strong>），前五个点一定是$(1,2,4,5,8)$，后五个点一定是$(3,6,7,9,10)$，顺序不重要<ul>
<li>第一个点，计算TPR和FPR，TPR=1/5=0.2，FPR=0/1=0，坐标$(0.2,0)$</li>
<li>第二个点，TPR=2/5=0.4，FPR=0/2=0，坐标$(0.4,0)$</li>
<li>第三个点，TPR=3/5=0.6，FPR=0/3=0，坐标$(0.6,0)$</li>
<li>第四个点，TPR=4/5=0.8，FPR=0/4=0，坐标$(0.8,0)$</li>
<li>第五个点，TPR=5/5=1，FPR=0/5=0，坐标$(1,0)$</li>
<li>第六个点，TPR=5/5=1，FPR=1/5=0.2，坐标$(1,0.2)$</li>
<li>第七个点，TPR=5/5=1，FPR=2/5=0.4，坐标$(1,0.4)$</li>
<li>第八个点，TPR=5/5=1，FPR=3/5=0.6，坐标$(1,0.6)$</li>
<li>第九个点，TPR=5/5=1，FPR=4/5=0.8，坐标$(1,0.8)$</li>
<li>第十个点，TPR=5/5=1，FPR=5/5=1，坐标，坐标$(1,1)$</li>
</ul>
</li>
<li>依次描点作图，顺次连接起来，围住了整个$1 \times 1$的矩形面积，AUC=1<ul>
<li><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221213235210644.png" alt="image-20221213235210644" style="zoom:33%;" /></li>
</ul>
</li>
<li>再看这句话应该能理解了 “AUC值衡量的是<strong>模型是否把正例都排在了负例前面</strong>” 。</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]NFL定理的理解：<a href="https://zhuanlan.zhihu.com/p/11312671">https://zhuanlan.zhihu.com/p/11312671</a></p>
<p>[2]<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=precision_recall">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=precision_recall</a></p>
<p>[3]AUC计算过程 <a href="https://www.scholat.com/teamwork/teamwork/showPostMessage.html?id=9229">https://www.scholat.com/teamwork/teamwork/showPostMessage.html?id=9229</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Datawhale组队学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习02：西瓜书第三章 线性模型</title>
    <url>/2022/12/20/%5B%E5%90%83%E7%93%9C%E6%95%99%E7%A8%8B%5D%20Task02%20%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC%E4%B8%89%E7%AB%A0/</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>主要 follw 教程：<a href="https://datawhale.feishu.cn/docs/doccndJC2sbSfdziNcahCYCx70W#">https://datawhale.feishu.cn/docs/doccndJC2sbSfdziNcahCYCx70W#</a></p>
<h2 id="机器学习三要素"><a href="#机器学习三要素" class="headerlink" title="机器学习三要素"></a>机器学习三要素</h2><ol>
<li>模型：根据具体问题，确定假设空间</li>
<li>策略：根据评价标准，确定选取最优模型的策略（通常会产出一个“损失函数”）</li>
<li>算法：求解损失函数，确定最优模型</li>
</ol>
<h2 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h2><p>以一元线性回归为例：求解发际线高度x和计算机水平y的关系。</p>
<ol>
<li><p>模型：根据经验（或观察数据形态），呈线性关系，所以假设空间 $f(x)=wx+b$，而不是曲线关系。</p>
<ul>
<li>一元线性回归表达式:$ y = wx + b $</li>
</ul>
</li>
<li><p>策略：所有点距离拟合的直线垂直距离最小，即均方误差小，最小二乘法。或者使用极大似然估计，假设$y=wx+b+\epsilon$，对$\epsilon$误差建模，能得出同样策略。</p>
<ul>
<li><p>损失函数为均方误差(最小二乘法):</p>
</li>
<li><script type="math/tex; mode=display">
\begin{aligned}
E_{(w, b)} & =\sum_{i=1}^{m}\left(y_{i}-f\left(x_{i}\right)\right)^{2} \\
& =\sum_{i=1}^{m}\left(y_{i}-\left(w x_{i}+b\right)\right)^{2} \\
& =\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}
\end{aligned}</script></li>
</ul>
</li>
<li><p>算法：可以证明 $E_{(w, b)}$是凸函数（可以求2阶偏导数，证明Hessian矩阵半正定），所以能求出闭式解。（但机器学习算法通常没有闭式解，就要用梯度下降法、牛顿法近似求解）</p>
<ul>
<li><p>令一阶偏导等于0，可以求出闭式解(此处省略推导):</p>
</li>
<li><script type="math/tex; mode=display">
w=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}} \\
b=\bar{y}-w \bar{x}</script></li>
</ul>
</li>
</ol>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">n, w_true, b_true</span>):</span></span><br><span class="line">    X = np.arange(n)</span><br><span class="line">    eps = -<span class="number">0.5</span> * b_true + np.random.random(n) * b_true</span><br><span class="line">    Y = w_true * X + eps</span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">closed_form_solution</span>(<span class="params">X,Y</span>):</span></span><br><span class="line">  	<span class="comment"># 参考闭式解公式</span></span><br><span class="line">    X_bar = X.<span class="built_in">sum</span>() / X.size</span><br><span class="line">    Y_bar = Y.<span class="built_in">sum</span>() / Y.size</span><br><span class="line">    w = Y.dot(X - X_bar) / (X.dot(X) - (X.<span class="built_in">sum</span>()) ** <span class="number">2</span> / X.size)</span><br><span class="line">    b = Y_bar - w * X_bar</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line">  </span><br><span class="line">X, Y = load_data(<span class="number">20</span>, <span class="number">3</span>, <span class="number">15</span>)</span><br><span class="line">w, b = closed_form_solution(X, Y)</span><br><span class="line">Y_predict = X * w + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(X, Y, label=<span class="string">&quot;原始数据&quot;</span>, c=<span class="string">&#x27;b&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.plot(X, Y_predict, label=<span class="string">&quot;回归线&quot;</span>, c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;一元线性回归&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果可视化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(X, Y, label=<span class="string">&quot;原始数据&quot;</span>, c=<span class="string">&#x27;b&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.plot(X, Y_predict, label=<span class="string">&quot;回归线&quot;</span>, c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;一元线性回归&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221219202638629.png" alt="image-20221219202638629" style="zoom:50%;" /></p>
<h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><p>与一元线性回归相比，$\boldsymbol{x}$维度变高，写成向量形式：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \\
f\left(\boldsymbol{x}_{i}\right)=\left(\begin{array}{llll}
w_{1} & w_{2} & \cdots & w_{d}
\end{array}\right)\left(\begin{array}{c}
x_{i 1} \\
x_{i 2} \\
\vdots \\
x_{i d}
\end{array}\right)+b \\
\end{array}</script><p>把$\boldsymbol{w}$和$b$合并成$\boldsymbol{\hat w}$：</p>
<script type="math/tex; mode=display">
f\left(\boldsymbol{x}_{i}\right)=w_{1} x_{i 1}+w_{2} x_{i 2}+\ldots+w_{d} x_{i d}+b \\
f\left(\boldsymbol{x}_{i}\right)=w_{1} x_{i 1}+w_{2} x_{i 2}+\ldots+w_{d} x_{i d}+w_{d+1} \cdot 1 \\
\begin{array}{c}
f\left(\boldsymbol{x}_{i}\right)=\left(\begin{array}{ccccc}
w_{1} & w_{2} & \cdots & w_{d} & w_{d+1}
\end{array}\right)\left(\begin{array}{c}
x_{i 1} \\
x_{i 2} \\
\vdots \\
x_{i d} \\
1
\end{array}\right) \\
f\left(\hat{\boldsymbol{x}}_{i}\right)=\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}
\end{array}</script><p>故多元线性回归的均方误差可以写成：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
E_{\hat{\boldsymbol{w}}}=\sum_{i=1}^{m}\left(y_{i}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}\right)^{2}=\left(y_{1}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{1}\right)^{2}+\left(y_{2}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{2}\right)^{2}+\ldots+\left(y_{m}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{m}\right)^{2} \\
E_{\hat{\boldsymbol{w}}}=\left(\begin{array}{llll}
y_{1}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{1} & y_{2}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{2} & \cdots & y_{m}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{m}
\end{array}\right)\left(\begin{array}{c}
y_{1}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{1} \\
y_{2}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{2} \\
\vdots \\
y_{m}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{m}
\end{array}\right)
\end{array}</script><p>其中：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\left(\begin{array}{c}
y_{1}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{1} \\
y_{2}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{2} \\
\vdots \\
y_{m}-\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{m}
\end{array}\right)=\left(\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{m}
\end{array}\right)-\left(\begin{array}{c}
\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{1} \\
\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{2} \\
\vdots \\
\hat{\boldsymbol{w}}^{\mathrm{T}} \hat{\boldsymbol{x}}_{m}
\end{array}\right)=\left(\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{m}
\end{array}\right)-\left(\begin{array}{c}
\hat{\boldsymbol{x}}_{1}^{\mathrm{T}} \hat{\boldsymbol{w}} \\
\hat{\boldsymbol{x}}_{2}^{\mathrm{T}} \hat{\boldsymbol{w}} \\
\vdots \\
\hat{\boldsymbol{x}}_{m}^{\mathrm{T}} \hat{\boldsymbol{w}}
\end{array}\right) \\
\boldsymbol{y}=\left(\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{m}
\end{array}\right), \quad\left(\begin{array}{c}
\hat{\boldsymbol{x}}_{1}^{\mathrm{T}} \hat{\boldsymbol{w}} \\
\hat{\boldsymbol{x}}_{2}^{\mathrm{T}} \hat{\boldsymbol{w}} \\
\vdots \\
\hat{\boldsymbol{x}}_{m}^{\mathrm{T}} \hat{\boldsymbol{w}}
\end{array}\right)=\left(\begin{array}{c}
\hat{\boldsymbol{x}}_{1}^{\mathrm{T}} \\
\hat{\boldsymbol{x}}_{2}^{\mathrm{T}} \\
\vdots \\
\hat{\boldsymbol{x}}_{m}^{\mathrm{T}}
\end{array}\right) \cdot \hat{\boldsymbol{w}}=\left(\begin{array}{cc}
\boldsymbol{x}_{1}^{\mathrm{T}} & 1 \\
\boldsymbol{x}_{2}^{\mathrm{T}} & 1 \\
\vdots & \vdots \\
\boldsymbol{x}_{m}^{\mathrm{T}} & 1
\end{array}\right) \cdot \hat{\boldsymbol{w}}=\mathbf{X} \cdot \hat{\boldsymbol{w}}
\end{array}</script><p>所以可以把$E_{\hat{\boldsymbol{w}}}$向量化为：$E_{\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$</p>
<p>同样可以证明$E_{\hat{\boldsymbol{w}}}$是凸函数（求二阶偏导证明hessian矩阵半正定），$E_{\hat{\boldsymbol{w}}}$对$\hat{\boldsymbol{w}}$求一阶偏导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}} & =2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y}) \text { 公式 } 3.10
\end{aligned}</script><p>其中用到的矩阵微分公式：$\frac{\partial \boldsymbol{x}^{\mathrm{T}} \boldsymbol{a}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{a}^{\mathrm{T}} \boldsymbol{x}}{\partial \boldsymbol{x}}=\boldsymbol{a}, \frac{\partial \boldsymbol{x}^{\mathrm{T}} \mathbf{A} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\mathbf{A}+\mathbf{A}^{\mathrm{T}}\right) \boldsymbol{x},\frac{\partial \mathbf{A} \boldsymbol{x}}{\partial \boldsymbol{x}}=\mathbf{A}^{\mathrm{T}}$</p>
<p>再令一阶偏导为零，可以求得闭式解：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
\frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}}=2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})=0 \\
2 \mathbf{X}^{\mathrm{T}} \mathbf{X} \hat{\boldsymbol{w}}-2 \mathbf{X}^{\mathrm{T}} \boldsymbol{y}=0 \\
2 \mathbf{X}^{\mathrm{T}} \mathbf{X} \hat{\boldsymbol{w}}=2 \mathbf{X}^{\mathrm{T}} \boldsymbol{y} \\
\hat{\boldsymbol{w}}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}
\end{array}</script><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">n_sample:<span class="built_in">int</span>, W_true:np.array, b_true</span>):</span></span><br><span class="line">    n_feature = <span class="built_in">len</span>(W_true)</span><br><span class="line">    ones = np.ones(shape=(n_sample,<span class="number">1</span>))</span><br><span class="line">    X = np.concatenate([np.random.random((n_sample, n_feature)), ones],axis=<span class="number">1</span>)</span><br><span class="line">    eps = -<span class="number">0.5</span> * b_true + np.random.random(n_sample) * b_true</span><br><span class="line">    W_true = np.concatenate([W_true,[<span class="number">1</span>,]],axis=<span class="number">0</span>)</span><br><span class="line">    W_true = np.mat(W_true).T</span><br><span class="line">    Y = X * W_true + np.mat(eps).T</span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line"><span class="comment">## 求闭式解</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">closed_form_solution</span>(<span class="params">X, Y</span>):</span></span><br><span class="line">    <span class="comment"># 用np.mat()转换成matrix以后方便矩阵运算</span></span><br><span class="line">    X = np.mat(X)</span><br><span class="line">    Y = np.mat(Y)</span><br><span class="line">    XTX = X.T * X</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(XTX) == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 行列式为0,不可逆</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;X^T·X 不是满秩矩阵，解不唯一&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    W = XTX.I * X.T * Y</span><br><span class="line">    <span class="keyword">return</span> W</span><br><span class="line"></span><br><span class="line"><span class="comment">## 梯度下降法求解</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_trainer</span>(<span class="params">X:np.matrix, Y:np.matrix, W:np.matrix, lr:<span class="built_in">float</span></span>):</span></span><br><span class="line">    <span class="comment"># 一阶导数</span></span><br><span class="line">    gradient = <span class="number">2</span> * X.T * (X * W - Y)</span><br><span class="line">    W = W - lr * gradient</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> W</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">X:np.matrix, Y:np.matrix, W:np.matrix</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>((Y - X * W).T * (Y - X * W))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_solution</span>(<span class="params">X, Y, epoch, lr</span>):</span></span><br><span class="line">    X = np.mat(X)</span><br><span class="line">    Y = np.mat(Y)</span><br><span class="line">    W = np.mat(np.random.random((X.shape[<span class="number">1</span>],<span class="number">1</span>)))</span><br><span class="line">    lossList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        loss = loss_function(X, Y, W)</span><br><span class="line">        lossList.append(loss)</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>)%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;epoch:<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>  loss:<span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">        W = gradient_descent_trainer(X, Y, W, lr)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> W, lossList</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_sample = <span class="number">10</span></span><br><span class="line">W_true = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">b_true = <span class="number">1</span></span><br><span class="line">X,Y = load_data(n_sample, W_true, b_true)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求闭式解</span></span><br><span class="line">W_1 = closed_form_solution(X, Y)</span><br><span class="line">Y_1 = X * W_1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降求解</span></span><br><span class="line">epoch = <span class="number">200</span></span><br><span class="line">lr = <span class="number">1e-2</span></span><br><span class="line">W_2, lossList = gradient_descent_solution(X, Y, epoch, lr)</span><br><span class="line">Y_2 = X * W_2</span><br></pre></td></tr></table></figure>
<p>梯度下降 loss 曲线：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221221000604133.png" alt="image-20221221000604133" style="zoom:50%;" /></p>
<p>结果可视化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>,n_sample+<span class="number">1</span>)), Y.tolist(), label=<span class="string">&quot;原始数据&quot;</span>, c=<span class="string">&#x27;b&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.scatter(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>,n_sample+<span class="number">1</span>)), Y_1.tolist(), label=<span class="string">&quot;闭式求解数据&quot;</span>, c=<span class="string">&#x27;r&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.scatter(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>,n_sample+<span class="number">1</span>)), Y_2.tolist(), label=<span class="string">&quot;梯度下降求解数据&quot;</span>, c=<span class="string">&#x27;y&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;多元线性回归&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221221000525846.png" alt="image-20221221000525846" style="zoom:50%;" /></p>
<h2 id="对数几率回归（逻辑回归-LR）"><a href="#对数几率回归（逻辑回归-LR）" class="headerlink" title="对数几率回归（逻辑回归 LR）"></a>对数几率回归（逻辑回归 LR）</h2><p>对数几率回归算法的机器学习三要素：</p>
<ol>
<li>模型：线性模型，输出值的范围是(0,1)，近似阶跃的单调可微函数</li>
<li>策略：极大似然估计，信息论，殊途同归</li>
<li>算法：梯度下降，牛顿法</li>
</ol>
<p>对数几率回归（逻辑回归）：用线性模型做二分类任务</p>
<p>用sigmoid函数，将实数域的取值映射到(0,1)：</p>
<script type="math/tex; mode=display">
sigmoid\left(\boldsymbol{x}\right)=\frac{1}{1+e^{- \left( \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \right)}}</script><h3 id="为什么可以用sigmoid？"><a href="#为什么可以用sigmoid？" class="headerlink" title="为什么可以用sigmoid？"></a>为什么可以用sigmoid？</h3><p>西瓜书上的解释：$y =\frac{1}{1+e^{- \left( \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \right)}}$ 可以推导出 $\frac{y}{1-y}= \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$，若将$y$视作样本$\boldsymbol{x}$作为正例的可能性， 则$1-y$是样本$\boldsymbol{x}$作为负例的可能性，$\frac{y}{1-y}$反应了样本$\boldsymbol{x}$作为正例的相对可能性。</p>
<h3 id="损失函数推导"><a href="#损失函数推导" class="headerlink" title="损失函数推导"></a>损失函数推导</h3><blockquote>
<p>如果随机变量X只取0和1两个值，并且相应的概率为：</p>
<script type="math/tex; mode=display">
\operatorname{Pr}(X=1)=p, \operatorname{Pr}(X=0)=1-p, 0<p<1</script><p>则称随机变量X服从参数为p的伯努利分布</p>
</blockquote>
<p>逻辑回归是假设数据服从<strong>伯努利分布</strong>。</p>
<p>在伯努利分布下，假设样本概率为：$f(n)=\left\{\begin{array}{ll}<br>p, &amp; y=1 \\<br>1-p, &amp; y=0<br>\end{array}\right.$，则概率密度函数 $p\left(y_{i} \mid x_i\right)=p^{y_{i}}(1-p)^{1-y_{i}}$，</p>
<h4 id="优化均方误差？"><a href="#优化均方误差？" class="headerlink" title="优化均方误差？"></a>优化均方误差？</h4><p>$E_{\hat{\boldsymbol{w}}}=\left(\frac{1}{1+e^{- \left( \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b \right)}}-y\right)^2$，可以证明是<strong>非凸</strong>的，无法求解</p>
<h4 id="极大似然估计法"><a href="#极大似然估计法" class="headerlink" title="极大似然估计法"></a>极大似然估计法</h4><p>所以考虑用极大似然估计求分布：</p>
<script type="math/tex; mode=display">
L=\prod_{i=1}^{m} p^{y_{i}}(1-p)^\left(1-y_{i}\right)</script><p>两边取对数：</p>
<script type="math/tex; mode=display">
ln(L)=\sum_{i=1}^{m}\left(y_{i} \ln (p)+\left(1-y_{i}\right) \ln (1-p)\right)</script><p>其中：</p>
<script type="math/tex; mode=display">
p=\frac{1}{1+e^{-\left(\boldsymbol{w}^{T} \boldsymbol{x}+b\right)}}=\frac{1}{1+e^{-\boldsymbol{\beta}^{T} \boldsymbol{\hat x}}}</script><p>带入$ln(L)$，可得：</p>
<script type="math/tex; mode=display">
\ln(L\left( \boldsymbol{\beta} \right))=\sum_{i=1}^{m}\left(y_{i} \boldsymbol{\beta}^{T} \boldsymbol{\hat x}- \ln (1+e^{\boldsymbol{\beta}^{T} \boldsymbol{\hat x}})\right)</script><p>因为最大化似然函数就等于最小化：</p>
<script type="math/tex; mode=display">
\mathcal{l}\left( \boldsymbol{\beta} \right)=\sum_{i=1}^{m}\left(-y_{i} \boldsymbol{\beta}^{T} \boldsymbol{\hat x}+ \ln (1+e^{\boldsymbol{\beta}^{T} \boldsymbol{\hat x}})\right)</script><p>而$\mathcal{l}\left( \boldsymbol{\beta} \right)$是高阶非凸函数，所以可以用经典的数值优化方法梯度下降法或牛顿法求近似解。$\mathcal{l}\left( \boldsymbol{\beta} \right)$也是模型的损失函数。</p>
<h4 id="为什么不求闭式解？"><a href="#为什么不求闭式解？" class="headerlink" title="为什么不求闭式解？"></a>为什么不求闭式解？</h4><p>即然$\mathcal{l}\left( \boldsymbol{\beta} \right)$是高阶非凸的，可以直接解，那为什么不直接按公式求闭式解？</p>
<p>回忆多元线性回归的闭式解：$\hat{\boldsymbol{w}}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$，这里需要求矩阵的逆，通常情况下都不存在，无法直接求解，甚至要用梯度下降近似求解的方法去求这个逆（回忆矩阵课上学的···），所以不如直接近似求解 $\hat{\boldsymbol{w}}$ 。</p>
<h4 id="最大熵法"><a href="#最大熵法" class="headerlink" title="最大熵法"></a>最大熵法</h4><p>前置知识：</p>
<ul>
<li><p>自信息 $I(X)=-\log _{b} p(x)$</p>
</li>
<li><p>信息熵 $H(X)=E[I(X)]=-\sum_{x} p(x) \log _{b} p(x)$，是自信息的期望，度量随机变量 $X$ 的不确定性，信息熵越大越不确定</p>
</li>
<li><p>相对熵（KL散度）：度量两个分布的差异，典型使用场景是用来度量理想分布$p(x)$和模拟分布$q(x)$ 之间的差异</p>
<ul>
<li><script type="math/tex; mode=display">
\begin{aligned}
D_{K L}(p \| q) & =\sum_{x} p(x) \log _{b}\left(\frac{p(x)}{q(x)}\right) \\
& =\sum_{x} p(x)\left(\log _{b} p(x)-\log _{b} q(x)\right) \\
& =\sum_{x} p(x) \log _{b} p(x)-\sum_{x} p(x) \log _{b} q(x)
\end{aligned}</script></li>
<li><p>其中 $-\sum_{x} p(x) \log _{b} q(x)$ 为交叉熵。</p>
</li>
<li><p>KL散度前半部分是常量，后半部分是交叉熵，这也是为什么常说逻辑回归的损失函数是交叉熵损失函数。</p>
</li>
</ul>
</li>
</ul>
<p>从机器学习三要素中“策略”的角度来说，与理想分布最接近的模拟分布即为最优分布，因此可以通过<strong>最小化相对熵</strong>这个策略来求出最优分布。由于理想分布$p(x)$是未知但固定的分布（频率学派的角度），所以KL散度前半部分是常量，那么最小化相对熵就等价于最小化交叉熵。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221220015135440.png" alt="image-20221220015135440" style="zoom: 25%;" /></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221220015220368.png" alt="image-20221220015220368" style="zoom:25%;" /></p>
<p>这边已经等价于用极大似然估计取对数的结果了，后面依然能推导得到公式3.27</p>
<h3 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h3><p>在 MNIST 数据集上实现逻辑回归二分类，标签0～4分类为0，标签5～9分类为1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 整个训练集更新不太合理,mini-batch更新暂时没精力写···所以先实现逐个样本更新的版本</span></span><br><span class="line"><span class="comment"># 部分实现参考 https://github.com/Dod-o/Statistical-Learning-Method_Code</span></span><br><span class="line"><span class="comment"># 使用MNIST数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_mnist</span>(<span class="params">fileName</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    加载Mnist数据集</span></span><br><span class="line"><span class="string">    :param fileName:要加载的数据集路径</span></span><br><span class="line"><span class="string">    :return: list形式的数据集及标记</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start to read data&#x27;</span>)</span><br><span class="line">    <span class="comment"># 存放数据及标记的list</span></span><br><span class="line">    dataArr = []; labelArr = []</span><br><span class="line">    <span class="comment"># 打开文件</span></span><br><span class="line">    fr = <span class="built_in">open</span>(fileName, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将文件按行读取</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># 对每一行数据按切割福&#x27;,&#x27;进行切割，返回字段列表</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Mnsit有0-9是个标记，由于是二分类任务，所以将&gt;=5的作为1，&lt;5为0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">int</span>(curLine[<span class="number">0</span>]) &gt;= <span class="number">5</span>:</span><br><span class="line">            labelArr.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labelArr.append(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#存放标记</span></span><br><span class="line">        <span class="comment">#[int(num) for num in curLine[1:]] -&gt; 遍历每一行中除了以第一个元素（标记）外将所有元素转换成int类型</span></span><br><span class="line">        <span class="comment">#[int(num)/255 for num in curLine[1:]] -&gt; 将所有数据除255归一化(非必须步骤，可以不归一化)</span></span><br><span class="line">        dataArr.append([<span class="built_in">int</span>(num)/<span class="number">255</span> <span class="keyword">for</span> num <span class="keyword">in</span> curLine[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#返回data和label</span></span><br><span class="line">    <span class="keyword">return</span> dataArr, labelArr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">X:np.matrix, W:np.matrix, y:np.matrix</span>):</span></span><br><span class="line">    <span class="comment"># 每条样本的loss</span></span><br><span class="line">    y = <span class="built_in">float</span>(y)</span><br><span class="line">    eps = <span class="number">1e-10</span></span><br><span class="line">    y_hat = <span class="built_in">float</span>(X * W)</span><br><span class="line">    <span class="keyword">return</span> np.log(( <span class="number">1</span> + np.exp(y_hat))/np.exp(y * y_hat) + eps)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lr_forward</span>(<span class="params">x:np.matrix, w:np.matrix</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">    z = x * w</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent_trainer</span>(<span class="params">x:np.matrix, w:np.matrix, y:np.matrix, lr:<span class="built_in">float</span></span>) -&gt; np.matrix:</span></span><br><span class="line">    <span class="comment"># 逐个样本计算梯度</span></span><br><span class="line">    y = <span class="built_in">float</span>(y)</span><br><span class="line">    z = x * w</span><br><span class="line">    gradient = <span class="built_in">float</span>((-y + np.exp(z)/(<span class="number">1</span>+np.exp(z)))) * x.T</span><br><span class="line">    w = w - lr * gradient</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">X:<span class="built_in">list</span>, Y:<span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="comment"># 把 wx + b 转化成 beta * x_hat 形式</span></span><br><span class="line">    X = np.mat(X)</span><br><span class="line">    Y = np.mat(Y).T</span><br><span class="line">    ones = np.ones(shape=(X.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    X = np.concatenate([X, ones],axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">X:np.matrix, w:np.matrix, Y:np.matrix</span>):</span></span><br><span class="line">    n_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    y_predList = []</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(n_test):</span><br><span class="line">        y_pred = lr_forward(X[idx],w)</span><br><span class="line">        y_predList.append( <span class="number">1</span> <span class="keyword">if</span> y_pred &gt; <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">    y_truth = Y.T.tolist()[<span class="number">0</span>]</span><br><span class="line">    report = classification_report(y_true=y_truth, y_pred=y_predList, output_dict=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> report[<span class="string">&#x27;accuracy&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取训练集及标签</span></span><br><span class="line">trainData, trainLabel = load_mnist(<span class="string">&#x27;Mnist/mnist_train.csv&#x27;</span>)</span><br><span class="line">X_train, Y_train = collate_fn(trainData, trainLabel)</span><br><span class="line"><span class="comment"># 获取测试集及标签</span></span><br><span class="line">testData, testLabel = load_mnist(<span class="string">&#x27;Mnist/mnist_test.csv&#x27;</span>)</span><br><span class="line">X_test, Y_test = collate_fn(testData, testLabel)</span><br><span class="line"><span class="comment"># 初始化 w</span></span><br><span class="line">w = np.random.random((X_train.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 样本数</span></span><br><span class="line">n_train = X_train.shape[<span class="number">0</span>]</span><br><span class="line">n_test = X_test.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">lr = <span class="number">1e-4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练及评估</span></span><br><span class="line">lossTrainList = []</span><br><span class="line">lossTestList = []</span><br><span class="line">accTrainList = []</span><br><span class="line">accTestList = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    lossTrain4Epoch = <span class="number">0</span></span><br><span class="line">    lossTest4Epoch = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(n_train):</span><br><span class="line">        y_pred = lr_forward(X_train[idx], w)</span><br><span class="line">        <span class="comment"># 逐个样本更新w</span></span><br><span class="line">        w = gradient_descent_trainer(X_train[idx], w, Y_train[idx],lr)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 因为不是batch形式更新, 所以评估起来有些费劲</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(n_train):</span><br><span class="line">        y_pred = lr_forward(X_train[idx], w)</span><br><span class="line">        lossTrain = loss_fn(X_train[idx], w, Y_train[idx])</span><br><span class="line">        lossTrain4Epoch += lossTrain</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(n_test):</span><br><span class="line">        y_pred = lr_forward(X_test[idx], w)</span><br><span class="line">        lossTest = loss_fn(X_test[idx], w, Y_test[idx])</span><br><span class="line">        lossTest4Epoch += lossTest</span><br><span class="line">    </span><br><span class="line">    lossTrain4Epoch /= n_train</span><br><span class="line">    lossTest4Epoch /= n_test</span><br><span class="line">    lossTrainList.append(lossTrain4Epoch)</span><br><span class="line">    lossTestList.append(lossTest4Epoch)</span><br><span class="line">    <span class="comment"># 每个epoch评估</span></span><br><span class="line">    accTrain = evaluate(X_train, w, Y_train)</span><br><span class="line">    accTest = evaluate(X_test, w, Y_test)</span><br><span class="line">    accTrainList.append(accTrain)</span><br><span class="line">    accTestList.append(accTest)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch:<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[Train]   acc:<span class="subst">&#123;accTrain:<span class="number">.5</span>f&#125;</span>  loss:<span class="subst">&#123;lossTrain4Epoch:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[Test]    acc:<span class="subst">&#123;accTest:<span class="number">.5</span>f&#125;</span>  loss:<span class="subst">&#123;lossTest4Epoch:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>可视化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_loss</span>(<span class="params">lossTrain:<span class="built_in">list</span>,lossTest:<span class="built_in">list</span></span>):</span></span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(lossTrain)), lossTrain, linewidth=<span class="number">1</span>, linestyle=<span class="string">&quot;solid&quot;</span>, label=<span class="string">&quot;train loss&quot;</span>)</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(lossTest)), lossTrain, linewidth=<span class="number">1</span>, linestyle=<span class="string">&quot;solid&quot;</span>, label=<span class="string">&quot;test loss&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;梯度下降loss曲线&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_acc</span>(<span class="params">accTrain:<span class="built_in">list</span>,accTest:<span class="built_in">list</span></span>):</span></span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(accTrain)), accTrain, linewidth=<span class="number">1</span>, linestyle=<span class="string">&quot;solid&quot;</span>, label=<span class="string">&quot;train acc&quot;</span>)</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(accTest)), accTest, linewidth=<span class="number">1</span>, linestyle=<span class="string">&quot;solid&quot;</span>, label=<span class="string">&quot;test acc&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;梯度下降acc曲线&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">draw_loss(lossTrainList,lossTestList)</span><br><span class="line">draw_acc(accTrainList,accTestList)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221221000736827.png" alt="image-20221221000736827" style="zoom:33%;" /><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221221000753958.png" alt="image-20221221000753958" style="zoom:33%;" /></p>
<h3 id="为什么要用sigmoid？-1"><a href="#为什么要用sigmoid？-1" class="headerlink" title="为什么要用sigmoid？[1]"></a>为什么要用sigmoid？[1]</h3><blockquote>
<p>Q：为什么 LR 模型要使用 sigmoid 函数？</p>
<p>A：因为bernoulli的指数族分布形式为：</p>
<script type="math/tex; mode=display">
p(x|μ)=Bern(x|μ)=e^{\ln \left({\mu^x \left(1-\mu\right)^{1-x}}\right)}\\     =\left(1-\mu\right)\exp\left\{x\ln\left(\frac{\mu}{1-\mu}\right)\right\}</script><p>其自然系数 $\eta$ 与 $\mu$ 的关系，也就是inverse parameter mapping 为 $\mu = \frac{1}{1+e^{-\eta}}$</p>
<p>Q：为什么要使用指数族分布？</p>
<p>A：因为指数族分布是给定某些统计量下熵最大的分布，例如伯努利分布就是只有两个取值且给定期望值为 $\mu$ 下的熵最大的分布。</p>
<p>Q：为什么要使用熵最大的分布？</p>
<p>A：最大熵理论</p>
<p>最大熵原理是在1957 年由E.T.Jaynes 提出的，其主要思想是，在只掌握关于未知分布的部分知识时，应该选取符合这些知识但熵值最大的概率分布。因为在这种情况下，符合已知知识的<a href="https://baike.baidu.com/item/概率分布?fromModule=lemma_inlink">概率分布</a>可能不止一个。我们知道，熵定义的实际上是一个<a href="https://baike.baidu.com/item/随机变量?fromModule=lemma_inlink">随机变量</a>的不确定性，熵最大的时候，说明随机变量最不确定，换句话说，也就是随机变量最随机，对其行为做准确预测最困难。</p>
<p>从这个意义上讲，那么最大熵原理的实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的不偏不倚的选择，任何其它的选择都意味着我们增加了其它的约束和假设，这些约束和假设根据我们掌握的信息无法作出。</p>
</blockquote>
<h2 id="二分类线性判别分析（LDA）"><a href="#二分类线性判别分析（LDA）" class="headerlink" title="二分类线性判别分析（LDA）"></a>二分类线性判别分析（LDA）</h2><h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p>“高内聚、低耦合”的思想</p>
<p>从几何角度，让全体样本沿着一个方向投影（高维投影到一维），经过投影后，使得：</p>
<ul>
<li>异类样本的中心尽可能远</li>
<li>同类样本的方差尽可能小</li>
</ul>
<h3 id="损失函数推导-1"><a href="#损失函数推导-1" class="headerlink" title="损失函数推导"></a>损失函数推导</h3><h4 id="异类样本中心尽可能远"><a href="#异类样本中心尽可能远" class="headerlink" title="异类样本中心尽可能远"></a>异类样本中心尽可能远</h4><p>首先是投影，我们假定原来的数据是向量$x$，那么顺着$w$方向的投影就是标量：</p>
<script type="math/tex; mode=display">
z=w^{T} \cdot x(=|w| \cdot|x| \cos \theta)</script><p>经过投影后，异类样本的中心尽可能远：</p>
<script type="math/tex; mode=display">
\max \left\|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{0}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{1}\right\|_{2}^{2} \\</script><h4 id="同类样本的方差尽可能小"><a href="#同类样本的方差尽可能小" class="headerlink" title="同类样本的方差尽可能小"></a>同类样本的方差尽可能小</h4><p>我们假设属于两类的试验样本数量分别是$N_0$和$N_1$，那么我们采用方差矩阵${Var}_i$来表征每一个类内的总体分布，这里我们使用了协方差的定义：</p>
<script type="math/tex; mode=display">
\begin{aligned}
{Var}_1 &=\frac{1}{N_{1}} \sum_{i=1}^{N_{1}}\left(z_{i}-\overline{z_{ 1}}\right)\left(z_{i}-\overline{z_{ 1}}\right)^{T} \\
& =\frac{1}{N_{1}} \sum_{i=1}^{N_{1}}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}-\frac{1}{N_{1}} \sum_{j=1}^{N_{1}} \boldsymbol{w}^{T} \boldsymbol{x}_{j}\right)\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}-\frac{1}{N_{1}} \sum_{j=1}^{N_{1}} \boldsymbol{w}^{T} \boldsymbol{x}_{j}\right)^{T} \\
& =\boldsymbol{w}^{T} \frac{1}{N_{1}} \sum_{i=1}^{N_{1}}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{1}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}_{1}\right)^{T} w \\
& =\boldsymbol{w}^{T} \boldsymbol\Sigma_{1} \boldsymbol{w} \\
{Var}_0 &=\boldsymbol{w}^{T} \boldsymbol\Sigma_{0} \boldsymbol{w}

\end{aligned}</script><p>使同类样本方差尽可能小：</p>
<script type="math/tex; mode=display">
\min \ \boldsymbol{w}^{T} \left(\boldsymbol\Sigma_{0}+\boldsymbol\Sigma_{1}\right) \boldsymbol{w}</script><p>综合这两点，我们用将这两个值相除来得到我们的损失函数，并最大化这个值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max J & =\frac{\left\|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{0}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{1}\right\|_{2}^{2}}{\boldsymbol{w}^{T} \left(\boldsymbol\Sigma_{0}+\boldsymbol\Sigma_{1}\right) \boldsymbol{w}} \\
& =\frac{\left\|\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \boldsymbol{w}\right\|_{2}^{2}}{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\Sigma}_{0}+\boldsymbol{\Sigma}_{1}\right) \boldsymbol{w}} \\
& =\frac{\left[\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \boldsymbol{w}\right]^{\mathrm{T}}\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\Sigma}_{0}+\boldsymbol{\Sigma}_{1}\right) \boldsymbol{w}} \\
& =\frac{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\Sigma}_{0}+\boldsymbol{\Sigma}_{1}\right) \boldsymbol{w}} \\
&=\frac{\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{b} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{w} \boldsymbol{w}}
\end{aligned}</script><p>注意$\mathbf{S}_{w} $和$\mathbf{S}_{b} $ 都是 $d \times d$ 的矩阵</p>
<p>直接对$\boldsymbol{w}$求偏导[2]，因为只关心$\boldsymbol{w}$的方向：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221222155347165.png" alt="image-20221222155347165"></p>
<p>于是求得 $\boldsymbol{w}$。</p>
<p>感觉这部分主要是数学，就不撸代码了。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]<a href="https://www.zhihu.com/question/35322351">https://www.zhihu.com/question/35322351</a></p>
<p>[2]<a href="https://www.bilibili.com/video/BV1aE411o7qd?p=16&amp;vd_source=a207b39881900fb7c1580eaaa0301073">https://www.bilibili.com/video/BV1aE411o7qd?p=16&amp;vd_source=a207b39881900fb7c1580eaaa0301073</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Datawhale组队学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习03：西瓜书第四章 决策树</title>
    <url>/2022/12/22/%5B%E5%90%83%E7%93%9C%E6%95%99%E7%A8%8B%5D%20Task03%20%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC%E5%9B%9B%E7%AB%A0/</url>
    <content><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>主要 follw 教程：<a href="https://datawhale.feishu.cn/docs/doccndJC2sbSfdziNcahCYCx70W#">https://datawhale.feishu.cn/docs/doccndJC2sbSfdziNcahCYCx70W#</a></p>
<h2 id="机器学习三要素"><a href="#机器学习三要素" class="headerlink" title="机器学习三要素"></a>机器学习三要素</h2><ol>
<li>模型：根据具体问题，确定假设空间</li>
<li>策略：根据评价标准，确定选取最优模型的策略（通常会产出一个“损失函数”）</li>
<li>算法：求解损失函数，确定最优模型</li>
</ol>
<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><ul>
<li>从逻辑角度，就是一堆 if else 语句的组合</li>
<li>从几何角度，根据某种准则划分特征空间</li>
<li>最终目的：将样本越分越“纯”</li>
</ul>
<h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>决策树建树算法有三种ID3、C4.5、CART，每个算法主要考虑的事情主要有三个问题：</p>
<ol>
<li>如何选择最优划分属性？</li>
<li>条件判断的属性值是什么？</li>
<li>什么时候停止分裂，达到我们需要的决策？</li>
</ol>
<span id="more"></span>
<h3 id="三种启发函数"><a href="#三种启发函数" class="headerlink" title="三种启发函数"></a>三种启发函数</h3><p>ID3、C4.5、CART</p>
<h4 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h4><blockquote>
<ol>
<li>如何选择最优划分属性？<ul>
<li>以信息增益为准则，选择信息增益最大的属性划分</li>
</ul>
</li>
<li>条件判断的具体划分点是什么？<ul>
<li>离散值穷举各个取值，$k$个取值就有$k$个节点；连续值的话先对所有取值排序，再依次取中点（$k-1$个）作为划分点，假设$t$是$a_i$和$a_{i+1}$的中点，那么$t$对应的划分点（区间）就是$a \in [a_i,a_{i+1})$。</li>
</ul>
</li>
<li>什么时候停止分裂，达到我们需要的决策？<ul>
<li>预剪枝<ul>
<li>树到一定深度</li>
<li>当前结点样本数小于某个阈值</li>
<li>计算每次分裂对测试集准确率的提升，小于某个阈值时停止</li>
</ul>
</li>
</ul>
</li>
</ol>
</blockquote>
<p>采用<strong>信息熵</strong>衡量样本的“纯度”，以离散型为例，将样本类别标记$y$视为随机变量，各个类别在样本集合$D$中的占比$p_k(k=1,2,…,|\mathcal{Y}|)$视作各个类别取值的概率，则全体样本$D$的信息熵为：</p>
<script type="math/tex; mode=display">
\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_{k} \log _{2} p_{k}</script><p>信息熵越大，表示越混乱，纯度越低。</p>
<p>定义每个属性作为划分属性时的<strong>信息增益</strong>，即已知属性（特征）$a$的取值后$y$的不确定性减少的量，也即纯度的提升：</p>
<script type="math/tex; mode=display">
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)</script><p>ID3决策树<strong>以信息增益为准则</strong>来选择划分属性的决策树，<strong>每次选择信息增益最大的属性作为根节点分裂</strong>，分裂的叶子结点为该属性的若干取值，不断迭代。</p>
<h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h4><blockquote>
<ol>
<li>如何选择最优划分属性？<ul>
<li>以增益率为准则，选择增益率最大的属性划分</li>
</ul>
</li>
<li>条件判断的具体划分点是什么？<ul>
<li>离散值穷举各个取值，$k$个取值就有$k$个节点；连续值的话先对所有取值排序，再依次取中点（$k-1$个）作为划分点，假设$t$是$a_i$和$a_{i+1}$的中点，那么$t$对应的划分点（区间）就是$a \in [a_i,a_{i+1})$。</li>
</ul>
</li>
<li>什么时候停止分裂，达到我们需要的决策？<ul>
<li>预剪枝<ul>
<li>树到一定深度</li>
<li>当前结点样本数小于某个阈值</li>
<li>计算每次分裂对测试集准确率的提升，小于某个阈值时停止</li>
</ul>
</li>
</ul>
</li>
</ol>
</blockquote>
<h5 id="信息增益的缺点"><a href="#信息增益的缺点" class="headerlink" title="信息增益的缺点"></a>信息增益的缺点</h5><p>当某个属性取值太多时，每个划分的信息增益都很高（样本序号），所以会对取值数目较多的属性有所偏好</p>
<h5 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h5><p>改用<strong>增益率</strong>作为划分准则：</p>
<script type="math/tex; mode=display">
\text { Gain\_ratio }(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}</script><p>对信息增益除以<strong>取值熵</strong>[1]：</p>
<script type="math/tex; mode=display">
\operatorname{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}</script><p>如何理解？相当于对信息增益进行了一个惩罚，如果该属性取值很多，那么取值熵$\operatorname{IV}(a)$就越大，惩罚越大，导致增益率相应减少，缓解信息增益对“对取值数目较多的属性有所偏好”的问题。</p>
<h4 id="CART-（重要）"><a href="#CART-（重要）" class="headerlink" title="==CART==（重要）"></a>==CART==（重要）</h4><p>CART（Classification And Regression Tree），分类与回归树，是一棵<strong>二叉树</strong>！</p>
<p>西瓜书上只给了CART作为回归树的例子，但正如其名，CART既可以作为回归树也可以作为分类树。</p>
<h5 id="CART分类树"><a href="#CART分类树" class="headerlink" title="CART分类树"></a>CART分类树</h5><blockquote>
<ol>
<li>如何选择最优划分属性？<ul>
<li>以基尼指数为准则，选择基尼指数最小的“属性-属性值”划分</li>
</ul>
</li>
<li>条件判断的具体划分点是什么？<ul>
<li>离散值穷举各个属性和属性值；连续值的话先对某一属性$a_v$下的所有$|a_v|$个属性值排序，再依次取中点（$|a_v|-1$个）作为划分点，假设$m$是$a_{v_i}$和$a_{v_{i+1}}$的中点，那么$m$对应的划分点（区间）就是$a \in [a_{v_{i}},a_{v_{i+1}})$。</li>
</ul>
</li>
<li>什么时候停止分裂，达到我们需要的决策？<ul>
<li>预剪枝<ul>
<li>树到一定深度</li>
<li>当前结点样本数小于某个阈值</li>
<li>计算每次分裂对测试集准确率的提升，小于某个阈值时停止</li>
</ul>
</li>
</ul>
</li>
</ol>
</blockquote>
<p>CART树使用<strong>基尼指数</strong>（Gini index）来选择划分属性，数据集$D$的基尼值定义为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{Gini}(D) & =\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\
& =\sum_{k=1}^{|\mathcal{Y}|} p_{k}\left(1-p_{k}\right) \\
& =1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2}
\end{aligned}</script><p>基尼值可以用来表示样本的<strong>纯度</strong>：从样本集合$D$中随机抽取两个样本，其标记不一致的概率。因此，基尼值越小，碰到异类的概率就越小，纯度自然就越高。</p>
<p>定义每个属性作为划分属性时基尼指数：</p>
<script type="math/tex; mode=display">
\text { Gini\_index }(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)</script><p>CART树<strong>以基尼指数为准则</strong>来选择划分属性的决策树，<strong>每次选择基尼指数最小（分裂后纯度最高）的属性作为根节点分裂</strong>，分裂的叶子结点为该属性的若干取值，不断迭代。</p>
<h5 id="CART回归树"><a href="#CART回归树" class="headerlink" title="CART回归树"></a>CART回归树</h5><blockquote>
<ol>
<li>如何选择最优划分属性？<ul>
<li>以<strong>平方误差</strong>为准则，选择平方误差最小的“属性-属性值”划分</li>
</ul>
</li>
<li>条件判断的具体划分点是什么？<ul>
<li>离散值穷举各个属性和属性值；连续值的话先对某一属性$a_v$下的所有$|a_v|$个属性值排序，再依次取中点（$|a_v|-1$个）作为划分点，假设$m$是$a_{v_i}$和$a_{v_{i+1}}$的中点，那么$m$对应的划分点（区间）就是$a \in [a_{v_{i}},a_{v_{i+1}})$。</li>
</ul>
</li>
<li>什么时候停止分裂，达到我们需要的决策？<ul>
<li>预剪枝<ul>
<li>树到一定深度</li>
<li>当前结点样本数小于某个阈值</li>
<li>计算每次分裂对测试集准确率的提升，小于某个阈值时停止</li>
</ul>
</li>
</ul>
</li>
</ol>
</blockquote>
<p>参考这里[3]以及之前整理过的xmind</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20221222225705247.png" alt="image-20221222225705247" style="zoom:50%;" /></p>
<h2 id="决策树的剪枝处理"><a href="#决策树的剪枝处理" class="headerlink" title="决策树的剪枝处理"></a>决策树的剪枝处理</h2><p>剪枝分为预剪枝和后剪枝。</p>
<ul>
<li>预剪枝，在生成决策树的过程中提前停止树的增长。</li>
<li>后剪枝，在已生成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树</li>
</ul>
<h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>在自顶向下的过程中，常见算法有：</p>
<ol>
<li>树到一定深度时停止生长</li>
<li>当前结点样本数小于某个阈值停止生长</li>
<li>计算每次分裂对测试集准确率的提升，小于某个阈值时停止</li>
<li>···</li>
</ol>
<h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>自底向上，常见算法有：</p>
<ol>
<li>错误率降低剪枝（REP）</li>
<li>代价复杂度剪枝（CCP）[1]</li>
<li>···</li>
</ol>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>参考强哥[2]</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]百面机器学习p64、p68</p>
<p>[2]<a href="https://zhongqiang.blog.csdn.net/article/details/116712254?spm=1001.2014.3001.5502">https://zhongqiang.blog.csdn.net/article/details/116712254?spm=1001.2014.3001.5502</a></p>
<p>[3]<a href="https://www.bilibili.com/video/BV1mN411Z7j1/?spm_id_from=333.999.0.0">https://www.bilibili.com/video/BV1mN411Z7j1/?spm_id_from=333.999.0.0</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Datawhale组队学习</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>（待更新）推荐系统：经典算法——协同过滤（Collebrative Filtering）</title>
    <url>/2021/11/23/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/</url>
    <content><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>经典Movielens数据集</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">All ratings are contained in the file &quot;ratings.dat&quot; and are in the</span><br><span class="line">following format:</span><br><span class="line"></span><br><span class="line">UserID::MovieID::Rating::Timestamp</span><br><span class="line"></span><br><span class="line">- UserIDs range between 1 and 6040 </span><br><span class="line">- MovieIDs range between 1 and 3952</span><br><span class="line">- Ratings are made on a 5-star scale (whole-star ratings only)</span><br><span class="line">- Timestamp is represented in seconds since the epoch as returned by time(2)</span><br><span class="line">- Each user has at least 20 ratings</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><p>采用K-fold交叉验证，将用户行为数据均匀分成K份，其中一份作为测试集，K-1份作为训练集。协同过滤算法只考虑物品/用户的共现关系，所以用户序列都用集合表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span>(<span class="params">data, M, k, seed</span>):</span></span><br><span class="line">    test = []</span><br><span class="line">    train = []</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">0</span>, M) == k:</span><br><span class="line">            test.append([user, item])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train.append([user, item])</span><br><span class="line">    train_ = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">    test_ = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> train:</span><br><span class="line">        train_[user].add(item)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> test:</span><br><span class="line">        test_[user].add(item)</span><br><span class="line">    <span class="keyword">return</span> train_, test_</span><br></pre></td></tr></table></figure>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>召回率Recall，准确率Precision，覆盖率Coverage，新颖度Popularity。</p>
<span id="more"></span>
<p>召回率Recall：正确推荐的商品占所有应该推荐的商品的比例，即应该推荐的推荐了多少。公式描述：对用户u推荐N个物品（$R(u)$），令用户在测试集上喜欢的物品集合为$T(u)$，则</p>
<script type="math/tex; mode=display">
Recall=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |T(u)|}</script><p>准确率Precision：正确推荐的商品占推荐的商品列表的比例，即有多少推荐对了。公式描述：</p>
<script type="math/tex; mode=display">
Precision=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |R(u)|}</script><p>覆盖率Coverage：推荐的商品占所有商品的比例，即推荐的商品覆盖了多少所有商品。反映发掘长尾的能力。</p>
<script type="math/tex; mode=display">
Coverage = \frac{\bigcup_u R(u)}{|I|} \ \  , \ \bigcup:并集</script><p>新颖度Popularity：刻画推荐物品的平均流行度，平均流行度（Popularity）越高，新颖度越低。$Popularity(x)$定义为$x$在所有用户序列中出现的次数，出现次数越多，流行度越高。</p>
<script type="math/tex; mode=display">
Popularity= \sum _u \sum _ { i \in R(u) } \log (Popularity(i)+1)</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评价指标:召回率、准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Metric</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    hit = <span class="number">0</span></span><br><span class="line">    recall_all = <span class="number">0</span>      <span class="comment"># recall 的分母</span></span><br><span class="line">    precision_all = <span class="number">0</span>   <span class="comment"># precision 的分母</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        tu = test[user]</span><br><span class="line">        rank = all_recommend_list[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> tu:</span><br><span class="line">                hit += <span class="number">1</span></span><br><span class="line">        recall_all += <span class="built_in">len</span>(tu)</span><br><span class="line">        precision_all += N</span><br><span class="line">    recall = hit / (recall_all * <span class="number">1.0</span>)</span><br><span class="line">    precision = hit / (precision_all * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> recall, precision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：覆盖率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Coverage</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    recommend_items = <span class="built_in">set</span>()</span><br><span class="line">    all_items = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> train[user]:</span><br><span class="line">            all_items.add(item)</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            recommend_items.add(item)</span><br><span class="line">    coverage = <span class="built_in">len</span>(recommend_items) / (<span class="built_in">len</span>(all_items) * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> coverage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：新颖度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Popularity</span>(<span class="params">train, test, N, recommend_res</span>):</span>	<span class="comment"># N:推荐N个物品</span></span><br><span class="line">    item_popularity = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_popularity:</span><br><span class="line">                item_popularity[item] = <span class="number">0</span></span><br><span class="line">            item_popularity[item] += <span class="number">1</span></span><br><span class="line">    popularity = <span class="number">0</span></span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        rank = recommend_res[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            popularity += math.log(<span class="number">1</span> + item_popularity[item])</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    popularity /= n * <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> popularity</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统实战</category>
      </categories>
  </entry>
  <entry>
    <title>推荐系统基础1：评价指标、召回&amp;精排</title>
    <url>/2022/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%801/</url>
    <content><![CDATA[<hr>
<h1 id="任务1：推荐系统基础"><a href="#任务1：推荐系统基础" class="headerlink" title="任务1：推荐系统基础"></a>任务1：推荐系统基础</h1><ul>
<li>阅读推荐系统在工业落地的链接：<ul>
<li><a href="https://mp.weixin.qq.com/s/WXcfdzz7vts9UYBVxWs3AA">推荐系统整体架构及算法流程详解</a></li>
<li><a href="https://tech.meituan.com/2017/03/24/travel-recsys.html">美团旅游推荐系统的演进</a></li>
<li><a href="https://www.alibabacloud.com/zh/product/airec">阿里智能推荐AIRec</a></li>
</ul>
</li>
<li>思考 &amp; 回答以下问题，并将回答记录到博客<ul>
<li>推荐系统与常见的结构化问题的区别是什么？</li>
<li>如何评价推荐系统「推荐」的准不准？</li>
<li>推荐系统一般分为召回 &amp; 排序，为什么这样划分？</li>
</ul>
</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="推荐系统与常见的结构化问题的区别是什么？"><a href="#推荐系统与常见的结构化问题的区别是什么？" class="headerlink" title="推荐系统与常见的结构化问题的区别是什么？"></a>推荐系统与常见的结构化问题的区别是什么？</h2><p>结构化数据即表格数据（tabular data），绝大多数数据都是表格数据。</p>
<p>推荐系统有user信息，item信息，这些数据虽然都是结构化的，但是推荐系统涉及到user和item的交互，所以不仅仅是一条结构化数据预测一个分类or一个数值那么简单，还需要从交互中抽象出用户兴趣，例如将用户交互建模为序列，这就不是结构化问题了。</p>
<h2 id="如何评价推荐系统「推荐」的准不准？"><a href="#如何评价推荐系统「推荐」的准不准？" class="headerlink" title="如何评价推荐系统「推荐」的准不准？"></a>如何评价推荐系统「推荐」的准不准？</h2><p>常用的评价指标有：召回率Recall，准确率Precision，覆盖率Coverage，新颖度Popularity。</p>
<p>召回率Recall：正确推荐的商品占所有应该推荐的商品的比例，即应该推荐的推荐了多少。公式描述：对用户u推荐N个物品（$R(u)$），令用户在测试集上喜欢的物品集合为$T(u)$，则</p>
<script type="math/tex; mode=display">
Recall=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |T(u)|}</script><p>准确率Precision：正确推荐的商品占推荐的商品列表的比例，即有多少推荐对了。公式描述：</p>
<script type="math/tex; mode=display">
Precision=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |R(u)|}</script><p>覆盖率Coverage：推荐的商品占所有商品的比例，即推荐的商品覆盖了多少所有商品。反映发掘长尾的能力。</p>
<script type="math/tex; mode=display">
Coverage = \frac{\bigcup_u R(u)}{|I|} \ \  , \ \bigcup:并集</script><p>新颖度Popularity：刻画推荐物品的平均流行度，平均流行度（Popularity）越高，新颖度越低。$Popularity(x)$定义为$x$在所有用户序列中出现的次数，出现次数越多，流行度越高。</p>
<script type="math/tex; mode=display">
Popularity= \sum _u \sum _ { i \in R(u) } \log (Popularity(i)+1)</script><p>AUC曲线：AUC（Area Under Curve），ROC曲线下与坐标轴围成的面积。在讲AUC前需要理解混淆矩阵，召回率，精确率，ROC曲线等概念。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/v2-a253b01cf7f141b9ad11eefdf3cf58d3_1440w.jpg" alt="img"></p>
<p>根据混淆矩阵的定义，以另一种形式定义召回率和精确率：</p>
<script type="math/tex; mode=display">
Recall = \frac{TP}{TP+FN} \\
Precision = \frac{TP}{TP+FP}</script><p>ROC曲线的横坐标为假阳性率（False Positive Rate, FPR），$FPR=\frac{FP}{FP+TN}$，N是真实负样本的个数， FP是N个负样本中被分类器预测为正样本的个数。<strong>FPRate的意义是所有真实类别为0的样本中，预测类别为1的比例。</strong></p>
<p>纵坐标为真阳性率（True Positive Rate, TPR），$TPR=\frac{TP}{TP+FN}$，P是真实正样本的个数，TP是P个正样本中被分类器预测为正样本的个数。<strong>TPRate的意义是所有真实类别为1的样本中，预测类别为1的比例。</strong></p>
<p><img src="https://camo.githubusercontent.com/07a924ef2229334f903f1ba3e5cd17115a16159dcb1756cda93232b3cf998c0d/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f4a6176616175632e706e67" alt="img"></p>
<p>AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。对ROC更细致的解释：<a href="https://www.zhihu.com/question/39840928/answer/241440370">如何理解机器学习和统计中的AUC？ - 知乎 (zhihu.com)</a></p>
<p>下面是部分评价指标的代码实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评价指标:召回率、准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Metric</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    hit = <span class="number">0</span></span><br><span class="line">    recall_all = <span class="number">0</span>      <span class="comment"># recall 的分母</span></span><br><span class="line">    precision_all = <span class="number">0</span>   <span class="comment"># precision 的分母</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        tu = test[user]</span><br><span class="line">        rank = all_recommend_list[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> tu:</span><br><span class="line">                hit += <span class="number">1</span></span><br><span class="line">        recall_all += <span class="built_in">len</span>(tu)</span><br><span class="line">        precision_all += N</span><br><span class="line">    recall = hit / (recall_all * <span class="number">1.0</span>)</span><br><span class="line">    precision = hit / (precision_all * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> recall, precision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：覆盖率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Coverage</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    recommend_items = <span class="built_in">set</span>()</span><br><span class="line">    all_items = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> train[user]:</span><br><span class="line">            all_items.add(item)</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            recommend_items.add(item)</span><br><span class="line">    coverage = <span class="built_in">len</span>(recommend_items) / (<span class="built_in">len</span>(all_items) * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> coverage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：新颖度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Popularity</span>(<span class="params">train, test, N, recommend_res</span>):</span>	<span class="comment"># N:推荐N个物品</span></span><br><span class="line">    item_popularity = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_popularity:</span><br><span class="line">                item_popularity[item] = <span class="number">0</span></span><br><span class="line">            item_popularity[item] += <span class="number">1</span></span><br><span class="line">    popularity = <span class="number">0</span></span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        rank = recommend_res[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            popularity += math.log(<span class="number">1</span> + item_popularity[item])</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    popularity /= n * <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> popularity</span><br></pre></td></tr></table></figure>
<h2 id="推荐系统一般分为召回-amp-精排，为什么这样划分？"><a href="#推荐系统一般分为召回-amp-精排，为什么这样划分？" class="headerlink" title="推荐系统一般分为召回 &amp; 精排，为什么这样划分？"></a>推荐系统一般分为召回 &amp; 精排，为什么这样划分？</h2><blockquote>
<p>物品集量级非常大，召回先选出一部分候选物品，再对这一部分候选物品做精排，计算开销相比于对所有物品做精排大大降低了。</p>
</blockquote>
<p>相关知识点：召回、排序（精排）。</p>
<h3 id="召回"><a href="#召回" class="headerlink" title="召回"></a>召回</h3><p>从海量的物品中，先筛选出一小部分物品作为推荐的候选集。</p>
<h4 id="召回的目的"><a href="#召回的目的" class="headerlink" title="召回的目的"></a>召回的目的</h4><p>当用户和物品量比较大时，如果直接精排（计算预测得分）复杂度会非常高。计算预测得分通常是计算用户和物品的向量内积，假设 user 和 item 的 embedding 维度都是 D ，用户数为 M ，物品数为 N ，那么计算这个得分的复杂度就是 $O(D^2) *O(MN)$。当 M 和 N 都是百万量级、亿量级时，计算开销会非常大。</p>
<p>如果可以先从海量的物品中，先筛选出一小部分用户最可能喜欢的物品（召回），例如先选出 N/100 的物品，那么复杂度就是 $\frac{O(D^2) *O(MN)}{100}$ ，降低为原来的一百分之一，计算效率更高了。实际场景中，做热销召回的量级可能是百级，这样一来从百万量级的物品数降低到百量级的物品数，计算开销大大降低！另一方面，大量内容中真正的精品只是少数，对所有内容都计算将非常的低效，会浪费大量资源和时间。</p>
<h4 id="召回的重要性"><a href="#召回的重要性" class="headerlink" title="召回的重要性"></a>召回的重要性</h4><p>虽然精排模型一直是优化的重点，但召回模型也非常的重要，因为如果召回的内容不对，怎么精排都是错误的。</p>
<h4 id="召回的方法"><a href="#召回的方法" class="headerlink" title="召回的方法"></a>召回的方法</h4><ol>
<li>热销召回：将一段时间内的热门内容召回。</li>
<li>协同召回：基于用户与用户行为的相似性推荐，可以很好的突破一定的限制，发现用户潜在的兴趣偏好。</li>
<li>标签召回：根据每个用户的行为，构建标签，并根据标签召回内容。</li>
<li>时间召回：将一段时间内最新的内容召回，在新闻视频等有时效性的领域常用。是常见的几种召回方法。</li>
</ol>
<h4 id="多路召回"><a href="#多路召回" class="headerlink" title="多路召回"></a>多路召回</h4><p>一开始我们可能有成千上万的物品，首先要由召回（也叫触发，recall）来挖掘出原则上任何用户有可能感兴趣的东西。这个环节是入口。有时候，单独的召回可能难以做到照顾所有方面，这个时候就需要多路召回。所谓的“多路召回”策略，就是指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用。下图只是一个多路召回的例子，也就是说可以使用多种不同的策略来获取用户排序的候选商品集合，<strong>而具体使用哪些召回策略其实是与业务强相关的</strong>，针对不同的任务就会有对于该业务真实场景下需要考虑的召回规则。例如视频推荐，召回规则可以是“热门视频”、“导演召回”、“演员召回”、“最近上映“、”流行趋势“、”类型召回“等等。</p>
<p><img src="https://camo.githubusercontent.com/5194f61aac70bfec14ede3fa6b27aed0670f2cd59b5e9ad688f1f526a4c5f658/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731373230313431313336372e706e67237069635f63656e746572" alt="img" style="zoom:67%;" /></p>
<h4 id="Embedding召回"><a href="#Embedding召回" class="headerlink" title="Embedding召回"></a>Embedding召回</h4><h3 id="精排"><a href="#精排" class="headerlink" title="精排"></a>精排</h3><p>排序负责将多个召回策略的结果进行个性化排序。</p>
<h4 id="精排的重要性"><a href="#精排的重要性" class="headerlink" title="精排的重要性"></a>精排的重要性</h4><p>精排是最纯粹的排序，也是最纯粹的机器学习模块。它的目标只有一个，就是<strong>根据手头所有的信息输出最准</strong>的预测。精排一直是优化的重点。召回的物品中，筛选出用户最感兴趣的物品，进一步做出个性化排序，才最终达到推荐的目的。</p>
<h4 id="精排模型"><a href="#精排模型" class="headerlink" title="精排模型"></a>精排模型</h4><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/icTMNdGHpfJYqcAFSwiaWKjeqTweM9aJrNKqZVvMn2GZvoDTnPHjYMVywvGicII8P9d4nMjib5Jia8kGlDbicibTGSPlQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" /></p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础2：Movielens数据集介绍</title>
    <url>/2022/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%802/</url>
    <content><![CDATA[<hr>
<h1 id="任务2：Movielens介绍"><a href="#任务2：Movielens介绍" class="headerlink" title="任务2：Movielens介绍"></a>任务2：Movielens介绍</h1><ul>
<li>下载并读取Movielens 1M数据集（用户、电影、评分）</li>
<li>统计如下指标：<ul>
<li>总共包含多少用户？</li>
<li>总共包含多个电影？</li>
<li>平均每个用户对多少个电影进行了评分？</li>
<li>每部电影 &amp; 每个用户的平均评分是？</li>
</ul>
</li>
<li>如果你来进行划分数据集为训练和验证，你会如何划分？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="统计指标"><a href="#统计指标" class="headerlink" title="统计指标"></a>统计指标</h2><ul>
<li>总共包含 6040 个用户</li>
<li>总共包含 3883 部电影</li>
<li>平均每个用户对 165.6 部电影进行评分</li>
</ul>
<p>其余细节见 notebook 。</p>
<h2 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>参考项亮《推荐系统实践》，将用户行为数据集按照均匀分布随机分成 K 份，挑选一份作为测试集，剩下的 K-1 份作为训练集，进行 K 次实验，然后将 K 次评测指标的平均值作为最终评测指标 （即 K-fold 交叉验证）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span>(<span class="params">data, K, i, seed</span>):</span></span><br><span class="line">    test = []</span><br><span class="line">    train = []</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">0</span>,K)==i:</span><br><span class="line">            test.append([user,item])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train.append([user,item])</span><br><span class="line">    <span class="keyword">return</span> train,test</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础3：协同过滤基础</title>
    <url>/2022/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%803/</url>
    <content><![CDATA[<hr>
<h1 id="任务3：协同过滤基础"><a href="#任务3：协同过滤基础" class="headerlink" title="任务3：协同过滤基础"></a>任务3：协同过滤基础</h1><ul>
<li><a href="https://github.com/datawhalechina/fun-rec/blob/master/docs/第一章 推荐系统基础/1.1 基础推荐算法/1.1.2 协同过滤.md">阅读协同过滤教程</a></li>
<li>编写代码计算两个用户的相似度</li>
<li>编写代码计算两个物品的相似度</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>协同过滤（Collaborative Filtering）推荐算法是最经典、最常用的推荐算法。</p>
<p>所谓协同过滤， 基本思想是<strong>根据用户之前的喜好</strong>以及<strong>其他兴趣相近的用户的选择</strong>来给用户推荐物品(基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐)，<strong>一般是仅仅基于用户的行为数据（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）</strong>。目前应用比较广泛的协同过滤算法是基于邻域的方法， 而这种方法主要有下面两种算法：</p>
<ul>
<li><strong>基于用户的协同过滤算法(UserCF)</strong>: 给用户推荐和他兴趣相似的其他用户喜欢的产品</li>
<li><strong>基于物品的协同过滤算法(ItemCF)</strong>: 给用户推荐和他之前喜欢的物品相似的物品</li>
</ul>
<p>不管是UserCF还是ItemCF算法， 非常重要的步骤之一就是计算用户和用户或者物品和物品之间的<strong>相似度</strong>， 所以下面先整理常用的相似性度量方法， 然后再对每个算法的具体细节进行展开。</p>
<h2 id="相似性度量方法"><a href="#相似性度量方法" class="headerlink" title="相似性度量方法"></a>相似性度量方法</h2><h4 id="杰卡德相似系数"><a href="#杰卡德相似系数" class="headerlink" title="杰卡德相似系数"></a><strong>杰卡德相似系数</strong></h4><p>杰卡德(Jaccard)相似系数是衡量两个<strong>集合的相似度</strong>一种指标。两个用户$u$和$v$交互商品交集的数量占这两个用户交互商品并集的数量的比例，称为两个集合的杰卡德相似系数，用符号$sim_{uv}$表示，其中$N(u),N(v)$分别表示用户$u$和用户$v$交互商品的集合。 <script type="math/tex">sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)| \cup|N(v)|}}</script> 由于杰卡德相似系数一般无法反映具体用户的评分喜好信息， 所以常用来评估用户<strong>是否</strong>会对某商品进行打分， 而不是预估用户会对某商品打多少分。</p>
<h4 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a><strong>余弦相似度</strong></h4><p>余弦相似度(cosine similarity) 衡量了两个向量的夹角，夹角越小越相似。首先从集合的角度描述余弦相似度，相比于Jaccard公式来说就是分母有差异，不是两个用户交互商品的并集的数量，而是两个用户分别交互的商品数量的乘积，公式如下：</p>
<script type="math/tex; mode=display">
sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}</script><p>从向量的角度进行描述，令矩阵$A$为用户-商品交互矩阵(因为是TopN推荐并不需要用户对物品的评分，只需要知道用户对商品是否有交互就行)，即矩阵的每一行表示一个用户对所有商品的交互情况，有交互的商品值为1没有交互的商品值为0，矩阵的列表示所有商品。若用户和商品数量分别为$m,n$的话，交互矩阵$A$就是一个$m$行$n$列的矩阵。此时用户的相似度可以表示为(其中$u\cdot v$指的是向量点积)： </p>
<script type="math/tex; mode=display">
sim_{uv} = cos(u,v) =\frac{u\cdot v}{|u|\cdot |v|}</script><p>上述用户-商品交互矩阵在现实情况下是非常的稀疏了，为了避免存储这么大的稀疏矩阵，在计算用户相似度的时候一般会采用集合的方式进行计算。理论上向量之间的相似度计算公式都可以用来计算用户之间的相似度，但是会根据实际的情况选择不同的用户相似度度量方法。</p>
<p>这个在具体实现的时候， 可以使用<code>cosine_similarity</code>进行实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>]</span><br><span class="line">cosine_similarity([i, j])</span><br></pre></td></tr></table></figure>
<h4 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h4><p>皮尔逊相关系数的公式与余弦相似度的计算公式非常的类似，首先对于上述的余弦相似度的计算公式写成求和的形式 ：</p>
<script type="math/tex; mode=display">
sim_{uv} = \frac{\sum_i r_{ui}·r_{vi}}{\sqrt{\sum_i r_{ui}^2}\sqrt{\sum_i r_{vi}^2}}</script><p>其中$r_{ui},r_{vi}$分别表示用户$u$和用户$v$对商品$i$是否有交互(或者具体的评分值)。</p>
<p>如下是皮尔逊相关系数计算公式：</p>
<script type="math/tex; mode=display">
sim(u,v)=\frac{\sum_{i\in I}(r_{ui}-\bar r_u)(r_{vi}-\bar r_v)}{\sqrt{\sum_{i\in I }(r_{ui}-\bar r_u)^2}\sqrt{\sum_{i\in I }(r_{vi}-\bar r_v)^2}}</script><p>其中$r_{ui},r_{vi}$分别表示用户$u$和用户$v$对商品$i$是否有交互(或者具体的评分值)，$\bar r_u, \bar r_v$分别表示用户$u$和用户$v$交互的所有商品交互数量或者具体评分的平均值。所以相比余弦相似度，皮尔逊相关系数通过使用用户的<strong>平均分对各独立评分进行修正</strong>，减小了用户评分偏置的影响。具体实现， 我们也是可以调包， 这个计算方式很多， 下面是其中的一种：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"></span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>]</span><br><span class="line">pearsonr(i, j)</span><br></pre></td></tr></table></figure>
<p><a href="https://scipy.github.io/devdocs/reference/generated/scipy.stats.pearsonr.html?highlight=pearson#scipy.stats.pearsonr">皮尔逊相关系数 scipy官方文档</a></p>
<h2 id="编写代码计算两个用户、物品的相似度"><a href="#编写代码计算两个用户、物品的相似度" class="headerlink" title="编写代码计算两个用户、物品的相似度"></a>编写代码计算两个用户、物品的相似度</h2><p>详见notebook</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础4：协同过滤进阶进阶</title>
    <url>/2022/04/23/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%804/</url>
    <content><![CDATA[<hr>
<h1 id="任务4：协同过滤进阶"><a href="#任务4：协同过滤进阶" class="headerlink" title="任务4：协同过滤进阶"></a>任务4：协同过滤进阶</h1><ul>
<li>编写User-CF代码，通过用户相似度得到电影推荐</li>
<li>编写Item-CF代码，通过物品相似度得到电影推荐</li>
<li>进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="基于用户的协同过滤"><a href="#基于用户的协同过滤" class="headerlink" title="基于用户的协同过滤"></a>基于用户的协同过滤</h2><h3 id="UserCF原理介绍"><a href="#UserCF原理介绍" class="headerlink" title="UserCF原理介绍"></a>UserCF原理介绍</h3><p>基于用户的协同过滤算法(UserCF)的假设是：<strong>相似用户的兴趣也相似</strong>。所以，当一个用户A需要个性化推荐的时候， 我们可以先找到和他有相似兴趣的其他用户， 然后把那些用户喜欢的， 而用户A没有听说过的物品推荐给A。</p>
<p><img src="https://camo.githubusercontent.com/4cf6c62c1f1e533dffdd89db9bc045b560c95444a4a6e61b7b2d49cf28703f06/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2fe59bbee78987696d6167652d32303231303632393233323534303238392e706e67" alt="image-20210629232540289"  /></p>
<p><strong>UserCF算法主要包括两个步骤：</strong></p>
<ol>
<li>找到和目标用户兴趣相似的用户集合</li>
<li>找到这个集合中的用户喜欢的， 且目标用户没有听说过的物品推荐给目标用户。</li>
</ol>
<p>上面的两个步骤中， 第一个步骤里面， 我们会基于前面给出的相似性度量的方法找出与目标用户兴趣相似的用户， 而第二个步骤里面， 如何基于相似用户喜欢的物品来对目标用户进行推荐呢？ 这个要依赖于目标用户对相似用户喜欢的物品的一个喜好程度， 那么如何衡量这个程度大小呢？ 为了更好理解上面的两个步骤， 下面拿一个具体的例子把两个步骤具体化。</p>
<p><strong>以下图为例，此例将会用于本文各种算法中</strong></p>
<p><img src="https://camo.githubusercontent.com/38b1abf510c90bc3a8850a09ba19e39ea11b1f83b916f0432cc29fdaed665e5c/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f254535253942254245254537253839253837696d6167652d32303231303632393233323632323735382e706e67" alt="image-20210629232622758"></p>
<p>给用户推荐物品的过程可以<strong>形象化为一个猜测用户对商品进行打分的任务</strong>，上面表格里面是5个用户对于5件物品的一个打分情况，可以理解为用户对物品的喜欢程度</p>
<p>应用UserCF算法的两个步骤：</p>
<ol>
<li>首先根据前面的这些打分情况(或者说已有的用户向量）计算一下Alice和用户1， 2， 3， 4的相似程度， 找出与Alice最相似的n个用户</li>
<li>根据这n个用户对物品5的评分情况和与Alice的相似程度会猜测出Alice对物品5的评分， 如果评分比较高的话， 就把物品5推荐给用户Alice， 否则不推荐。</li>
</ol>
<p>关于第一个步骤， 上面已经给出了计算两个用户相似性的方法， 这里不再过多赘述， 这里主要解决第二个问题， 如何产生最终结果的预测。</p>
<p><strong>最终结果的预测</strong></p>
<p>根据上面的几种方法， 我们可以计算出向量之间的相似程度， 也就是可以计算出Alice和其他用户的相近程度， 这时候我们就可以选出与Alice最相近的前n个用户， 基于他们对物品5的评价猜测出Alice的打分值， 那么是怎么计算的呢？</p>
<p>这里常用的方式之一是<strong>利用用户相似度和相似用户的评价加权平均获得用户的评价预测</strong>， 用下面式子表示：</p>
<script type="math/tex; mode=display">
R_{\mathrm{u}, \mathrm{p}}=\frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot R_{\mathrm{s}, \mathrm{p}}\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}</script><p>这个式子里面， 权重$w_{u,s}$是用户$u$和用户$s$的相似度， $R_{s,p}$是用户$s$对物品$p$的评分。</p>
<p>还有一种方式如下：</p>
<script type="math/tex; mode=display">
P_{i, j}=\bar{R}_{i}+\frac{\sum_{k=1}^{n}\left(S_{i, k}\left(R_{k, j}-\bar{R}_{k}\right)\right)}{\sum_{k=1}^{n} S_{i, k}}</script><p>这种方式考虑的更加全面， 依然是用户相似度作为权值， 但后面不单纯是其他用户对物品的评分， 而是<strong>该物品的评分与此用户的所有评分的差值进行加权平均， 这时候考虑到了有的用户内心的评分标准不一的情况</strong>， 即有的用户喜欢打高分， 有的用户喜欢打低分的情况。</p>
<p>所以这一种计算方式更为推荐。下面的计算将使用这个方式。这里的$S_{i,k}$与上面的$w_{u,s}$的意思是类似的，表示的是用户i和用户k之间的相似度。</p>
<p>在获得用户$u$对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。 至此，基于用户的协同过滤算法的推荐过程完成。</p>
<p>根据上面的问题， 下面手算一下：</p>
<p>目标: 猜测Alice对物品5的得分：</p>
<ol>
<li><strong>计算Alice与其他用户的相似度（这里使用皮尔逊相关系数）</strong>:</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    ratings=&#123;<span class="string">&#x27;Alice&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user1&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user2&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user3&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user4&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> ratings</span><br><span class="line">ratings = loadData()</span><br><span class="line">ratings = pd.DataFrame(ratings).T</span><br><span class="line">ratings</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422153425763.png" alt="image-20220422153425763"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取出用户向量</span></span><br><span class="line">Alice = ratings.loc[<span class="string">&#x27;Alice&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user1 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user2 = ratings.loc[<span class="string">&#x27;user2&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user3 = ratings.loc[<span class="string">&#x27;user3&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user4 = ratings.loc[<span class="string">&#x27;user4&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义皮尔逊相似度</span></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearsonrSim</span>(<span class="params">x,y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    皮尔森相似度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> pearsonr(x,y)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算Alice和其它用户的相似度</span></span><br><span class="line">Alice_user1_similarity = pearsonrSim(Alice,user1)</span><br><span class="line">Alice_user2_similarity = pearsonrSim(Alice,user2)</span><br><span class="line">Alice_user3_similarity = pearsonrSim(Alice,user3)</span><br><span class="line">Alice_user4_similarity = pearsonrSim(Alice,user4)</span><br><span class="line">Alice_user1_similarity,Alice_user2_similarity,Alice_user3_similarity,Alice_user4_similarity</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出相似度</span></span><br><span class="line">(<span class="number">0.8528028654224415</span>, <span class="number">0.7071067811865475</span>, <span class="number">0.0</span>, -<span class="number">0.7921180343813393</span>)</span><br></pre></td></tr></table></figure>
<p>从这里看出, Alice用户1和用户2,用户3,用户4的相似度是0.85, 0.7, 0, -0.79。 所以如果n=2， 找到与Alice最相近的两个用户是用户1， 和Alice的相似度是0.85， 用户2， 和Alice相似度是0.7。</p>
<ol>
<li><strong>根据相似度用户计算Alice对物品5的最终得分</strong> 用户1对物品5的评分是3， 用户2对物品5的打分是5， 那么根据上面的计算公式， 可以计算出Alice对物品5的最终得分是 </li>
</ol>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{Alice}+\frac{\sum_{k=1}^{2}\left(S_{Alice,user k}\left(R_{userk, 物品5}-\bar{R}_{userk}\right)\right)}{\sum_{k=1}^{2} S_{Alice, userk}}=4+\frac{0.85*(3-2.4)+0.7*(5-3.8)}{0.85+0.7}=4.87</script><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422154709719.png" alt="image-20220422154709719"></p>
<ol>
<li><strong>根据用户评分对用户进行推荐</strong> 这时候， 我们就得到了Alice对物品5的得分是4.87， 根据Alice的打分对物品排个序从大到小：<script type="math/tex">物品1>物品5>物品3=物品4>物品2</script> 这时候，如果要向Alice推荐2款产品的话， 我们就可以推荐物品1和物品5给Alice</li>
</ol>
<p>至此， 基于用户的协同过滤算法原理介绍完毕。</p>
<h3 id="UserCF代码实现"><a href="#UserCF代码实现" class="headerlink" title="UserCF代码实现"></a>UserCF代码实现</h3><p>这里简单的通过编程实现上面的案例，为后面的大作业做一个热身， 梳理一下上面的过程其实就是三步： 计算用户相似性矩阵、得到前n个相似用户、计算最终得分。</p>
<p>所以我们下面的程序也是分为这三步：</p>
<ol>
<li><strong>首先， 先把数据表给建立起来</strong> 这里采用字典的方式， 之所以没有用pandas， 是因为上面举得这个例子其实是个个例， 在真实情况中， 我们知道， 用户对物品的打分情况并不会这么完整， 会存在大量的空值， 所以矩阵会很稀疏， 这时候用DataFrame， 会有大量的NaN。故这里用字典的形式存储。 用两个字典， 第一个字典是物品-用户的评分映射， 键是物品1-5， 用A-E来表示， 每一个值又是一个字典， 表示的是每个用户对该物品的打分。 第二个字典是用户-物品的评分映射， 键是上面的五个用户， 用1-5表示， 值是该用户对每个物品的打分。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    items=&#123;<span class="string">&#x27;A&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">5</span>, <span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">4</span>, <span class="number">4</span>: <span class="number">3</span>, <span class="number">5</span>: <span class="number">1</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;B&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">3</span>, <span class="number">2</span>: <span class="number">1</span>, <span class="number">3</span>: <span class="number">3</span>, <span class="number">4</span>: <span class="number">3</span>, <span class="number">5</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;C&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">2</span>, <span class="number">3</span>: <span class="number">4</span>, <span class="number">4</span>: <span class="number">1</span>, <span class="number">5</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;D&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">3</span>, <span class="number">4</span>: <span class="number">5</span>, <span class="number">5</span>: <span class="number">2</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;E&#x27;</span>: &#123;<span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">5</span>, <span class="number">4</span>: <span class="number">4</span>, <span class="number">5</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    users=&#123;<span class="number">1</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">2</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="number">3</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="number">4</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">5</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> items,users</span><br><span class="line"></span><br><span class="line">items, users = loadData()</span><br><span class="line">item_df = pd.DataFrame(items).T</span><br><span class="line">user_df = pd.DataFrame(users).T</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>计算用户相似性矩阵</strong> 这个是一个共现矩阵, 5*5，行代表每个用户， 列代表每个用户， 值代表用户和用户的相关性，这里的思路是这样， 因为要求用户和用户两两的相关性， 所以需要用双层循环遍历用户-物品评分数据， 当不是同一个用户的时候， 我们要去遍历物品-用户评分数据， 在里面去找这两个用户同时对该物品评过分的数据放入到这两个用户向量中。 因为正常情况下会存在很多的NAN， 即可能用户并没有对某个物品进行评分过， 这样的不能当做用户向量的一部分， 没法计算相似性。 还是看代码吧， 感觉不太好描述：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算用户相似性矩阵&quot;&quot;&quot;</span></span><br><span class="line">similarity_matrix = pd.DataFrame(-<span class="number">1</span> * np.ones((<span class="built_in">len</span>(users), <span class="built_in">len</span>(users))), index=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], columns=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> userx <span class="keyword">in</span> users:</span><br><span class="line">    <span class="keyword">for</span> usery <span class="keyword">in</span> users:</span><br><span class="line">        userxVec=[]</span><br><span class="line">        useryVec=[]</span><br><span class="line">        <span class="keyword">if</span> userx == usery:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            userx_history = users[userx].keys()</span><br><span class="line">            usery_history = users[usery].keys()</span><br><span class="line">            intersection = <span class="built_in">set</span>(userx_history).intersection(usery_history) <span class="comment"># 用户x和用户y行为历史的交集，否则有nan无法计算相似性</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> intersection:</span><br><span class="line">                userxVec.append(users[userx][i])</span><br><span class="line">                useryVec.append(users[usery][i])</span><br><span class="line">            similarity_matrix[userx][usery]=np.corrcoef(np.array(userxVec),np.array(useryVec))[<span class="number">0</span>][<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>得到如下user相似性矩阵：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422161751278.png" alt="image-20220422161751278"></p>
<p>注意相似度矩阵的初始值为-1，因为皮尔逊相关系数的取值为[-1,1]。</p>
<ol>
<li><strong>计算前n个相似的用户</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算前n个相似的用户&quot;&quot;&quot;</span></span><br><span class="line">n = <span class="number">2</span></span><br><span class="line">similar_users = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">    similar_users[user] = similarity_matrix[user].sort_values(ascending=<span class="literal">False</span>)[:n].index.tolist()</span><br><span class="line">    </span><br><span class="line">similar_users</span><br><span class="line">&#123;<span class="number">1</span>: [<span class="number">2</span>, <span class="number">3</span>], <span class="number">2</span>: [<span class="number">1</span>, <span class="number">4</span>], <span class="number">3</span>: [<span class="number">1</span>, <span class="number">2</span>], <span class="number">4</span>: [<span class="number">2</span>, <span class="number">1</span>], <span class="number">5</span>: [<span class="number">3</span>, <span class="number">4</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>经计算，与用户1最相似的2个用户分别是 用户2 和 用户3 。</p>
<ol>
<li><strong>计算最终得分</strong></li>
</ol>
<p>这里就是上面的那个公式：</p>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{Alice}+\frac{\sum_{k=1}^{2}\left(S_{Alice,user k}\left(R_{userk, 物品5}-\bar{R}_{userk}\right)\right)}{\sum_{k=1}^{2} S_{Alice, userk}}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算最后得分,用户1对物品E的预测评分&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有用户平均评分</span></span><br><span class="line">user_mean_rating = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">    user_mean = np.mean([value <span class="keyword">for</span> value <span class="keyword">in</span> users[user].values()])</span><br><span class="line">    user_mean_rating[user] = user_mean</span><br><span class="line"><span class="comment"># 计算预测得分</span></span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> similar_users[<span class="number">1</span>]:</span><br><span class="line">    weighted_scores += similarity_matrix[<span class="number">1</span>][user]</span><br><span class="line">    corr_values_sum += similarity_matrix[<span class="number">1</span>][user] * (users[user][<span class="string">&#x27;E&#x27;</span>] - user_mean_rating[user])</span><br><span class="line">predict = user_mean_rating[<span class="number">1</span>] + corr_values_sum/weighted_scores</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户1对物品E的预测评分为 <span class="subst">&#123;predict:<span class="number">.2</span>f&#125;</span> &#x27;</span>)</span><br><span class="line"></span><br><span class="line">用户<span class="number">1</span>对物品E的预测评分为 <span class="number">4.87</span></span><br></pre></td></tr></table></figure>
<p>计算结果如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422164842510.png" alt="image-20220422164842510" style="zoom:67%;" /></p>
<h3 id="UserCF的缺点"><a href="#UserCF的缺点" class="headerlink" title="UserCF的缺点"></a>UserCF的缺点</h3><p>UserCF算法存在两个重大问题：</p>
<ol>
<li>数据稀疏性。 一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。<strong>这导致UserCF不适用于那些正反馈获取较困难的应用场景</strong>(如酒店预订， 大件商品购买等低频应用)</li>
<li>算法扩展性。 基于用户的协同过滤需要维护用户相似度矩阵以便快速的找出Topn相似用户， 该矩阵的存储开销非常大，存储空间随着用户数量的增加而增加，<strong>不适合用户数据量大的情况使用</strong>。</li>
</ol>
<p>由于UserCF技术上的两点缺陷， 导致很多电商平台并没有采用这种算法， 而是采用了ItemCF算法实现最初的推荐系统。</p>
<h2 id="基于物品的协同过滤"><a href="#基于物品的协同过滤" class="headerlink" title="基于物品的协同过滤"></a>基于物品的协同过滤</h2><h3 id="ItemCF原理介绍"><a href="#ItemCF原理介绍" class="headerlink" title="ItemCF原理介绍"></a>ItemCF原理介绍</h3><p>基于物品的协同过滤(ItemCF)的基本思想是预先根据所有用户的历史偏好数据计算物品之间的相似性，然后把与用户喜欢的物品相类似的物品推荐给用户。比如物品a和c非常相似，因为喜欢a的用户同时也喜欢c，而用户A喜欢a，所以把c推荐给用户A。<strong>ItemCF算法并不利用物品的内容属性计算物品之间的相似度， 主要通过分析用户的行为记录计算物品之间的相似度， 该算法认为， 物品a和物品c具有很大的相似度是因为喜欢物品a的用户大都喜欢物品c</strong>。</p>
<p><strong>和UserCF类似，ItemCF算法主要包括两个步骤：</strong></p>
<ul>
<li>计算物品之间的相似度</li>
<li>根据物品的相似度和用户的历史行为给用户生成推荐列表（购买了该商品的用户也经常购买的其他商品）</li>
</ul>
<p>这里直接还是拿上面Alice的那个例子来看。</p>
<p><img src="https://camo.githubusercontent.com/38b1abf510c90bc3a8850a09ba19e39ea11b1f83b916f0432cc29fdaed665e5c/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f254535253942254245254537253839253837696d6167652d32303231303632393233323632323735382e706e67" alt="image-20210629232622758"></p>
<p>如果想知道Alice对物品5打多少分， 基于物品的协同过滤算法会这么做：</p>
<ol>
<li>首先计算一下物品5和物品1， 2， 3， 4之间的相似性(它们也是向量的形式， 每一列的值就是它们的向量表示， 因为ItemCF认为如果物品a和物品c具有很大的相似度，那么是因为喜欢物品a的用户大都喜欢物品c， 所以就可以基于每个用户对该物品的打分或者说喜欢程度来向量化物品)</li>
<li>找出与物品5最相近的n个物品（取n=2）</li>
<li>根据Alice对最相近的n个物品的打分去计算对物品5的打分情况，加入评分偏置的预测公式如下：</li>
</ol>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{物品5}+\frac{\sum_{k=1}^{2}\left(S_{物品5,物品 k}\left(R_{Alice, 物品k}-\bar{R}_{物品k}\right)\right)}{\sum_{k=1}^{2} S_{物品k, 物品5}}</script><p><strong>下面我们就可以具体计算一下，猜测Alice对物品5的打分：</strong></p>
<p>首先是步骤1：计算物品5和其它物品之间的相似度。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422193745961.png" alt="image-20220422193745961"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取item向量</span></span><br><span class="line">item5 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item5&#x27;</span>].values.tolist()</span><br><span class="line">item4 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item4&#x27;</span>].values.tolist()</span><br><span class="line">item3 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item3&#x27;</span>].values.tolist()</span><br><span class="line">item2 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item2&#x27;</span>].values.tolist()</span><br><span class="line">item1 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item1&#x27;</span>].values.tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算item相似度</span></span><br><span class="line">item51_similarity = pearsonrSim(item5,item1)</span><br><span class="line">item52_similarity = pearsonrSim(item5,item2)</span><br><span class="line">item53_similarity = pearsonrSim(item5,item3)</span><br><span class="line">item54_similarity = pearsonrSim(item5,item4)</span><br><span class="line">[x.<span class="built_in">round</span>(<span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> (item51_similarity,item52_similarity,item53_similarity,item54_similarity)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出相似度</span></span><br><span class="line">[<span class="number">0.97</span>, -<span class="number">0.48</span>, -<span class="number">0.43</span>, <span class="number">0.58</span>]</span><br></pre></td></tr></table></figure>
<p>步骤2：对相似度进行排序，选择最靠前的n=2个物品：item1和item4</p>
<p>步骤3：下面根据公式计算Alice对物品5的打分</p>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{物品5}+\frac{\sum_{k=1}^{2}\left(S_{物品5,物品 k}\left(R_{Alice, 物品k}-\bar{R}_{物品k}\right)\right)}{\sum_{k=1}^{2} S_{物品k, 物品5}}=\frac{13}{4}+\frac{0.97*(5-3.2)+0.58*(4-3.4)}{0.97+0.58}=4.6</script><p>这时候依然可以向Alice推荐物品5。</p>
<p>下面也是简单编程实现一下， 和上面的差不多：</p>
<h3 id="ItemCF代码实现"><a href="#ItemCF代码实现" class="headerlink" title="ItemCF代码实现"></a>ItemCF代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算物品的相似矩阵&quot;&quot;&quot;</span></span><br><span class="line">similarity_matrix = pd.DataFrame(-<span class="number">1</span> * np.ones((<span class="built_in">len</span>(items), <span class="built_in">len</span>(items))), index=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;E&#x27;</span>], columns=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;E&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每条物品-用户评分数据</span></span><br><span class="line"><span class="keyword">for</span> itemx <span class="keyword">in</span> items:</span><br><span class="line">    <span class="keyword">for</span> itemy <span class="keyword">in</span> items:</span><br><span class="line">        itemxVec = []</span><br><span class="line">        itemyVec = []</span><br><span class="line">        <span class="keyword">if</span> itemx == itemy:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            itemx_history = <span class="built_in">set</span>(items[itemx].keys())</span><br><span class="line">            itemy_history = <span class="built_in">set</span>(items[itemy].keys())</span><br><span class="line">            intersection = itemx_history.intersection(itemy_history)  <span class="comment"># 求交集，同时对两个物品都打分的用户，才有意义</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> intersection:</span><br><span class="line">                itemxVec.append(items[itemx][i])</span><br><span class="line">                itemyVec.append(items[itemy][i])</span><br><span class="line">            similarity_matrix[itemx][itemy] = pearsonrSim(itemxVec,itemyVec).<span class="built_in">round</span>(<span class="number">2</span>)</span><br><span class="line">            <span class="comment"># similarity_matrix[itemx][itemy] = np.corrcoef(np.array(itemxVec),np.array(itemyVec))[0][1] 两种计算方式等价</span></span><br></pre></td></tr></table></figure>
<p>得到物品相似度矩阵：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422195802322.png" alt="image-20220422195802322"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算前n个相似的用户&quot;&quot;&quot;</span></span><br><span class="line">n = <span class="number">2</span></span><br><span class="line">similar_items = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">    similar_items[item] = similarity_matrix[item].sort_values(ascending=<span class="literal">False</span>)[:n].index.tolist()</span><br><span class="line">    </span><br><span class="line">similar_items[<span class="string">&#x27;E&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 与E最相似的2个物品是A和D</span></span><br><span class="line">[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;D&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算最后得分,用户1对物品E的预测评分&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算物品平均打分情况</span></span><br><span class="line">item_ratings_mean = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> item,rating <span class="keyword">in</span> items.items():</span><br><span class="line">    item_ratings_mean[item] = np.mean([value <span class="keyword">for</span> value <span class="keyword">in</span> rating.values()])</span><br><span class="line"></span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> similar_items[<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    weighted_scores += similarity_matrix[<span class="string">&#x27;E&#x27;</span>][item]</span><br><span class="line">    corr_values_sum += similarity_matrix[<span class="string">&#x27;E&#x27;</span>][item] * (users[<span class="number">1</span>][item] -  item_ratings_mean[item])</span><br><span class="line"></span><br><span class="line">predict = item_ratings_mean[<span class="string">&#x27;E&#x27;</span>] + corr_values_sum/weighted_scores</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户1对物品E的预测得分为 <span class="subst">&#123;predict:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="协同过滤算法的问题分析"><a href="#协同过滤算法的问题分析" class="headerlink" title="协同过滤算法的问题分析"></a>协同过滤算法的问题分析</h3><p>协同过滤算法存在的问题之一就是<strong>泛化能力弱</strong>， 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。 导致的问题是<strong>热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐</strong>。 比如下面这个例子：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422203637025.png" alt="image-20220422203637025"></p>
<p>A, B, C, D是物品， 看右边的物品共现矩阵， 可以发现物品D与A、B、C的相似度比较大， 所以很有可能将D推荐给用过A、B、C的用户。 但是物品D与其他物品相似的原因是因为D是一件热门商品， 系统无法找出A、B、C之间相似性的原因是其特征太稀疏， 缺乏相似性计算的直接数据。 所以这就是协同过滤的天然缺陷：<strong>推荐系统头部效应明显， 处理稀疏向量的能力弱</strong>。</p>
<p>为了解决这个问题， 同时增加模型的泛化能力，2006年，<strong>矩阵分解技术(Matrix Factorization,MF</strong>)被提出， 该方法在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征， 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。</p>
<h2 id="进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？"><a href="#进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？" class="headerlink" title="进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？"></a>进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？</h2><p>矩阵乘法用在求两个向量的内积。当问题场景是仅预测用户是否会对物品评分，即共现矩阵只有 0 和 1 时，两个向量的内积可以用集合的形式表示。例如： <code>u = [1,0,0,1,0], v = [0,0,1,1,0]</code> ，矩阵乘法求得两个向量内积为 1 ，从集合的角度看，内积的计算结果其实也是用户 u 和 v 交互过的物品的交集元素个数。</p>
<p>所以集合角度的余弦相似度计算如下：</p>
<script type="math/tex; mode=display">
sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}</script><p>如果只建立 user 对 item 的索引，形式如： <code>&#123;uid: &#123;item1, item2,...&#125;, uid: &#123;item1, item2,...&#125;, ...&#125;</code>，计算两两用户的交互交集时，比较麻烦。建立倒排表形如：<code>&#123;item_id1: &#123;user_id1, user_id2, ... , user_idn&#125;, item_id2: ...&#125;</code> ，只需要对每个 item 遍历，就可以统计两两用户的交互交集。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立item-&gt;users倒排表</span></span><br><span class="line"><span class="comment"># 倒排表的格式为: &#123;item_id1: &#123;user_id1, user_id2, ... , user_idn&#125;, item_id2: ...&#125; 也就是每个item对应有那些用户有过点击</span></span><br><span class="line"><span class="comment"># 建立倒排表的目的就是为了更方便的统计用户之间共同交互的商品数量</span></span><br><span class="line">item_users = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> uid, items <span class="keyword">in</span> tqdm(tra_users.items()): <span class="comment"># 遍历每一个用户的数据,其中包含了该用户所有交互的item</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items: <span class="comment"># 遍历该用户的所有item, 给这些item对应的用户列表添加对应的uid</span></span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_users:</span><br><span class="line">            item_users[item] = <span class="built_in">set</span>()</span><br><span class="line">            item_users[item].add(uid)</span><br></pre></td></tr></table></figure>
<h2 id="课后思考"><a href="#课后思考" class="headerlink" title="课后思考"></a>课后思考</h2><p>1.<strong>什么时候使用UserCF，什么时候使用ItemCF？为什么？</strong></p>
<blockquote>
<ol>
<li>UserCF 由于是基于用户相似度进行推荐， 所以具备更强的社交特性， 这样的特点非常适于<strong>用户少， 物品多， 时效性较强的场合</strong>， 比如新闻推荐场景， 因为新闻本身兴趣点分散， 相比用户对不同新闻的兴趣偏好， 新闻的及时性，热点性往往更加重要， 所以正好适用于发现热点，跟踪热点的趋势。 另外还具有推荐新信息的能力， 更有可能发现惊喜, 因为看的是人与人的相似性, 推出来的结果可能更有惊喜，可以发现用户潜在但自己尚未察觉的兴趣爱好。</li>
<li>ItemCF 这个更适用于兴趣变化较为稳定的应用， 更接近于个性化的推荐， 适合<strong>物品少，用户多，用户兴趣固定持久， 物品更新速度不是太快的场合</strong>， 比如推荐艺术品， 音乐， 电影。</li>
</ol>
</blockquote>
<p>2.<strong>协同过滤在计算上有什么缺点？有什么比较好的思路可以解决（缓解）？</strong></p>
<blockquote>
<p>第一个问题就是<strong>泛化能力弱</strong>， 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。 导致的问题是<strong>热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐</strong>。</p>
</blockquote>
<p><strong>3.上面介绍的相似度计算方法有什么优劣之处？</strong></p>
<blockquote>
<p>cosine相似度还是比较常用的， 一般效果也不会太差， 但是对于评分数据不规范的时候， 也就是说， 存在有的用户喜欢打高分， 有的用户喜欢打低分情况的时候，有的用户喜欢乱打分的情况， 这时候consine相似度算出来的结果可能就不是那么准确了。所以对于这种用户评分偏置的情况， 余弦相似度就不是那么好了， 可以考虑使用下面的皮尔逊相关系数。</p>
<script type="math/tex; mode=display">
P_{i, j}=\bar{R}_{i}+\frac{\sum_{k=1}^{n}\left(S_{i, k}\left(R_{k, j}-\bar{R}_{k}\right)\right)}{\sum_{k=1}^{n} S_{i, k}}</script></blockquote>
<p>4.<strong>协同过滤还存在其他什么缺陷？有什么比较好的思路可以解决（缓解）？</strong></p>
<blockquote>
<p>协同过滤的特点就是完全没有利用到物品本身或者是用户自身的属性， 仅仅利用了用户与物品的交互信息就可以实现推荐，比较简单高效， 但这也是它的一个短板所在， 由于无法有效的引入用户年龄， 性别，商品描述，商品分类，当前时间，地点等一系列用户特征、物品特征和上下文特征， 这就造成了有效信息的遗漏，不能充分利用其它特征数据。</p>
<p>为了解决这个问题， 在推荐模型中引用更多的特征，<strong>推荐系统慢慢的从以协同过滤为核心到了以逻辑回归模型为核心</strong>， 提出了能够综合不同类型特征的机器学习模型。</p>
<p>演化图左边的时间线梳理完毕：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220424123935703.png" alt="《深度学习推荐系统》- 王喆"></p>
</blockquote>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>使用皮尔逊相似度计算的预测结果非常差，可能因为计算皮尔逊相似度时出现大量nan（不知道为什么…），然后我的做法是用0填充，pearsonr = 0 代表无关。</p>
<p>皮尔逊相关系数计算公式：</p>
<script type="math/tex; mode=display">
P_{i, j}=\bar{R}_{i}+\frac{\sum_{k=1}^{n}\left(S_{i, k}\left(R_{k, j}-\bar{R}_{k}\right)\right)}{\sum_{k=1}^{n} S_{i, k}}</script><p>预测结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Pearsonr_ItemCF(K=<span class="number">10</span>,N=<span class="number">10</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">recall: 0.18</span></span><br><span class="line"><span class="string">precision 0.61</span></span><br><span class="line"><span class="string">coverage 35.17</span></span><br><span class="line"><span class="string">Popularity 5.539</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">Cosine_Item_CF(K=<span class="number">10</span>,N=<span class="number">10</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">recall: 9.2</span></span><br><span class="line"><span class="string">precision 30.48</span></span><br><span class="line"><span class="string">coverage 19.18</span></span><br><span class="line"><span class="string">Popularity 7.171</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础5：矩阵分解SVD</title>
    <url>/2022/04/25/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%805/</url>
    <content><![CDATA[<hr>
<h1 id="任务5：矩阵分解SVD"><a href="#任务5：矩阵分解SVD" class="headerlink" title="任务5：矩阵分解SVD"></a>任务5：矩阵分解SVD</h1><ul>
<li>阅读<a href="https://github.com/datawhalechina/fun-rec/blob/master/docs/%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/1.1%20%E5%9F%BA%E7%A1%80%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/1.1.3%20%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3.md">矩阵分解基础教程</a>，<a href="https://alyssaq.github.io/2015/20150426-simple-movie-recommender-using-svd/">代码实现</a></li>
<li>编写SVD用于电影推荐的流程</li>
<li>比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="1-隐语义模型与矩阵分解"><a href="#1-隐语义模型与矩阵分解" class="headerlink" title="1. 隐语义模型与矩阵分解"></a>1. 隐语义模型与矩阵分解</h2><p>隐语义模型（Latent factor model，简称LFM），是基于矩阵分解（Matrix Factorization，简称MF）的推荐算法。</p>
<p>协同过滤算法的特点就是完全没有利用到物品本身或者是用户自身的属性， 仅仅利用了用户与物品的交互信息就可以实现推荐，是一个可解释性很强， 非常直观的模型， 但是也存在一些问题， 第一个就是处理稀疏矩阵的能力比较弱， 所以<strong>为了使得协同过滤更好处理稀疏矩阵问题， 增强泛化能力</strong>， 从协同过滤中衍生出矩阵分解模型或者叫隐语义模型，两者差不多说的一个意思， 就是在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征， 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。下图直观展现了协同过滤算法和隐语义模型（矩阵分解算法）的区别。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220425130039276.png" alt="image-20220425130039276"></p>
<h2 id="2-隐语义模型"><a href="#2-隐语义模型" class="headerlink" title="2. 隐语义模型"></a>2. 隐语义模型</h2><p>隐语义模型最早在文本领域被提出，用于找到文本的隐含语义。在2006年， 被用于推荐中， <strong>它的核心思想是通过隐含特征（latent factor）联系用户和物品（item）， 基于用户的行为找出潜在的主题和分类</strong>。</p>
<p>我们下面拿一个音乐评分的例子来具体看一下隐含特征的含义。</p>
<p>假设每个用户都有自己的听歌偏好， 比如A喜欢带有<strong>小清新的</strong>， <strong>吉他伴奏的</strong>， <strong>王菲</strong>的歌曲，如果一首歌正好<strong>是王菲唱的， 并且是吉他伴奏的小清新</strong>， 那么就可以将这首歌推荐给这个用户。 也就是说是<strong>小清新， 吉他伴奏， 王菲</strong>这些元素连接起了用户和歌曲。 当然每个用户对不同的元素偏好不同， 每首歌包含的元素也不一样， 所以我们就希望找到下面的两个矩阵：</p>
<ol>
<li><strong>潜在因子—— 用户矩阵Q</strong> 这个矩阵表示不同用户对于不同元素的偏好程度， 1代表很喜欢， 0代表不喜欢， 比如下面这样：</li>
</ol>
<p><img src="https://camo.githubusercontent.com/bc712ed4c34be52a07689afee52c5024248c7cc330484e6d305de2ac799dbe3c/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f323032303038323232323032353936382e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<ol>
<li><strong>潜在因子——音乐矩阵P</strong> 表示每种音乐含有各种元素的成分， 比如下表中， 音乐A是一个偏小清新的音乐， 含有小清新的Latent Factor的成分是0.9， 重口味的成分是0.1， 优雅成分0.2…..</li>
</ol>
<p><img src="https://camo.githubusercontent.com/89d841394de2508f511e1a05b87499ed3987b2abdb7a5bf8933ec5a36cf089b5/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832323232303735313339342e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<p><strong>利用上面的这两个矩阵， 我们就能得出张三对音乐A的喜欢程度：</strong></p>
<blockquote>
<p>张三对<strong>小清新</strong>的偏好 <em> 音乐A含有<strong>小清新</strong>的成分 + 张三对<strong>重口味</strong>的偏好 </em> 音乐A含有<strong>重口味</strong>的成分 + 张三对<strong>优雅</strong>的偏好 <em> 音乐A含有<em>*优雅</em></em>的成分….,</p>
</blockquote>
<p>下面是对应的两个隐向量：</p>
<p><img src="https://camo.githubusercontent.com/950a5f6a6bbcbb19c476595e18bbab5a441b09ae2f5d3ac19e77063ceef97fec/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832323232313632373231392e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<p>根据隐向量其实就可以得到张三对音乐A的打分，即： <script type="math/tex">0.6 * 0.9 + 0.8 * 0.1 + 0.1 * 0.2 + 0.1 * 0.4 + 0.7 * 0 = 0.69</script> 按照这个计算方式， 每个用户对每首歌其实都可以得到这样的分数， 最后就得到了我们的评分矩阵：</p>
<p><img src="https://camo.githubusercontent.com/16fde22e8adaf3cf91458b50417ce0fa22239d474263e0b7106b7c45d437083e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832323232323134313233312e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<p>这里的红色表示用户没有打分，我们通过隐向量计算得到的。</p>
<p>上面例子中的<strong>小清晰， 重口味， 优雅，伤感，五月天</strong>这些就可以看做是隐含特征， 而通过这个隐含特征就可以把用户和音乐联系起来。其实就是找到了每个用户、每个音乐的一个 5 维的隐向量表达形式（类似embedding）。</p>
<p>用户隐向量的每个维度代表该用户对这个隐含特征的相关性（兴趣），音乐隐向量的每个维度代表该音乐与这个隐含特征的相关性。</p>
<p>于是隐向量就可以反映出用户的兴趣和物品的风格，并能将相似的物品推荐给相似的用户等。 <strong>有没有感觉到是把协同过滤算法进行了一种延伸， 把用户的相似性和物品的相似性通过了一个叫做隐向量的方式进行表达</strong>。</p>
<p>但是， 真实的情况下我们其实是没有上面那两个矩阵的， 音乐那么多， 用户那么多， 我们没有办法去找一些隐特征去表示出这些东西， 另外一个问题就是即使能表示也不一定准， 对于每个用户或者每个物品的风格，我们每个人都有不同的看法。 所以事实上， 我们有的只有用户的评分矩阵， 也叫“共现矩阵”， 一般这种矩阵长这样：</p>
<p><img src="https://camo.githubusercontent.com/eda2821a1bbb617ec7f25e57e6664891cf538106d0ec85f64e8321de32458ecc/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832323232333331333334392e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="user-item评分矩阵"></p>
<p>上面说到过，如果直接用协同过滤算法去填充这个矩阵，很容易头部效应明显、泛化能力弱、长尾效应明显的问题。</p>
<p>如何才能将这个矩阵转化成我们想要的隐语义模型呢？矩阵分解算法实现了这一点。</p>
<h2 id="3-矩阵分解算法"><a href="#3-矩阵分解算法" class="headerlink" title="3. 矩阵分解算法"></a>3. 矩阵分解算法</h2><p>矩阵分解算法其实就是在<strong>想办法基于这个评分矩阵去找到上面例子中的那两个矩阵， 也就是用户兴趣和物品的隐向量表达。 具体做法是：将评分矩阵分解成Q和P两个矩阵乘积的形式， 这时候就可以基于这两个矩阵去预测某个用户对某个物品的评分了。 然后基于这个评分去进行推荐</strong>。这就是矩阵分解算法的原理。</p>
<h3 id="矩阵分解算法原理"><a href="#矩阵分解算法原理" class="headerlink" title="矩阵分解算法原理"></a>矩阵分解算法原理</h3><p>在矩阵分解的算法框架下， <strong>我们可以通过分解协同过滤的共现矩阵来得到用户和物品的隐向量</strong>， 就是上面的用户矩阵Q和物品矩阵P， 这也是“矩阵分解”名字的由来。</p>
<p><img src="https://camo.githubusercontent.com/e09db5bdf02c408d2231e52318d348d4aec26e864d22ab1a158136ee27de9da3/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832333130313531333233332e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<p>矩阵分解算法将 $m\times n$ 维的共现矩阵 $R$ 分解成 $m \times k$ 维的用户矩阵 $U$ 和 $k \times n$ 维的物品矩阵 $V$ 相乘的形式。 其中 $m$ 是用户数量，  $n$ 是物品数量， $k$ 是隐向量维度， 也就是隐含特征个数， 只不过这里的隐含特征变得<strong>不可解释</strong>了， <strong>即我们不知道具体含义了</strong>， 要模型自己去学。$k$ 的大小决定了隐向量表达能力的强弱， $k$ 越大， 表达信息的能力就越强， 理解起来就是把隐含特征分类划分的越具体。</p>
<p>那么如果有了用户矩阵和物品矩阵的话， 我们就能计算用户 $u$ 对物品 $i$ 的评分， 只需要 </p>
<script type="math/tex; mode=display">
\operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{f=1}^{F} p_{u, k} q_{k,i}</script><p>（熟悉推荐的同学应该对这个表达式非常熟悉，几乎所有预测得分都是这个计算得来的。）这里的 ${p_u}$ 就是用户 $u$ 的隐向量， 就类似与上面的张三向量， 注意这是列向量， $q_i$ 是物品 $i$ 的隐向量， 就类似于上面的音乐A向量， 这个也是列向量， 所以才用了 $p_{u}^{T} q_{i}$ 得到了一个标量， 也就是用户的最终评分， 计算过程其实和上面例子中一样。 这里的 $p_{u,k}$ 和 $q_{i,k}$ 是模型的参数， 也正是我们想办法要计算的， $p_{u,k}$度量的是用户 $u$ 对第 $k$ 个隐含特征的兴趣， 而 $q_{i,k}$ 度量了物品 $i$ 与第 $k$ 个隐含特征联系。</p>
<h3 id="矩阵分解算法的求解"><a href="#矩阵分解算法的求解" class="headerlink" title="矩阵分解算法的求解"></a>矩阵分解算法的求解</h3><p>对矩阵进行矩阵分解的主要方法有三种：特征值分解（Eigen Decomposition）、奇异值分解（Singular Value Decomposition，SVD）和梯度下降（Gradient Descent）。</p>
<p>其中，特征值分解只能作用于方阵，显然不适用于分解用户-物品矩阵。</p>
<p>传统的SVD分解的具体描述如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220425134607188.png" alt="image-20220425134607188"></p>
<p>可以说，奇异值分解似乎完美地解决了矩阵分解的问题，但其存在两点缺陷，使其不宜作为互联网场景下矩阵分解的主要方法。（1）SVD分解；（2）SVD分解的计算复杂度达到了 $O(mn^2)$ 的级别，这对于商品数量动辄上百万、用户数量往往上千万的互联网场景来说几乎是不可接受的。原理详解参考文章：<a href="https://blog.csdn.net/wuzhongqiang/article/details/108168238">奇异值分解(SVD)的原理详解及推导</a></p>
<p>由于上述两个原因，传统奇异值分解也不适用于解决大规模稀疏矩阵的矩阵分解问题。因此，梯度下降法成了进行矩阵分解的主要方法，这里对其进行具体的介绍。</p>
<h4 id="梯度下降法：Funk-SVD"><a href="#梯度下降法：Funk-SVD" class="headerlink" title="梯度下降法：Funk-SVD"></a>梯度下降法：Funk-SVD</h4><p>2006年的Netflix Prize之后， Simon Funk公布了一个矩阵分解算法叫做<strong>Funk-SVD</strong>, 后来被Netflix Prize的冠军Koren称为<strong>Latent Factor Model(LFM)</strong>。 Funk-SVD的思想很简单： <strong>把求解上面两个矩阵的参数问题转换成一个最优化问题， 可以通过训练集里面的观察值利用最小化来学习用户矩阵和物品矩阵</strong>。</p>
<p>我们上面已经知道， 如果有了用户矩阵和物品矩阵的话， 就能计算用户 $u$ 对物品 $i$ 的评分： <script type="math/tex">\operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}</script> </p>
<p>而现在， 我们有真实的 $r_{u,i}$ , 但是没有 $p_{u}^{T} q_{i}$ ，那么我们可以初始化一个啊， 随机初始化一个用户矩阵 $U$ 和一个物品矩阵 $V$ ， 然后不就有 $p_{u}^{T} q_{i}$ 了？ 当然你说， 随机初始化的肯定不准啊， 但是， 有了 $p_{u}^{T} q_{i}$ 之后， 我们就可以计算一个预测值 $\hat{r}_{u i}$ , 即 <script type="math/tex">\hat{r}_{u i}=p_{u}^{T} q_{i}</script></p>
<p>一开始预测肯定是不准的， 那么这个预测的和真实值之间就会有一个误差： <script type="math/tex">e_{u i}=r_{u i}-\hat{r}_{u i}</script> </p>
<p>于是可以得到一个优化目标，即总的误差平方和： <script type="math/tex">\operatorname{SSE}=\sum_{u, i} e_{u i}^{2}=\sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k}· q_{k, i}\right)^{2}</script> </p>
<p>我们想办法进行训练，把 SSE 降到最小，那么我们的两个矩阵参数就可以算出来。</p>
<p>所以就把这个问题转成了最优化的的问题， 可以利用非常标准的梯度下降过程完成。</p>
<p>（1）确定目标函数。我们的目标函数就是：</p>
<script type="math/tex; mode=display">
\min _{\boldsymbol{q}^*, \boldsymbol{p}^*} \sum_{(u, i) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}\right)^{2}</script><p>这里的 $K$ 表示所有用户评分样本的集合。为了减少过拟合现象，加入正则化项后，目标函数如下：</p>
<script type="math/tex; mode=display">
\min _{\boldsymbol{q}^*, \boldsymbol{p}^*} \sum_{(u, i) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}\right)^{2} + \lambda ·(||q_i||^2+||p_u||^2)</script><p>（2）对目标函数求偏导，求取梯度下降的方向和幅度</p>
<p>先对 $p_u$ 和 $q_i$ 求偏导</p>
<script type="math/tex; mode=display">
\operatorname{Loss} =\sum_{(u, i) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}\right)^{2} + \lambda ·(||q_i||^2+||p_u||^2)\\
{\frac{\partial Loss}{\partial p_u} } = -2\ *\ (\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}) q_i\ +\ 2\lambda p_u \\
{\frac{\partial Loss}{\partial q_i} } = -2\ *\ (\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}) p_u \ +\ 2\lambda q_i \\</script><p>（3）利用第 2 步的求导结果，沿梯度的反方向更新参数：</p>
<script type="math/tex; mode=display">
q_i = q_i + \gamma ((\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}) q_i\ -\ \lambda p_u )\\
p_u = p_u + \gamma ((\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}) p_u \ -\ \lambda q_i)</script><p>其中 $\gamma$ 是学习率，原来的系数 2 可以省略，因为仅仅是放缩学习率。 </p>
<p>（4）当迭代次数超过上限次数 $n$ ，或者损失低于阈值 $\theta$ 时，结束训练，否则循环第三步。</p>
<p>在完成矩阵分解过程后，即可得到所有用户和物品的隐向量。在对某用户进行推荐时，可利用该用户的隐向量与所有物品的隐向量进行逐一的内积运算，得出该用户对所有物品的评分预测，再依次进行排序，得到最终的推荐列表。</p>
<p>在了解了矩阵分解的原理之后，就可以更清楚地解释为什么矩阵分解相较协同过滤有更强的泛化能力。在矩阵分解算法中，由于隐向量的存在，使任意的用户和物品之间都可以得到预测分值。而隐向量的生成过程其实是<strong>对共现矩阵进行全局拟合</strong>的过程，因此隐向量其实是利用全局信息生成的，有<strong>更强的泛化能力</strong>；而对协同过滤来说，如果两个用户没有相同的历史行为，两个物品没有相同的人购买，那么这两个用户和两个物品的相似度都将为 0（因为协同过滤只能利用用户和物品自己的信息进行相似度计算，这就使协同过滤不具备泛化利用全局信息的能力）。</p>
<h5 id="消除打分偏差"><a href="#消除打分偏差" class="headerlink" title="消除打分偏差"></a>消除打分偏差</h5><p>但在实际中， 单纯的 $\hat{r}_{u i}=p_{u}^{T} q_{i}$ 也是不够的， 还要考虑其他的一些因素， 比如一个评分系统， 有些固有的属性和用户物品无关， 而用户也有些属性和物品无关， 物品也有些属性和用户无关。 因此， Netfix Prize中提出了另一种LFM， 在原来的基础上加了偏置项， 来消除用户和物品打分的偏差， 即预测公式如下： <script type="math/tex">\hat{r}_{u i}=\mu+b_{u}+b_{i}+p_{u}^{T} \cdot q_{i}</script> 这个预测公式加入了3项偏置 $\mu,b_u,b_i$，作用如下：</p>
<ul>
<li>$\mu$：训练集中所有记录的评分的全局平均数。 在不同网站中， 因为网站定位和销售物品不同， 网站的整体评分分布也会显示差异。 比如有的网站中用户就喜欢打高分， 有的网站中用户就喜欢打低分。 而全局平均数可以表示网站本身对用户评分的影响。</li>
<li>$b_u$：用户偏差系数， 可以使用用户 $u$ 给出的所有评分的均值， 也可以当做训练参数。 这一项表示了用户的评分习惯中和物品没有关系的那种因素。 比如有些用户比较苛刻， 对什么东西要求很高， 那么他评分就会偏低， 而有些用户比较宽容， 对什么东西都觉得不错， 那么评分就偏高。</li>
<li>$b_i$：物品偏差系数， 可以使用物品 $i$ 收到的所有评分的均值， 也可以当做训练参数。 这一项表示了物品接受的评分中和用户没有关系的因素。 比如有些物品本身质量就很高， 因此获得的评分相对比较高， 有的物品本身质量很差， 因此获得的评分相对较低。</li>
</ul>
<p>加了用户和物品的打分偏差之后， 矩阵分解得到的隐向量更能反映不同用户对不同物品的“真实”态度差异， 也就更容易捕捉评价数据中有价值的信息， 从而避免推荐结果有偏。 注意此时的目标函数会发生变化： </p>
<script type="math/tex; mode=display">
\min _{\boldsymbol{q}^{*}, \boldsymbol{p}^{*}, \boldsymbol{b}^{*}} \sum_{(\mathrm{u}, \mathrm{i}) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-\mu-b_{\mathrm{u}}-b_{\mathrm{i}}-\boldsymbol{p}_{\mathrm{u}}^{\mathrm{T}} \boldsymbol{q}_{\mathrm{i}}\right)^{2}+\lambda\left(\left\|\boldsymbol{p}_{\mathrm{u}}\right\|^{2}+\left\|\boldsymbol{q}_{\mathrm{i}}\right\|^{2}+b_{\mathrm{u}}^{2}+b_{\mathrm{i}}^{2}\right)</script><p>此时如果把 $b_u$ 和 $b_i$ 当做训练参数的话， 那么它俩的梯度是：</p>
<script type="math/tex; mode=display">
\frac{\partial Loss}{\partial b_{u}}=-e_{u i}+\lambda b_{u} \\ \frac{\partial Loss}{\partial b_{i}}=-e_{u i}+\lambda b_{i}</script><p>更新公式为：</p>
<script type="math/tex; mode=display">
b_u = b_u + \gamma (e_{ui}-\ \lambda p_u )\\
p_u = p_u + \gamma (e_{ui}-\ \lambda q_i)</script><p>而对于 $p_{u}$ 和$q_{i}$， 导数没有变化， 更新公式也没有变化。</p>
<h3 id="矩阵分解算法的代码实现"><a href="#矩阵分解算法的代码实现" class="headerlink" title="矩阵分解算法的代码实现"></a>矩阵分解算法的代码实现</h3><p>我们这里用代码实现一下上面的算法来预测上一篇文章里面的那个预测 Alice 对物品 5 的评分， 看看矩阵分解到底是怎么进行预测或者是推荐的。 我把之前的例子拿过来：</p>
<p><img src="https://camo.githubusercontent.com/68d8995d1a9bacf4e58fa39359de71cbb99e3bf5abc5174bd1033e8b93fdae81/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832373135303233373932312e706e67237069635f63656e746572" alt="在这里插入图片描述"></p>
<p>任务就是根据这个评分矩阵， 猜测Alice对物品5的打分。</p>
<p>在实现 MF 之前， 先来回忆一下 ItemCF 和 UserCF 对于这个问题的做法：</p>
<p>首先 ItemCF 的做法， 根据已有的用户打分计算物品之间的相似度， 得到物品的相似度矩阵， 根据这个相似度矩阵， 选择出前 K 个与物品 5 最相似的物品， 然后基于 Alice 对这 K 个物品的得分， 猜测 Alice 对物品 5 的得分， 有一个加权的计算公式。 </p>
<p>UserCF的做法，根据用户对其他物品的打分， 计算用户之间的相似度， 选择出与Alice最相近的K个用户， 然后基于那K个用户对物品5的打分计算出Alice对物品5的打分。 </p>
<p>但是，这两种方式有个问题， 就是如果矩阵非常稀疏的话， 当然这个例子是个特例， 一般矩阵都是非常稀疏的， 那么预测效果就不好， 因为两个相似用户对同一物品打分的概率以及 Alice 同时对两个相似物品打分的概率可能都比较小。 另外， 这两种方法显然没有考虑到全局的物品或者用户， 只是基于了最相似的例子， 很可能有偏。</p>
<p>那么 MF 在解决这个问题上是这么做的：</p>
<ol>
<li>首先， 它会先初始化用户矩阵 P 和物品矩阵 Q ，  P 的维度是<code>[users_num, K]</code>, Q的维度是<code>[item_nums, K]</code>， 这个 K 是隐向量的维度。 也就是把通过隐向量的方式把用户的兴趣和物品的特点关联了起来。 初始化这两个矩阵的方式很多， 但根据经验， 随机数需要和 $\frac{1}{\sqrt{K}}$ 成正比。 下面代码中会发现。</li>
<li>有了两个矩阵之后， 我就可以根据用户已经打分的数据去更新参数， 这就是训练模型的过程， 方法很简单， 就是遍历用户， 对于每个用户， 遍历它打分的物品， 这样就拿到了该用户和物品的隐向量， 然后两者相乘加上偏置就是预测的评分， 这时候与真实评分有个差距， 根据上面的梯度下降就可以进行参数的更新</li>
</ol>
<p>这样训练完之后， 我们就可以得到用户 Alice 和物品 5 的隐向量， 根据这个就可以预测 Alice 对物品 5 的打分。下面的代码的逻辑就是上面这两步， 这里使用带有偏置项和正则项的 MF 算法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVD</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, rating_data, F=<span class="number">5</span>, alpha=<span class="number">0.1</span>, lmbda=<span class="number">0.1</span>, max_iter=<span class="number">100</span></span>):</span></span><br><span class="line">        self.F = F           <span class="comment"># 这个表示隐向量的维度</span></span><br><span class="line">        self.P = <span class="built_in">dict</span>()          <span class="comment">#  用户矩阵P  大小是[users_num, F]</span></span><br><span class="line">        self.Q = <span class="built_in">dict</span>()     <span class="comment"># 物品矩阵Q  大小是[item_nums, F]</span></span><br><span class="line">        self.bu = <span class="built_in">dict</span>()   <span class="comment"># 用户偏差系数</span></span><br><span class="line">        self.bi = <span class="built_in">dict</span>()    <span class="comment"># 物品偏差系数</span></span><br><span class="line">        self.mu = <span class="number">1.0</span>        <span class="comment"># 全局偏差系数</span></span><br><span class="line">        self.alpha = alpha   <span class="comment"># 学习率</span></span><br><span class="line">        self.lmbda = lmbda    <span class="comment"># 正则项系数</span></span><br><span class="line">        self.max_iter = max_iter    <span class="comment"># 最大迭代次数</span></span><br><span class="line">        self.rating_data = rating_data <span class="comment"># 评分矩阵</span></span><br></pre></td></tr></table></figure>
<p>​        </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="comment"># 初始化矩阵P和Q, 方法很多， 一般用随机数填充， 但随机数大小有讲究， 根据经验， 随机数需要和1/sqrt(F)成正比</span></span><br><span class="line">    cnt = <span class="number">0</span>    <span class="comment"># 统计总的打分数， 初始化mu用</span></span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> self.rating_data.items():</span><br><span class="line">        self.P[user] = [random.random() / math.sqrt(self.F)  <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, F)]</span><br><span class="line">        self.bu[user] = <span class="number">0</span></span><br><span class="line">        cnt += <span class="built_in">len</span>(items) </span><br><span class="line">        <span class="keyword">for</span> item, rating <span class="keyword">in</span> items.items():</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> self.Q:</span><br><span class="line">                self.Q[item] = [random.random() / math.sqrt(self.F) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, F)]</span><br><span class="line">                self.bi[item] = <span class="number">0</span></span><br><span class="line">    self.mu /= cnt</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 有了矩阵之后， 就可以进行训练, 这里使用随机梯度下降的方式训练参数P和Q</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">        <span class="keyword">for</span> user, items <span class="keyword">in</span> self.rating_data.items():</span><br><span class="line">            <span class="keyword">for</span> item, rui <span class="keyword">in</span> items.items():</span><br><span class="line">                rhat_ui = self.predict(user, item)   <span class="comment"># 得到预测评分</span></span><br><span class="line">                <span class="comment"># 计算误差</span></span><br><span class="line">                e_ui = rui - rhat_ui</span><br><span class="line">                </span><br><span class="line">                self.bu[user] += self.alpha * (e_ui - self.lmbda * self.bu[user])</span><br><span class="line">                self.bi[item] += self.alpha * (e_ui - self.lmbda * self.bi[item])</span><br><span class="line">                <span class="comment"># 随机梯度下降更新梯度</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.F):</span><br><span class="line">                    self.P[user][k] += self.alpha * (e_ui*self.Q[item][k] - self.lmbda * self.P[user][k])</span><br><span class="line">                    self.Q[item][k] += self.alpha * (e_ui*self.P[user][k] - self.lmbda * self.Q[item][k])</span><br><span class="line">                </span><br><span class="line">        self.alpha *= <span class="number">0.1</span>    <span class="comment"># 每次迭代步长要逐步缩小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测user对item的评分， 这里没有使用向量的形式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, user, item</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(self.P[user][f] * self.Q[item][f] <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.F)) + self.bu[user] + self.bi[item] + self.mu   </span><br></pre></td></tr></table></figure>
<p>下面我建立一个字典来存放数据， 之所以用字典， 是因为很多时候矩阵非常的稀疏， 如果用pandas的话， 会出现很多Nan的值， 反而不好处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    rating_data=&#123;<span class="number">1</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">2</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="number">3</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="number">4</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">5</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> rating_data</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 接下来就是训练和预测</span></span><br><span class="line">rating_data = loadData()</span><br><span class="line">basicsvd = SVD(rating_data, F=<span class="number">10</span>)</span><br><span class="line">basicsvd.train()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> [<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    <span class="built_in">print</span>(item, basicsvd.predict(<span class="number">1</span>, item))</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">E <span class="number">3.252210242858994</span></span><br></pre></td></tr></table></figure>
<p>通过这个方式， 得到的预测评分是3.25， 这个和隐向量的维度， 训练次数和训练方式有关， 这里只说一下这个东西应该怎么用， 具体结果可以不用纠结。</p>
<p><a href="https://github.com/Guadzilla/recpre/tree/master/task5">pytorch的代码实现</a>已给出。</p>
<h2 id="比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？"><a href="#比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？" class="headerlink" title="比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？"></a>比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？</h2><p>$RMSE=\sqrt{\frac{1}{m}\sum^m_{i=1}(f(x_i)-y_i)^2}$</p>
<p>用基于皮尔逊相似度的 ItemCF 协同过滤算法，推荐的 TopN 物品的评分与对应真实评分计算 RMSE 。</p>
<p>SVD 算法可以计算所有用户对所有物品的评分，所以计算所有验证集的预测得分与对应真实评分计算 RMSE。</p>
<h1 id="课后思考"><a href="#课后思考" class="headerlink" title="课后思考"></a>课后思考</h1><ol>
<li><p>矩阵分解算法后续有哪些改进呢？针对这些改进，是为了解决什么的问题呢？请大家自行探索</p>
<p>SVD，计算复杂度太高。Funk-SVD，用梯度下降法，大大减少复杂度。BiasSVD，消除用户和物品打分偏差。SVD++，考虑邻域影响。TSVD，加入时间信息。</p>
</li>
<li><p>矩阵分解的优缺点分析</p>
<ul>
<li>优点：<ul>
<li>泛化能力强： 一定程度上解决了稀疏问题</li>
<li>空间复杂度低： 由于用户和物品都用隐向量的形式存放， 少了用户和物品相似度矩阵， 空间复杂度由$n^2$降到了$(n+m)*f$</li>
<li>更好的扩展性和灵活性：矩阵分解的最终产物是用户和物品隐向量， 这个深度学习的embedding思想不谋而合， 因此矩阵分解的结果非常便于与其他特征进行组合和拼接， 并可以与深度学习无缝结合。</li>
</ul>
</li>
</ul>
<p>但是， 矩阵分解算法依然是只用到了评分矩阵， 没有考虑到用户特征， 物品特征和上下文特征， 这使得矩阵分解丧失了利用很多有效信息的机会， 同时在缺乏用户历史行为的时候， 无法进行有效的推荐。 所以为了解决这个问题， <strong>逻辑回归模型及后续的因子分解机模型</strong>， 凭借其天然的融合不同特征的能力， 逐渐在推荐系统领域得到了更广泛的应用。</p>
</li>
</ol>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li>王喆 - 《深度学习推荐系统》</li>
<li>项亮 - 《推荐系统实战》</li>
<li><a href="https://blog.csdn.net/wuzhongqiang/article/details/108168238">奇异值分解(SVD)的原理详解及推导</a></li>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5197422&amp;tag=1">Matrix factorization techniques for recommender systems论文</a></li>
<li><a href="https://blog.csdn.net/wuzhongqiang/article/details/108173885">隐语义模型(LFM)和矩阵分解(MF)</a></li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础6：Slope One</title>
    <url>/2022/04/26/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%806/</url>
    <content><![CDATA[<hr>
<h1 id="任务6：Slope-One"><a href="#任务6：Slope-One" class="headerlink" title="任务6：Slope One"></a>任务6：Slope One</h1><ul>
<li>阅读<a href="https://blog.csdn.net/xidianliutingting/article/details/51916578">Slope One基础原理</a></li>
<li>编写Slope One用于电影推荐的流程</li>
<li>比较Slope One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="slope-one-算法"><a href="#slope-one-算法" class="headerlink" title="slope one 算法"></a>slope one 算法</h2><p>1.示例引入</p>
<p>我们可以这么认为，商品间受欢迎的差异从某种程度上是固定的，比如所有人都喜欢海底捞火锅，但对赛百味的喜爱程度一般。此时小明对海底捞火锅的评分为4，对赛百味的评分为2；而小吴对海底捞火锅的评分为5，对赛百味的评分为3。尽管两个人评分的习惯上不同，小明平均打的分都高，但是对两个物品来说，他们之间的评分差值是不变的，即 $5-3=4-2$ 。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220426192311847.png" alt="image-20220426192311847"></p>
<p>Slope one 的思路大抵如此，现在假设我们想预测 Alice 对物品 2 的评分，已有的是别的用户对物品 1 和对物品 2 的评分和 Alice 对物品 1 的评分。</p>
<p>从物品评分偏差的角度，我们可以求出物品 1 和物品 2 之间的评分偏差，即 $R_{1,2}=\frac{(3-1)+(4-3)+(3-3)+(1-5)}{4}=-0.25$，再用 Alice 对物品 1 的评分减去这个偏差，即 $p_{Alice,2}=r_{Alice,1}-R_{1,2}=5-(-0.25)=5.25$，把它作为 Alice 对物品 2 的预测评分。这就是Slope one 算法最简单的场景。</p>
<h2 id="slope-one-算法思想"><a href="#slope-one-算法思想" class="headerlink" title="slope one 算法思想"></a>slope one 算法思想</h2><p>Slope One 算法是由 Daniel Lemire 教授在 2005 年提出的一个 <strong>Item-Based</strong> 的协同过滤推荐算法。和其它类似算法相比, 它的最大优点在于算法很简单, 易于实现, 执行效率高, 同时推荐的准确性相对较高。<br>Slope One算法是基于不同物品之间的评分差的线性算法，预测用户对物品评分的个性化算法。主要分为三步：</p>
<p>Step1: 计算物品之间的评分差的均值，记为物品间的评分偏差(两物品同时被评分)；</p>
<p><img src="https://img-blog.csdn.net/20160715114006473" alt="这里写图片描述"></p>
<p>Step2:根据物品间的评分偏差和用户的历史评分，预测用户对未评分的物品的评分。</p>
<p><img src="https://img-blog.csdn.net/20160715114054480" alt="这里写图片描述"></p>
<p>Step3:将预测评分排序，取topN对应的物品推荐给用户。</p>
<p><strong>举例：</strong><br>假设有100个人对物品A和物品B打分了，R(AB)表示这100个人对A和B打分的平均偏差;有1000个人对物品B和物品C打分了， R(CB)表示这1000个人对C和B打分的平均偏差；</p>
<p><img src="https://img-blog.csdn.net/20160715114619049" alt="这里写图片描述"></p>
<h2 id="slope-one-的代码实现"><a href="#slope-one-的代码实现" class="headerlink" title="slope one 的代码实现"></a>slope one 的代码实现</h2><p>1.准备数据</p>
<p><img src="https://camo.githubusercontent.com/68d8995d1a9bacf4e58fa39359de71cbb99e3bf5abc5174bd1033e8b93fdae81/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832373135303233373932312e706e67237069635f63656e746572" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    rating_data=&#123;<span class="number">1</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">                 <span class="number">2</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">                 <span class="number">3</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">                 <span class="number">4</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">                 <span class="number">5</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">              &#125;</span><br><span class="line">    <span class="keyword">return</span> rating_data	</span><br><span class="line">users_rating = loadData()</span><br><span class="line">users_rating</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;1: &#123;&#x27;A&#x27;: 5, &#x27;B&#x27;: 3, &#x27;C&#x27;: 4, &#x27;D&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string"> 2: &#123;&#x27;A&#x27;: 3, &#x27;B&#x27;: 1, &#x27;C&#x27;: 2, &#x27;D&#x27;: 3, &#x27;E&#x27;: 3&#125;,</span></span><br><span class="line"><span class="string"> 3: &#123;&#x27;A&#x27;: 4, &#x27;B&#x27;: 3, &#x27;C&#x27;: 4, &#x27;D&#x27;: 3, &#x27;E&#x27;: 5&#125;,</span></span><br><span class="line"><span class="string"> 4: &#123;&#x27;A&#x27;: 3, &#x27;B&#x27;: 3, &#x27;C&#x27;: 1, &#x27;D&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string"> 5: &#123;&#x27;A&#x27;: 1, &#x27;B&#x27;: 5, &#x27;C&#x27;: 5, &#x27;D&#x27;: 2, &#x27;E&#x27;: 1&#125;&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>2.建立倒排索引</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立倒排索引</span></span><br><span class="line">items_rating = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user,ratings <span class="keyword">in</span> users_rating.items():</span><br><span class="line">    <span class="keyword">for</span> item,rating <span class="keyword">in</span> ratings.items():</span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> items_rating:</span><br><span class="line">            items_rating[item] = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> items_rating[item]:</span><br><span class="line">            items_rating[item][user] = <span class="number">0</span></span><br><span class="line">        items_rating[item][user] = rating</span><br><span class="line">items_rating</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;&#x27;A&#x27;: &#123;1: 5, 2: 3, 3: 4, 4: 3, 5: 1&#125;,</span></span><br><span class="line"><span class="string"> &#x27;B&#x27;: &#123;1: 3, 2: 1, 3: 3, 4: 3, 5: 5&#125;,</span></span><br><span class="line"><span class="string"> &#x27;C&#x27;: &#123;1: 4, 2: 2, 3: 4, 4: 1, 5: 5&#125;,</span></span><br><span class="line"><span class="string"> &#x27;D&#x27;: &#123;1: 4, 2: 3, 3: 3, 4: 5, 5: 2&#125;,</span></span><br><span class="line"><span class="string"> &#x27;E&#x27;: &#123;2: 3, 3: 5, 4: 4, 5: 1&#125;&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>3.计算物品间评分偏差矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算物品间评分偏差</span></span><br><span class="line">Ratings_diffs = &#123;&#125;	<span class="comment"># 评分偏差矩阵</span></span><br><span class="line">N_set = &#123;&#125;			<span class="comment"># 物品对间共同被评分次数</span></span><br><span class="line"><span class="keyword">for</span> itemx,itemx_history <span class="keyword">in</span> items_rating.items():</span><br><span class="line">    <span class="keyword">if</span> itemx <span class="keyword">not</span> <span class="keyword">in</span> Ratings_diffs:</span><br><span class="line">        Ratings_diffs[itemx] = &#123;&#125;</span><br><span class="line">        N_set[itemx] = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> itemy,itemy_history <span class="keyword">in</span> items_rating.items():</span><br><span class="line">        <span class="keyword">if</span> itemx != itemy:</span><br><span class="line">            Ratings_diffs[itemx][itemy] = <span class="number">0</span></span><br><span class="line">            N_set[itemx][itemy] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x_user <span class="keyword">in</span> itemx_history:</span><br><span class="line">                <span class="keyword">if</span> x_user <span class="keyword">in</span> itemy_history:</span><br><span class="line">                    Ratings_diffs[itemx][itemy] += items_rating[itemy][x_user] - items_rating[itemx][x_user]</span><br><span class="line">                    N_set[itemx][itemy] += <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> itemx,ys <span class="keyword">in</span> Ratings_diffs.items():</span><br><span class="line">    <span class="keyword">for</span> itemy,rating <span class="keyword">in</span> ys.items():</span><br><span class="line">        Ratings_diffs[itemx][itemy] /= N_set[itemx][itemy]</span><br><span class="line"></span><br><span class="line">Ratings_diffs,N_set</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(&#123;&#x27;A&#x27;: &#123;&#x27;B&#x27;: -0.2, &#x27;C&#x27;: 0.0, &#x27;D&#x27;: 0.2, &#x27;E&#x27;: 0.5&#125;,</span></span><br><span class="line"><span class="string">  &#x27;B&#x27;: &#123;&#x27;A&#x27;: 0.2, &#x27;C&#x27;: 0.2, &#x27;D&#x27;: 0.4, &#x27;E&#x27;: 0.25&#125;,</span></span><br><span class="line"><span class="string">  &#x27;C&#x27;: &#123;&#x27;A&#x27;: 0.0, &#x27;B&#x27;: -0.2, &#x27;D&#x27;: 0.2, &#x27;E&#x27;: 0.25&#125;,</span></span><br><span class="line"><span class="string">  &#x27;D&#x27;: &#123;&#x27;A&#x27;: -0.2, &#x27;B&#x27;: -0.4, &#x27;C&#x27;: -0.2, &#x27;E&#x27;: 0.0&#125;,</span></span><br><span class="line"><span class="string">  &#x27;E&#x27;: &#123;&#x27;A&#x27;: -0.5, &#x27;B&#x27;: -0.25, &#x27;C&#x27;: -0.25, &#x27;D&#x27;: 0.0&#125;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;A&#x27;: &#123;&#x27;B&#x27;: 5, &#x27;C&#x27;: 5, &#x27;D&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string">  &#x27;B&#x27;: &#123;&#x27;A&#x27;: 5, &#x27;C&#x27;: 5, &#x27;D&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string">  &#x27;C&#x27;: &#123;&#x27;A&#x27;: 5, &#x27;B&#x27;: 5, &#x27;D&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string">  &#x27;D&#x27;: &#123;&#x27;A&#x27;: 5, &#x27;B&#x27;: 5, &#x27;C&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string">  &#x27;E&#x27;: &#123;&#x27;A&#x27;: 4, &#x27;B&#x27;: 4, &#x27;C&#x27;: 4, &#x27;D&#x27;: 4&#125;&#125;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>4.预测评分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预测评分</span></span><br><span class="line"><span class="comment"># 首先找出Alice交互过的物品哪些与要预测的物品有过”共同被统一用户评分“的经历，即存在倒排索引Ratings_item[x][y]</span></span><br><span class="line">A_history = users_rating[<span class="number">1</span>]</span><br><span class="line">candidate_items = <span class="built_in">set</span>()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> Ratings_diffs[<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    <span class="keyword">if</span> item <span class="keyword">in</span> A_history:</span><br><span class="line">        candidate_items.add(item)</span><br><span class="line">weighted_score = <span class="number">0</span></span><br><span class="line">weighted_sum = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> Ratings_diffs[<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    weighted_sum += N_set[<span class="string">&#x27;E&#x27;</span>][item]</span><br><span class="line">    weighted_score += Ratings_diffs[<span class="string">&#x27;E&#x27;</span>][item] * N_set[<span class="string">&#x27;E&#x27;</span>][item]</span><br><span class="line">predict = weighted_score/weighted_sum</span><br><span class="line">predict</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">-0.25</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="slope-one使用场景"><a href="#slope-one使用场景" class="headerlink" title="slope one使用场景"></a>slope one使用场景</h2><p>该算法适用于物品更新不频繁，数量相对较稳定并且物品数目明显小于用户数的场景。依赖用户的用户行为日志和物品偏好的相关内容。<br>优点：<br>1.算法简单，易于实现，执行效率高；<br>2.可以发现用户潜在的兴趣爱好；<br>缺点：<br>依赖用户行为，存在冷启动问题和稀疏性问题。</p>
<h2 id="比较Slope-One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？"><a href="#比较Slope-One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？" class="headerlink" title="比较Slope One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？"></a>比较Slope One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr>
<td>UserCF</td>
<td>3.80369</td>
</tr>
<tr>
<td>ItemCF</td>
<td>3.74319</td>
</tr>
<tr>
<td>SVD(MF)</td>
<td>3.75332</td>
</tr>
<tr>
<td>Slope One</td>
<td>3.91533</td>
</tr>
</tbody>
</table>
</div>
<h1 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h1><p><a href="https://blog.csdn.net/xidianliutingting/article/details/51916578">https://blog.csdn.net/xidianliutingting/article/details/51916578</a></p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础7：词向量基础</title>
    <url>/2022/04/27/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%807/</url>
    <content><![CDATA[<hr>
<h1 id="任务7：词向量基础"><a href="#任务7：词向量基础" class="headerlink" title="任务7：词向量基础"></a>任务7：词向量基础</h1><ul>
<li>学习<a href="https://cloud.tencent.com/developer/article/1486055">word2vec基础</a></li>
<li>将用户历史观看电影转为列表数据（一个用户一个列表）</li>
<li>使用gensim训练word2vec，然后对用户完成聚类</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h1 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h1><p>来源 | Analytics Vidhya 【磐创AI导读】：这篇文章主要介绍了如何使用word2vec构建推荐系统。想要获取更多的机器学习、深度学习资源，欢迎大家点击上方蓝字关注我们的公众号：磐创AI。</p>
<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a><strong>概览</strong></h2><ul>
<li>如今，推荐引擎无处不在，人们希望数据科学家知道如何构建一个推荐引擎</li>
<li>Word2vec是一个非常流行的词嵌入，用于执行各种NLP任务</li>
<li>我们将使用word2vec来构建我们自己的推荐系统。就让我们来看看NLP和推荐引擎是如何结合的吧！</li>
</ul>
<p>完整的代码可以从这里下载：</p>
<blockquote>
<p><a href="https://github.com/prateekjoshi565/recommendation_system/blob/master/recommender_2.ipynb">https://github.com/prateekjoshi565/recommendation_system/blob/master/recommender_2.ipynb</a></p>
</blockquote>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="　介绍"></a>　<strong>介绍</strong></h2><p>老实说，你在亚马逊上有注意到网站为你推荐的内容吗（Recommended for you部分)? 自从几年前我发现机器学习可以增强这部分内容以来，我就迷上了它。每次登录Amazon时，我都会密切关注该部分。</p>
<p>Netflix、谷歌、亚马逊、Flipkart等公司花费数百万美元完善他们的推荐引擎是有原因的，因为这是一个强大的信息获取渠道并且提高了消费者的体验。</p>
<p>让我用一个最近的例子来说明这种作用。我去了一个很受欢迎的网上市场购买一把躺椅，那里有各种各样的躺椅，我喜欢其中的大多数并点击了查看了一把人造革手动躺椅。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/aurj300r0x.jpeg" alt="img"></p>
<p>请注意页面上显示的不同类型的信息，图片的左半部分包含了不同角度的商品图片。右半部分包含有关商品的一些详细信息和部分类似的商品。</p>
<p>而这是我最喜欢的部分，该网站正在向我推荐类似的商品，这为我节省了手动浏览类似躺椅的时间。</p>
<p>在本文中，我们将构建自己的推荐系统。但是我们将从一个独特的视角来处理这个问题。我们将使用一个NLP概念—Word2vec,向用户推荐商品。如果你觉得这个教程让你有点小期待，那就让我们开始吧！</p>
<p>在文中，我会提及一些概念。我建议可以看一下以下这两篇文章来快速复习一下</p>
<blockquote>
<p><a href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/?utm_source=blog&amp;utm_medium=how-to-build-recommendation-system-word2vec-python">理解神经网络:</a></p>
<p><a href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/?utm_source=blog&amp;utm_medium=how-to-build-recommendation-system-word2vec-python">构建推荐引擎的综合指南 </a></p>
</blockquote>
<h2 id="word2vec-词的向量表示"><a href="#word2vec-词的向量表示" class="headerlink" title="word2vec - 词的向量表示"></a><strong>word2vec - 词的向量表示</strong></h2><p>我们知道机器很难处理原始文本数据。事实上，除了数值型数据，机器几乎不可能处理其他类型的数据。因此，以向量的形式表示文本几乎一直是所有NLP任务中最重要的步骤。</p>
<p>在这个方向上，最重要的步骤之一就是使用 word2vec embeddings，它是在2013年引入NLP社区的并彻底改变了NLP的整个发展。</p>
<p>事实证明，这些 embeddings在单词类比和单词相似性等任务中是最先进的。word2vec embeddings还能够实现像 <code>King - man +woman ~= Queen</code>之类的任务，这是一个非常神奇的结果。</p>
<p>有两种⁠word2vec模型——Continuous Bag of Words模型和Skip-Gram模型。在本文中，我们将使用Skip-Gram模型。</p>
<p>首先让我们了解word2vec向量或者说embeddings是怎么计算的。</p>
<h2 id="如何获得word2vec-embeddings"><a href="#如何获得word2vec-embeddings" class="headerlink" title="如何获得word2vec embeddings?"></a><strong>如何获得word2vec embeddings?</strong></h2><p>word2vec模型是一个简单的神经网络模型，其只有一个隐含层，该模型的任务是预测句子中每个词的近义词。然而，我们的目标与这项任务无关。我们想要的是一旦模型被训练好，通过模型的<strong>隐含层学习到的权重</strong>。然后可以将这些权重用作单词的embeddings。</p>
<p>让我举个例子来说明word2vec模型是如何工作的。请看下面这句话:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/39q26nmjqb.png" alt="img"></p>
<p>假设单词“teleport”(用黄色高亮显示)是我们的输入单词。它有一个大小为2的上下文窗口。这意味着我们只考虑输入单词两边相邻的两个单词作为邻近的单词。</p>
<p><em>注意:上下文窗口的大小不是固定的，可以根据我们的需要进行更改。</em></p>
<p>现在，任务是逐个选择邻近的单词(上下文窗口中的单词)，并给出词汇表中每个单词成为选中的邻近单词的概率。这听起来应该挺直观的吧？</p>
<p>让我们再举一个例子来详细了解整个过程。</p>
<h4 id="准备训练数据"><a href="#准备训练数据" class="headerlink" title="准备训练数据"></a><strong>准备训练数据</strong></h4><p>我们需要一个标记数据集来训练神经网络模型。这意味着数据集应该有一组输入和对应输入的输出。在这一点上，你可能有一些问题，像:</p>
<ul>
<li>在哪里可以找到这样的数据集?</li>
<li>这个数据集包含什么?</li>
<li>这个数据有多大?</li>
</ul>
<p>等等。</p>
<p>然而我要告诉你的是：我们可以轻松地创建自己的标记数据来训练word2vec模型。下面我将演示如何从任何文本生成此数据集。让我们使用一个句子并从中创建训练数据。</p>
<p><strong>第一步</strong>: 黄色高亮显示的单词将作为输入，绿色高亮显示的单词将作为输出单词。我们将使用2个单词的窗口大小。让我们从第一个单词作为输入单词开始。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/uqzmaohs5l.png" alt="img"></p>
<p>所以，关于这个输入词的训练样本如下:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/jikh7horxy.png" alt="img"></p>
<p><strong>第二步</strong>: 接下来，我们将第二个单词作为输入单词。上下文窗口也会随之移动。现在，邻近的单词是“we”、“become”和“what”。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/rrq9cfy6m9.png" alt="img"></p>
<p>新的训练样本将会被添加到之前的训练样本中，如下所示:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/q4sl5f7xlo.png" alt="img"></p>
<p>我们将重复这些步骤，直到最后一个单词。最后，这句话的完整训练数据如下:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/djmiyovdx5.png" alt="img"></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/sflnd9gu81.png" alt="img"></p>
<p>我们从一个句子中抽取了27个训练样本，这是我喜欢处理非结构化数据的许多方面之一——凭空创建了一个标记数据集。</p>
<h4 id="获得-word2vec-Embeddings"><a href="#获得-word2vec-Embeddings" class="headerlink" title="获得 word2vec Embeddings"></a><strong>获得 word2vec Embeddings</strong></h4><p>现在，假设我们有一堆句子，我们用同样的方法从这些句子中提取训练样本。我们最终将获得相当大的训练数据。</p>
<p>假设这个数据集中有5000个惟一的单词，我们希望为每个单词创建大小为100维的向量。然后，对于下面给出的word2vec架构:</p>
<ul>
<li>V = 5000(词汇量)</li>
<li>N = 100(隐藏单元数量或单词embeddings长度)</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/78ugqeahim.png" alt="img"></p>
<p>输入将是一个<strong>热编码向量</strong>，而输出层将给出词汇表中<strong>每个单词都在其附近的概率</strong>。</p>
<p>一旦对该模型进行训练，我们就可以很容易地提取学习到的权值矩阵 </p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/455ulptdh8.png" alt="img"></p>
<p>x N，并用它来提取单词向量:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/n01f88stii.png" alt="img"></p>
<p>正如你在上面看到的，权重矩阵的形状为5000 x 100。这个矩阵的第一行对应于词汇表中的第一个单词，第二个对应于第二个单词，以此类推。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/9sffijf6gc.png" alt="img"></p>
<p>这就是我们如何通过word2vec得到固定大小的词向量或embeddings。这个数据集中相似的单词会有相似的向量，即指向相同方向的向量。例如，单词“car”和“jeep”有类似的向量:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/b5dbjph3bi.png" alt="img"></p>
<p>这是对word2vec如何在NLP中使用的高级概述。</p>
<p>在我们开始构建推荐系统之前，让我问你一个问题。如何将word2vec用于非nlp任务，如商品推荐?我相信自从你读了这篇文章的标题后，你就一直在想这个问题。让我们一起解出这个谜题。</p>
<h2 id="在非文本数据上应用word2vec模型"><a href="#在非文本数据上应用word2vec模型" class="headerlink" title="在非文本数据上应用word2vec模型"></a><strong>在非文本数据上应用word2vec模型</strong></h2><p>你能猜到word2vec用来创建文本向量表示的自然语言的基本特性吗?</p>
<p>是<strong>文本的顺序性</strong>。每个句子或短语都有一个单词序列。如果没有这个顺序，我们将很难理解文本。试着解释下面这句话:</p>
<blockquote>
<p>“these most been languages deciphered written of have already”</p>
</blockquote>
<p>这个句子没有顺序，我们很难理解它，这就是为什么在任何自然语言中，单词的顺序是如此重要。正是这个特性让我想到了其他不像文本具有顺序性质的数据。</p>
<p>其中一类数据是<strong>消费者在电子商务网站的购买行为</strong>。大多数时候，消费者的购买行为都有一个模式，例如，一个从事体育相关活动的人可能有一个类似的在线购买模式:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/e7v1rjzihf.png" alt="img"></p>
<p>如果我们可以用向量表示每一个商品，那么我们可以很容易地找到相似的商品。因此，如果用户在网上查看一个商品，那么我们可以通过使用商品之间的向量相似性评分轻松地推荐类似商品。</p>
<p>但是我们如何得到这些商品的向量表示呢?我们可以用word2vec模型来得到这些向量吗?</p>
<p>答案当然是可以的! 把消费者的购买历史想象成一句话，而把商品想象成这句话的单词:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/v2vl4bkiqr.png" alt="img"></p>
<p>更进一步，让我们研究在线零售数据，并使用word2vec构建一个推荐系统。</p>
<h2 id="案例研究-使用Python中的word2vec进行在线商品推荐"><a href="#案例研究-使用Python中的word2vec进行在线商品推荐" class="headerlink" title="案例研究:使用Python中的word2vec进行在线商品推荐"></a><strong>案例研究:使用Python中的word2vec进行在线商品推荐</strong></h2><p>现在让我们再一次确定我们的问题和需求：</p>
<p>我们被要求创建一个系统，根据消费者过去的购买行为，自动向电子商务网站的消费者推荐一定数量的商品。</p>
<p>我们将使用一个在线零售数据集，你可以从这个链接下载:</p>
<blockquote>
<p><a href="https://archive.ics.uci.edu/ml/machine-learning-databases/00352/">https://archive.ics.uci.edu/ml/machine-learning-databases/00352/</a></p>
</blockquote>
<p>详细代码见：<a href="https://github.com/Guadzilla/recpre/blob/master/task7/word2vec_demo.ipynb">recpre/word2vec_demo.ipynb at master · Guadzilla/recpre (github.com)</a> 因为和原文差不多，就不重复介绍了。</p>
<h1 id="在-Movielens-数据集上用-Word2Vec-对用户聚类"><a href="#在-Movielens-数据集上用-Word2Vec-对用户聚类" class="headerlink" title="在 Movielens 数据集上用 Word2Vec 对用户聚类"></a>在 Movielens 数据集上用 Word2Vec 对用户聚类</h1><p>见 word2vec_cluster.ipynb</p>
<h3 id="导入相关包"><a href="#导入相关包" class="headerlink" title="导入相关包"></a>导入相关包</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> umap</span><br><span class="line"><span class="keyword">import</span> umap.plot</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)	</span><br></pre></td></tr></table></figure>
<h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h3><p>最后要得到：所有用户的列表 <code>users=[user1,user2,...]</code>，语料库<code>corpus=[[item1,item2,...],[item2,item5,...],...]</code>，和商品字典：<code>items_dict=&#123;item1:&quot;item1的描述&quot;, item2:&quot;item2的描述&quot;,...&#125;</code></p>
<p>这里的语料库其实就是每个用户的购买序列组成的列表，我们把每个商品看作一个词，用户的一个购买序列看作一句话，通过这种方式构建语料库。</p>
<p>构建商品字典是为了方便后续查看相似物品信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    df = pd.read_table(<span class="string">&#x27;../ml-1m/ratings.dat&#x27;</span>, sep=<span class="string">&#x27;::&#x27;</span>, names = [<span class="string">&#x27;userID&#x27;</span>,<span class="string">&#x27;itemID&#x27;</span>,<span class="string">&#x27;Rating&#x27;</span>,<span class="string">&#x27;Zip-code&#x27;</span>])</span><br><span class="line">    movies = pd.read_table(<span class="string">&#x27;../ml-1m/movies.dat&#x27;</span>,sep=<span class="string">&#x27;::&#x27;</span>,names=[<span class="string">&#x27;MovieID&#x27;</span>,<span class="string">&#x27;Title&#x27;</span>,<span class="string">&#x27;Genres&#x27;</span>],encoding=<span class="string">&#x27;ISO-8859-1&#x27;</span>)</span><br><span class="line">    movies[<span class="string">&#x27;content&#x27;</span>] = movies[<span class="string">&#x27;Title&#x27;</span>] + <span class="string">&#x27;__&#x27;</span> + movies[<span class="string">&#x27;Genres&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有userID</span></span><br><span class="line">    users = df[<span class="string">&quot;userID&quot;</span>].unique().tolist()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储user的购买历史,每个user对应一个list,每个list当作一句话,所有list作为语料库</span></span><br><span class="line">    corpus = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(users):</span><br><span class="line">        temp = df[df[<span class="string">&quot;userID&quot;</span>] == i][<span class="string">&quot;itemID&quot;</span>].tolist()</span><br><span class="line">        corpus.append(temp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 建立商品字典,方便后续查看相似物品信息</span></span><br><span class="line">    items_dict = movies.groupby(<span class="string">&#x27;MovieID&#x27;</span>)[<span class="string">&#x27;content&#x27;</span>].apply(<span class="built_in">list</span>).to_dict()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> users, corpus, items_dict</span><br></pre></td></tr></table></figure>
<p>Movielens-1m 数据集很完整，没有缺失值要处理。</p>
<h3 id="训练-Word2Vec"><a href="#训练-Word2Vec" class="headerlink" title="训练 Word2Vec"></a>训练 Word2Vec</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练word2vec模型</span></span><br><span class="line">model = Word2Vec(window = <span class="number">10</span>, sg = <span class="number">1</span>, hs = <span class="number">0</span>, negative = <span class="number">10</span>, alpha=<span class="number">0.03</span>, min_alpha=<span class="number">0.0007</span>, seed = <span class="number">14</span>)</span><br><span class="line">model.build_vocab(corpus, progress_per=<span class="number">200</span>)</span><br><span class="line">model.train(corpus, total_examples = model.corpus_count, epochs=<span class="number">10</span>, report_delay=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 模型训练完成, init_sims()提高内存运行效率</span></span><br><span class="line">model.init_sims(replace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="查看模型参数"><a href="#查看模型参数" class="headerlink" title="查看模型参数"></a>查看模型参数</h3><p>共有3416个 item embedding ,每个维度为100.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打印模型</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Word2Vec(vocab=3416, vector_size=100, alpha=0.03)</span></span><br></pre></td></tr></table></figure>
<h3 id="查看相似物品"><a href="#查看相似物品" class="headerlink" title="查看相似物品"></a>查看相似物品</h3><p>首先提取所有向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取向量</span></span><br><span class="line">X = model.wv[model.wv.key_to_index.keys()]</span><br></pre></td></tr></table></figure>
<p>查看 movies 信息，这里我们就选第一个电影 “Toy Story”，看看它的相似电影</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">movies = pd.read_table(<span class="string">&#x27;../ml-1m/movies.dat&#x27;</span>,sep=<span class="string">&#x27;::&#x27;</span>,names=[<span class="string">&#x27;MovieID&#x27;</span>,<span class="string">&#x27;Title&#x27;</span>,<span class="string">&#x27;Genres&#x27;</span>],encoding=<span class="string">&#x27;ISO-8859-1&#x27;</span>)</span><br><span class="line">movies.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427172027663.png" alt="image-20220427172027663" style="zoom:67%;" /></p>
<p>Toy Story 的 key 就是对应的 MovieID = 1，通过模型内置的字典找到它对应的索引为 29</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.wv.key_to_index[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 29</span></span><br></pre></td></tr></table></figure>
<p>计算并返回与它相似的10部电影</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similar_products</span>(<span class="params">v, n = <span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回最相似的n个物品</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 为输入向量提取最相似的商品</span></span><br><span class="line">    ms = model.wv.similar_by_vector(v, topn= n+<span class="number">1</span>)[<span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># 提取相似产品的名称和相似度评分</span></span><br><span class="line">    new_ms = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> ms:</span><br><span class="line">        pair = (items_dict[j[<span class="number">0</span>]][<span class="number">0</span>], j[<span class="number">1</span>])</span><br><span class="line">        new_ms.append(pair)</span><br><span class="line">    <span class="keyword">return</span> new_ms   </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> similar_products(X[<span class="number">29</span>]):</span><br><span class="line">    <span class="built_in">print</span>(i[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Aladdin (1992)__Animation|Children&#x27;s|Comedy|Musical</span></span><br><span class="line"><span class="string">Silence of the Lambs, The (1991)__Drama|Thriller</span></span><br><span class="line"><span class="string">Train of Life (Train De Vie) (1998)__Comedy|Drama</span></span><br><span class="line"><span class="string">Home Alone (1990)__Children&#x27;s|Comedy</span></span><br><span class="line"><span class="string">Jumanji (1995)__Adventure|Children&#x27;s|Fantasy</span></span><br><span class="line"><span class="string">Waiting to Exhale (1995)__Comedy|Drama</span></span><br><span class="line"><span class="string">Ghost (1990)__Comedy|Romance|Thriller</span></span><br><span class="line"><span class="string">Beavis and Butt-head Do America (1996)__Animation|Comedy</span></span><br><span class="line"><span class="string">Tom and Huck (1995)__Adventure|Children&#x27;s</span></span><br><span class="line"><span class="string">Brady Bunch Movie, The (1995)__Comedy</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>Toy Story 的类别是 Animation|Children’s|Comedy ，与它相似的电影也大都是动画片、儿童电影，说明 word2vec 一定程度上的确给相似的电影学到了相似的embedding.</p>
<p>其中第2部 Silence of the Lambs , 现实中喜欢玩具总动员的大多是有冒险精神的大人，所以性格的另一面喜欢沉默的羔羊也是可以的（狡辩）</p>
<h3 id="对物品和用户向量进行聚类"><a href="#对物品和用户向量进行聚类" class="headerlink" title="对物品和用户向量进行聚类"></a>对物品和用户向量进行聚类</h3><p>定义用来聚类和画图的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 两种画图方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_emb</span>(<span class="params">X</span>):</span></span><br><span class="line">    cluster_embedding = umap.UMAP(n_neighbors=<span class="number">30</span>, min_dist=<span class="number">0.0</span>,</span><br><span class="line">    n_components=<span class="number">2</span>, random_state=<span class="number">42</span>).fit_transform(X)</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">9</span>))</span><br><span class="line">    plt.scatter(cluster_embedding[:, <span class="number">0</span>], cluster_embedding[:, <span class="number">1</span>], s=<span class="number">3</span>, cmap=<span class="string">&#x27;Spectral&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">umap_plot_emb</span>(<span class="params">X</span>):</span></span><br><span class="line">    cluster_embedding = umap.UMAP(n_neighbors=<span class="number">30</span>, min_dist=<span class="number">0.0</span>,</span><br><span class="line">    n_components=<span class="number">2</span>, random_state=<span class="number">42</span>).fit(X)</span><br><span class="line">    umap.plot.points(cluster_embedding)</span><br></pre></td></tr></table></figure>
<h4 id="可视化物品向量"><a href="#可视化物品向量" class="headerlink" title="可视化物品向量"></a>可视化物品向量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_emb(X)</span><br><span class="line">umap_plot_emb(X)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427164950421.png" alt="image-20220427164950421" style="zoom:50%;" /><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165014070.png" alt="image-20220427165014070" style="zoom: 41%;" /></p>
<p>居然类似一个圆？意思是接近随机生成吗？</p>
<h4 id="可视化用户向量"><a href="#可视化用户向量" class="headerlink" title="可视化用户向量"></a>可视化用户向量</h4><p>用户的向量可以用看过的电影的向量求均值表示，也可以对一部分求均值表示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate_vectors</span>(<span class="params">products</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回用户向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    product_vec = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> products:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            product_vec.append(model.wv[i])</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">return</span> np.mean(product_vec, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">usersVec_all_item = []  <span class="comment"># 对看过的所有电影的向量求均值</span></span><br><span class="line">usersVec_last_10 = []   <span class="comment"># 对看过的最后10个电影的向量求均值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(users)):</span><br><span class="line">    usersVec_all_item.append(aggregate_vectors(corpus[i]))</span><br><span class="line">    usersVec_last_10.append(aggregate_vectors(corpus[i][-<span class="number">10</span>:]))</span><br></pre></td></tr></table></figure>
<p>用户向量 = 看过的所有电影的向量的均值，可视化结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_emb(usersVec_all_item)</span><br><span class="line">umap_plot_emb(usersVec_all_item)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165406864.png" alt="image-20220427165406864" style="zoom:50%;" /><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165450274.png" alt="image-20220427165450274" style="zoom:41%;" /></p>
<p>用户向量 = 看过的最后10部电影的向量的均值，可视化结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_emb(usersVec_last_10)</span><br><span class="line">umap_plot_emb(usersVec_last_10)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165724232.png" alt="image-20220427165724232" style="zoom:50%;" /><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165735996.png" alt="image-20220427165735996" style="zoom:41%;" /></p>
<p>至此完成了利用word2vec训练得到物品的embedding，再用物品embedding表示用户embedding，最后对用户embedding聚类。</p>
<h1 id="在-Movielens-数据集上用-Word2Vec-对用户进行推荐"><a href="#在-Movielens-数据集上用-Word2Vec-对用户进行推荐" class="headerlink" title="在 Movielens 数据集上用 Word2Vec 对用户进行推荐"></a>在 Movielens 数据集上用 Word2Vec 对用户进行推荐</h1><p>见 word2vec_rec.py</p>
<p>如果要对用户推荐商品，就要比上面的简单对用户进行聚类稍微麻烦一点，因为需要划分训练集和验证集。划分数据集的代码如下：</p>
<p>对数据集进行划分，我们在这一步需要获取的是，训练集、测试集出现了哪些用户，训练集的语料库，以及商品字典。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    data = pd.read_table(<span class="string">&#x27;../ml-1m/ratings.dat&#x27;</span>, sep=<span class="string">&#x27;::&#x27;</span>, names = [<span class="string">&#x27;userID&#x27;</span>,<span class="string">&#x27;itemID&#x27;</span>,<span class="string">&#x27;Rating&#x27;</span>,<span class="string">&#x27;Zip-code&#x27;</span>])</span><br><span class="line">    movies = pd.read_table(<span class="string">&#x27;../ml-1m/movies.dat&#x27;</span>,sep=<span class="string">&#x27;::&#x27;</span>,names=[<span class="string">&#x27;MovieID&#x27;</span>,<span class="string">&#x27;Title&#x27;</span>,<span class="string">&#x27;Genres&#x27;</span>],encoding=<span class="string">&#x27;ISO-8859-1&#x27;</span>)</span><br><span class="line">    movies[<span class="string">&#x27;content&#x27;</span>] = movies[<span class="string">&#x27;Title&#x27;</span>] + <span class="string">&#x27;__&#x27;</span> + movies[<span class="string">&#x27;Genres&#x27;</span>]</span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    tra_data, val_data = train_test_split(data, test_size=<span class="number">0.2</span>)</span><br><span class="line">    users_train = tra_data[<span class="string">&#x27;userID&#x27;</span>].unique().tolist()</span><br><span class="line">    users_valid = val_data[<span class="string">&#x27;userID&#x27;</span>].unique().tolist()</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 存储消费者的购买历史</span></span><br><span class="line">    train_users = &#123;&#125;</span><br><span class="line">    train_corpus = []</span><br><span class="line">    <span class="comment"># 用 itemID 填充列表</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(users_train):</span><br><span class="line">        temp = tra_data[tra_data[<span class="string">&quot;userID&quot;</span>] == i][<span class="string">&quot;itemID&quot;</span>].tolist()</span><br><span class="line">        train_users[i] = temp</span><br><span class="line">        train_corpus.append(temp)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;验证集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 存储消费者的购买历史</span></span><br><span class="line">    valid_users = &#123;&#125;</span><br><span class="line">    valid_corpus = []</span><br><span class="line">    <span class="comment"># 用商品代码填充列表</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(users_valid):</span><br><span class="line">        temp = val_data[val_data[<span class="string">&quot;userID&quot;</span>] == i][<span class="string">&quot;itemID&quot;</span>].tolist()</span><br><span class="line">        valid_users[i] = temp</span><br><span class="line">        valid_corpus.append(temp)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;建立商品字典&quot;&quot;&quot;</span></span><br><span class="line">    items_dict = movies.groupby(<span class="string">&#x27;MovieID&#x27;</span>)[<span class="string">&#x27;content&#x27;</span>].apply(<span class="built_in">list</span>).to_dict()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_users, train_corpus, valid_users, valid_corpus, items_dict</span><br></pre></td></tr></table></figure>
<p>其它步骤与前面的相似，具体看代码吧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">train_users, train_corpus, valid_users, valid_corpus, items_dict = load_data(<span class="string">&#x27;../ml-1m&#x27;</span>)</span><br><span class="line"><span class="comment"># train_users: &#123;user1:[item1,item2,...],user2:[item2,item5,...],...&#125;</span></span><br><span class="line"><span class="comment"># train_corpus: [[item1,item2,...],[item2,item5,...],...]</span></span><br><span class="line"><span class="comment"># 训练word2vec模型</span></span><br><span class="line">model = Word2Vec(window = <span class="number">10</span>, sg = <span class="number">1</span>, hs = <span class="number">0</span>, negative = <span class="number">10</span>, alpha=<span class="number">0.03</span>, min_alpha=<span class="number">0.0007</span>, seed = <span class="number">14</span>)</span><br><span class="line">model.build_vocab(train_corpus, progress_per=<span class="number">200</span>)</span><br><span class="line">model.train(train_corpus, total_examples = model.corpus_count, epochs=<span class="number">10</span>, report_delay=<span class="number">1</span>)</span><br><span class="line">model.init_sims(replace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 打印模型</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># 提取向量</span></span><br><span class="line">X = model.wv[model.wv.key_to_index.keys()]</span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line"><span class="comment">#visualize_emb(X)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similar_products_idx</span>(<span class="params">v, n = <span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回最相似的n个物品</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 为输入向量提取最相似的商品</span></span><br><span class="line">    ms = model.wv.similar_by_vector(v, topn= n+<span class="number">1</span>)[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> ms  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate_vectors</span>(<span class="params">products</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回购买记录的平均向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    product_vec = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> products:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            product_vec.append(model.wv[i])</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">return</span> np.mean(product_vec, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 推荐TopN相似商品</span></span><br><span class="line">rec_dict = &#123;&#125;</span><br><span class="line">rel_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> valid_users:    <span class="comment"># valid_users:&#123;user1:[item1,item2,...],user2:[item2,item5,...],...&#125;</span></span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> train_users:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    user_vec = aggregate_vectors(train_users[user][-<span class="number">10</span>:])</span><br><span class="line">    similar_items = similar_products_idx(user_vec,<span class="number">10</span>)</span><br><span class="line">    rec_dict[user] = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> similar_items]</span><br><span class="line">    rel_dict[user] = valid_users[user]</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">rec_eval(rec_dict,rel_dict,train_users)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>评估结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">recall: <span class="number">1.31</span></span><br><span class="line">precision <span class="number">4.34</span></span><br><span class="line">coverage <span class="number">71.29</span></span><br><span class="line">Popularity <span class="number">4.93</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础8：向量召回基础</title>
    <url>/2022/04/27/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%808/</url>
    <content><![CDATA[<hr>
<h1 id="任务8：向量召回基础"><a href="#任务8：向量召回基础" class="headerlink" title="任务8：向量召回基础"></a>任务8：向量召回基础</h1><ul>
<li>基于任务7的基础上，使用编码后的用户向量，计算用户相似度。</li>
<li>参考User-CF的过程，通过用户相似度得到电影推荐</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<p>任务 7 中，我们在 Movielens-1m 数据集上用 word2vec 训练得到了 电影的”词向量”（物品向量、item embedding，都是一个意思，下面都用 item embedding），再对用户所有看过的电影的 embedding 取均值作为”句向量“（用户向量，user embedding）。</p>
<p>在任务 7 的 word2vec_rec.py 里，实现了在 Movielens 数据集上用 Word2Vec 对用户进行推荐，主要步骤为：<strong>划分数据集、训练word2vec模型、计算用户向量</strong>、TopN推荐（计算相似物品）。</p>
<p>任务 8 就以它为基础，稍作修改，主要步骤为：<strong>划分数据集、训练word2vec模型、计算用户向量</strong>、计算用户相似度、建立向量库、构建用户相似性矩阵、利用UserCF算法进行TopN推荐。因为前三步骤和之前类似，不做过多介绍，这里重点介绍后四步。</p>
<h2 id="计算用户相似度"><a href="#计算用户相似度" class="headerlink" title="计算用户相似度"></a>计算用户相似度</h2><p>计算用户相似度的思路是，利用 faiss 库建立向量库，存入所有 user embedding，再用 user embedding 对向量库进行检索，取出 TopK 个最相似的用户，用户后续的 UserCF 推荐。</p>
<p>因为用 faiss 构建向量库会对 userID 重新从0开始索引，取出结果也是根据重新索引后的 index 来取，所以有必要提前对训练集的 userID 重建索引，用一个字典映射来实现。</p>
<p>实际上在 load 数据集的时候已经对 userID 和 itemID 重新索引了，这里为什么还要再做一次索引呢？</p>
<p>答：为了方便索引，使得训练集和验证集可以直接对向量库进行索引。具体来说，划分数据集会造成训练集和验证集上的用户数量不一致，也就是说可能有一部分用户只在验证集出现，训练集里没有他。例如：全部数据集有10个用户，并且已经对他们从零开始编号，<code>userID=[0,1,2,3,4,5,6,7,8,9]</code>。随机划分数据集以后，训练集里可能就只有<code>userID = [0,2,3,4,5,6,7,8]</code> 这8个用户了，验证集里只有 <code>userID = [0,1,2,3,4,5,6,7,9]</code> 9个用户。此时如果直接把训练集的 user embedding 直接放到索引库里，faiss 为向量库构建的索引为<code>index = [0,1,2,3,4,5,6,7]</code>，对应关系是<code>index=0对应userID=0</code>，<code>index=1对应userID=2</code>，…，这个对应关系必须保存下来，否则验证集只有 userID ，无法定位到 userID 对应哪个索引，也就无法提取该 user embedding。如果用字典 <code>MAP(key=userID,value=index)</code>保存下来这个对应关系，在训练集上就可以用<code>MAP[userID]</code>作为索引从向量库提取 user embedding ；验证集上，首先把在验证集首次出现的用户单独保存，剩下<code>userID = [0,2,3,4,5,6,7]</code>，其余一样。</p>
<p>在这里使用了另一种思路，先对训练集 <code>userID = [0,2,3,4,5,6,7,8]</code> 重新索引成 <code>userID = [0,1,2,3,4,5,6,7]</code> ，并保存下这个字典<code>MAP(key=userID,value=index)</code>，再把验证集  <code>userID = [0,1,2,3,4,5,6,7,9]</code> 的 userID 都 MAP 到 index 上，其中 1 和 9 在训练集上没有出现，单独保存，剩下的<code>userID = [0,2,3,4,5,6,7]</code>再做映射，变成 <code>userID = [0,1,2,3,4,5,6]</code>，这样做方便在以后就可以直接用 userID 访问向量库。</p>
<p>两种方式都可以，第一种方式的缺点是频繁访问字典，但优点是不用 in place 修改数据；第二种方式正好相反，不用频繁访问字典，但是需要 in place 修改数据。个人习惯用第二种方式，一劳永逸。</p>
<p>下面是代码部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TopK = <span class="number">100</span></span><br><span class="line">TopN = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对train_user重建索引, 求用户向量, 求重索引的new_train_users, 只需要一次循环就能做完</span></span><br><span class="line"><span class="comment"># 已有 train_users  # train_users: &#123;user1:[item1,item2,...], user2:[item3,item4,...],...&#125;</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">MAP = <span class="built_in">dict</span>()   <span class="comment"># 对train_user重建索引,从0开始   MAP: &#123;userID:faiss_idx, ...&#125;</span></span><br><span class="line">usersVec = []  <span class="comment"># 求用户向量, aggregate_vectors():对看过的所有电影的向量求均值</span></span><br><span class="line">new_train_users = &#123;&#125;    <span class="comment"># new_train_users: &#123;faiss_idx1:[item1,item2,...],faiss_idx2:[item3,item4,...],...&#125;</span></span><br><span class="line"><span class="keyword">for</span> user,items <span class="keyword">in</span> train_users.items():</span><br><span class="line">    MAP[user] = count</span><br><span class="line">    new_train_users[count] = items</span><br><span class="line">    usersVec.append(aggregate_vectors(train_corpus[count]))</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line"><span class="keyword">del</span> train_users</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对valid_users的userid按MAP重索引,得到new_valid_users: &#123;faiss_idx1:[item1,item2,...],faiss_idx2:[item3,item4,...],...&#125;</span></span><br><span class="line">new_valid_users = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user,items <span class="keyword">in</span> valid_users.items():</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> new_train_users:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    new_valid_users[MAP[user]] = items</span><br><span class="line"><span class="keyword">del</span> valid_users</span><br></pre></td></tr></table></figure>
<h2 id="建立向量索引库"><a href="#建立向量索引库" class="headerlink" title="建立向量索引库"></a>建立向量索引库</h2><p>要用 faiss 计算余弦相似度，需要注意的是 faiss 自带的两种常见相似算法是：<code>faiss.IndexFlatL2()</code>用来计算向量距离，和<code>faiss.IndexFlatIP()</code>用来计算向量内积。计算余弦相似度可以用向量内积形式，但前提是需要先把向量转成单位向量，faiss 自带了 <code>faiss.normalize_L2()</code>就是用来单位化向量的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立用户向量索引库</span></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">usersVec = np.array(usersVec)</span><br><span class="line">normalize_L2(usersVec)   <span class="comment"># 这是inplace操作 (计算余弦相似度,先正则化,再计算内积)</span></span><br><span class="line">index = faiss.IndexFlatIP(<span class="number">100</span>)</span><br><span class="line">index.add(usersVec)</span><br></pre></td></tr></table></figure>
<h2 id="构建用户相似性矩阵"><a href="#构建用户相似性矩阵" class="headerlink" title="构建用户相似性矩阵"></a>构建用户相似性矩阵</h2><p>构建用户相似性的思路是用字典保存，因为矩阵太稀疏，浪费空间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找TopK相似用户</span></span><br><span class="line">D, I = index.search(usersVec[<span class="built_in">list</span>(new_valid_users.keys())], TopK+<span class="number">1</span>) <span class="comment"># TopK要+1,因为底下计算相似度会计算自身一次</span></span><br><span class="line">similar_users_idxs = I</span><br><span class="line">similar_users = &#123;&#125;  <span class="comment"># similar_users: &#123;faiss_idx1:&#123;faiss_idx2:score,faiss_idx4:score,...&#125;,faiss_idx2:&#123;...&#125;,...&#125;</span></span><br><span class="line"><span class="keyword">for</span> idx,val_user <span class="keyword">in</span> <span class="built_in">enumerate</span>(new_valid_users):</span><br><span class="line">    similar_users[val_user] = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> iidx,s_user <span class="keyword">in</span> <span class="built_in">enumerate</span>(similar_users_idxs[idx]):</span><br><span class="line">        similar_users[val_user][s_user] = D[idx][iidx]</span><br></pre></td></tr></table></figure>
<h2 id="利用-UserCF-算法进行-TopN-推荐"><a href="#利用-UserCF-算法进行-TopN-推荐" class="headerlink" title="利用 UserCF 算法进行 TopN 推荐"></a>利用 UserCF 算法进行 TopN 推荐</h2><p>UserCF 的思路，先找到和目标用户最相似的 TopK 个用户（相似度用于计算目标用户对陌生物品的得分）再计算这些用户看过的、且目标用户没看过的电影的得分，对有得分的物品进行排序，进行 TopN 推荐。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 推荐TopN相似商品</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;开始生成推荐列表...&#x27;</span>)</span><br><span class="line">rec_dict = &#123;&#125;</span><br><span class="line">rel_dict = new_valid_users</span><br><span class="line"><span class="keyword">for</span> val_user <span class="keyword">in</span> tqdm(new_valid_users):   <span class="comment"># new_valid_users: &#123;faiss_idx1:[item1,item2,...],faiss_idx2:[item3,item4,...],...&#125;</span></span><br><span class="line">    rec_dict[val_user] = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> similar_users[val_user]:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> new_train_users[user]:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> new_train_users[val_user]:</span><br><span class="line">                <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> rec_dict[val_user]:</span><br><span class="line">                    rec_dict[val_user][item]=<span class="number">0</span></span><br><span class="line">                rec_dict[val_user][item] += similar_users[val_user][user]</span><br><span class="line"><span class="comment"># rec_dict: &#123;faiss_idx1:&#123;item2:score,item4:score,...&#125;,faiss_idx2:&#123;...&#125;,...&#125;</span></span><br><span class="line"><span class="comment"># 先选出每个user的TopN &quot;item-score&quot; 对，再提出item到最后的推荐列表, 变换后的rec_dict: &#123;faiss_idx1:[item2,item4,...],faiss_idx2:[item3,item4,...],...&#125;</span></span><br><span class="line">rec_dict = &#123;k: <span class="built_in">sorted</span>(v.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:TopN] <span class="keyword">for</span> k, v <span class="keyword">in</span> rec_dict.items()&#125;</span><br><span class="line">rec_dict = &#123;k: <span class="built_in">list</span>([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> v]) <span class="keyword">for</span> k, v <span class="keyword">in</span> rec_dict.items()&#125;</span><br></pre></td></tr></table></figure>
<p>最后进行评估，验证集上实验结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">rec_eval(rec_dict,rel_dict,new_train_users)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">recall: 6.01</span></span><br><span class="line"><span class="string">precision 19.91</span></span><br><span class="line"><span class="string">coverage 9.38</span></span><br><span class="line"><span class="string">Popularity 7.459</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>完整代码见 <a href="https://github.com/Guadzilla/recpre/tree/master/task8/word2vec_recall.py">recpre/task8 at master · Guadzilla/recpre (github.com)</a></p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础9：多路召回实践</title>
    <url>/2022/04/28/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%809/</url>
    <content><![CDATA[<hr>
<h1 id="任务9：多路召回实践"><a href="#任务9：多路召回实践" class="headerlink" title="任务9：多路召回实践"></a>任务9：多路召回实践</h1><ul>
<li>基于任务3、任务5、任务6、任务7、任务8，总共5个召回模型，进行多路召回。</li>
<li>可以考虑对每个召回模型的物品打分进行相加，也可以加权求和。</li>
<li>分别计算每个模型 &amp; 多路召回模型的Top10、Top20、Top50的命中率。</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<p>根据前面完成的任务，5个召回模型分别是 ItemCF、UserCF、MF、SlopeOne、Word2Vec。</p>
<p><strong>任务目标</strong>：对用户是否评分做出预测，即评分只有0和1（数据集中用户评分过的电影评分都取1，未评分过的电影评分都取0），进行推荐 TopN 推荐。</p>
<p>这样的话 SlopeOne 没法做了，因为评分只有0和1，有评分的哪些物品的评分都是1，物品评分之间没有均差。当然也可以把任务改为预测5分制的评分；或者两阶段预测，先预测是否会评分，再预测会评多少分。那样都需要对模型做不少修改，暂时没有精力做，图省事就只实现了4个召回模型 ItemCF、UserCF、MF、Word2Vec。</p>
<p><strong>进行多路召回（模型融合），步骤大致分为以下几步</strong>：</p>
<ol>
<li>划分数据集并保存</li>
<li>各模型读取数据进行训练</li>
<li>各模型进行预测，保存评估指标、保存预测结果</li>
<li>读取所有预测结果进行加权（取均值、或者训练得到权重），进行最终评估</li>
</ol>
<h2 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>多路召回，首先保证各个模型上数据集划分是一致的。所以考虑把划分数据集这一部分独立出来单独运行。划分完的数据要保存下来，方便后续各个模型加载并独立做实验。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_and_save</span>(<span class="params">load_path, save_path, test_rate = <span class="number">0.1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载数据，分割训练集、验证集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data = pd.read_table(os.path.join(load_path,<span class="string">&#x27;ratings.dat&#x27;</span>), sep=<span class="string">&#x27;::&#x27;</span>, names = [<span class="string">&#x27;userID&#x27;</span>,<span class="string">&#x27;itemID&#x27;</span>,<span class="string">&#x27;Rating&#x27;</span>,<span class="string">&#x27;Zip-code&#x27;</span>])</span><br><span class="line">    data = data[[<span class="string">&#x27;userID&#x27;</span>,<span class="string">&#x27;itemID&#x27;</span>]]</span><br><span class="line">    uid_lbe = LabelEncoder()</span><br><span class="line">    data[<span class="string">&#x27;userID&#x27;</span>] = uid_lbe.fit_transform(data[<span class="string">&#x27;userID&#x27;</span>])</span><br><span class="line">    iid_lbe = LabelEncoder()</span><br><span class="line">    data[<span class="string">&#x27;itemID&#x27;</span>] = iid_lbe.fit_transform(data[<span class="string">&#x27;itemID&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    train_data, valid_data = train_test_split(data,test_size=test_rate)</span><br><span class="line">    </span><br><span class="line">    train_users = train_data.groupby(<span class="string">&#x27;userID&#x27;</span>)[<span class="string">&#x27;itemID&#x27;</span>].apply(<span class="built_in">list</span>).to_dict()</span><br><span class="line">    valid_users = valid_data.groupby(<span class="string">&#x27;userID&#x27;</span>)[<span class="string">&#x27;itemID&#x27;</span>].apply(<span class="built_in">list</span>).to_dict()</span><br><span class="line"></span><br><span class="line">    train_data.to_csv(<span class="string">&#x27;train_data.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">    valid_data.to_csv(<span class="string">&#x27;valid_data.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">    pickle.dump(train_users, <span class="built_in">open</span>(os.path.join(save_path,<span class="string">&#x27;train_users.txt&#x27;</span>), <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    pickle.dump(valid_users, <span class="built_in">open</span>(os.path.join(save_path,<span class="string">&#x27;valid_users.txt&#x27;</span>), <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line"></span><br><span class="line">load_and_save(load_path=<span class="string">&#x27;../ml-1m&#x27;</span>, save_path=<span class="string">&#x27;./data&#x27;</span>, test_rate=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="训练各模型"><a href="#训练各模型" class="headerlink" title="训练各模型"></a>训练各模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ItemCF.py   	--&gt;   基于物品的协同过滤算法</span><br><span class="line">UserCF.py   	--&gt;   基于用户的协同过滤算法</span><br><span class="line">MF.py			--&gt;   梯度下降矩阵分解算法</span><br><span class="line">Word2Vec.py		--&gt;	  word2vec算法</span><br><span class="line"><span class="comment"># 各模型细节就不展示了</span></span><br></pre></td></tr></table></figure>
<h2 id="保存各模型指标"><a href="#保存各模型指标" class="headerlink" title="保存各模型指标"></a>保存各模型指标</h2><p>所有模型的 TopK 都取100，各模型在验证集上的评估指标为：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220429174656385.png" alt="image-20220429174656385"></p>
<p>保存各模型的预测结果，注意保存的是没有截断 TopN 的推荐列表：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_rec_dict</span>(<span class="params">save_path,rec_dict</span>):</span></span><br><span class="line">    pickle.dump(rec_dict, <span class="built_in">open</span>(os.path.join(save_path,<span class="string">&#x27;word2vec_rec_dict.txt&#x27;</span>), <span class="string">&#x27;wb&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>保存的格式为字典：<code>Top50_rec_dict=&#123;uid1:&#123;iid1:score,iid3:score,...&#125;, uid2:&#123;iid2:score,iid3:score,...&#125;,...&#125;</code></p>
<h2 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h2><p>观察各模型保存的预测得分结果，发现 MF 和另外几个算法得到的数据的值域不在同一区间，如果直接取均值的话， MF 的影响就很小了，所以先对每个用户的物品得分列表做 softmax 再取均值</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220428202209856.png" alt="image-20220428202209856"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_rec_dict</span>(<span class="params">file_path, model_name</span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(file_path,<span class="string">f&#x27;<span class="subst">&#123;model_name&#125;</span>_rec_dict.txt&#x27;</span>),<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        Rec_dict = pickle.load(f)</span><br><span class="line">    <span class="comment"># Rec_dict:&#123;uid1:[(iid1,score),(iid2,score),...],uid2:[(...),(...),...],...&#125;</span></span><br><span class="line">    new_rec_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> uid, iid_score_list <span class="keyword">in</span> Rec_dict.items():</span><br><span class="line">        new_rec_dict[uid] = <span class="built_in">dict</span>()</span><br><span class="line">        score_list = []</span><br><span class="line">        iid_list = []</span><br><span class="line">        <span class="keyword">for</span> iid, score <span class="keyword">in</span> iid_score_list.items():</span><br><span class="line">            score_list.append(score)</span><br><span class="line">            iid_list.append(iid)</span><br><span class="line">        prob = softmax(score_list)      <span class="comment"># 把物品评分转化成概率</span></span><br><span class="line">        <span class="keyword">for</span> iid, p <span class="keyword">in</span> <span class="built_in">zip</span>(iid_list, prob):</span><br><span class="line">            new_rec_dict[uid][iid] = p</span><br><span class="line">    <span class="keyword">return</span> new_rec_dict</span><br></pre></td></tr></table></figure>
<p>加权取平均融合：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mixup</span>(<span class="params">*rec_dicts</span>):</span></span><br><span class="line">    new_rec_dict = &#123;&#125;</span><br><span class="line">    num = &#123;&#125;    <span class="comment"># 记录目标用户被推荐同一商品几次，后续做除数</span></span><br><span class="line">    <span class="keyword">for</span> rec_dict <span class="keyword">in</span> tqdm(rec_dicts):</span><br><span class="line">        <span class="keyword">for</span> uid <span class="keyword">in</span> rec_dict:</span><br><span class="line">            <span class="keyword">if</span> uid <span class="keyword">not</span> <span class="keyword">in</span> new_rec_dict:</span><br><span class="line">                new_rec_dict[uid] = &#123;&#125;</span><br><span class="line">                num[uid] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> iid, prob <span class="keyword">in</span> rec_dict[uid].items():</span><br><span class="line">                <span class="keyword">if</span> iid <span class="keyword">not</span> <span class="keyword">in</span> new_rec_dict[uid]:</span><br><span class="line">                    new_rec_dict[uid][iid] = <span class="number">0</span></span><br><span class="line">                    num[uid][iid] = <span class="number">0</span></span><br><span class="line">                new_rec_dict[uid][iid] += prob</span><br><span class="line">                num[uid][iid] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> uid,iid_score <span class="keyword">in</span> new_rec_dict.items():</span><br><span class="line">        <span class="keyword">for</span> iid <span class="keyword">in</span> iid_score:</span><br><span class="line">            new_rec_dict[uid][iid] /= num[uid][iid]</span><br><span class="line">    <span class="keyword">return</span> new_rec_dict</span><br><span class="line"></span><br><span class="line">new_rec_dict = mixup(*models_rec_dict)</span><br></pre></td></tr></table></figure>
<p>TopN 推荐：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TopN推荐</span></span><br><span class="line">Top50_rec_dict = &#123;k: <span class="built_in">sorted</span>(v.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:<span class="number">50</span>] <span class="keyword">for</span> k, v <span class="keyword">in</span> new_rec_dict.items()&#125;</span><br><span class="line">Top50_rec_dict = &#123;k: <span class="built_in">list</span>([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> v]) <span class="keyword">for</span> k, v <span class="keyword">in</span> Top50_rec_dict.items()&#125;</span><br><span class="line">Top10_rec_dict = &#123;k: v[:<span class="number">10</span>] <span class="keyword">for</span> k, v <span class="keyword">in</span> Top50_rec_dict.items()&#125;</span><br><span class="line">Top20_rec_dict = &#123;k: v[:<span class="number">20</span>] <span class="keyword">for</span> k, v <span class="keyword">in</span> Top50_rec_dict.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算评价指标</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stack_eval</span>(<span class="params">val_rec_items, val_user_items, trn_user_items</span>):</span></span><br><span class="line">    recall = Recall(val_rec_items, val_user_items)</span><br><span class="line">    precision = Precision(val_rec_items, val_user_items)</span><br><span class="line">    coverage = Coverage(val_rec_items, trn_user_items)</span><br><span class="line">    popularity = Popularity(val_rec_items, trn_user_items)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;recall&#125;</span>\t<span class="subst">&#123;precision&#125;</span>\t<span class="subst">&#123;coverage&#125;</span>\t<span class="subst">&#123;popularity&#125;</span>&#x27;</span>,end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Top 10,20,50:&#x27;</span>)</span><br><span class="line">bag_eval(Top10_rec_dict,valid_users,train_users)</span><br><span class="line">bag_eval(Top20_rec_dict,valid_users,train_users)</span><br><span class="line">bag_eval(Top50_rec_dict,valid_users,train_users)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nDone.&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>实验结果：最优结果红色加粗，次优结果橙色加粗。前四行是单模型，后面的都是融合模型，取每个模型的首字母表示每个模型。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220429174808948.png" alt="image-20220429174808948"></p>
<p>居然是单模型UserCF最优…</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>模板</title>
    <url>/2021/10/17/%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[<p>中文空格用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&amp;emsp;</span><br></pre></td></tr></table></figure>
<p>替换</p>
<hr>
<p>原paper：</p>
<p>源码解读：</p>
<hr>
<p>中译：</p>
<p>总结：</p>
<hr>
<span id="more"></span>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>question作者想解决什么问题？ </li>
</ul>
<ul>
<li>method作者通过什么理论/模型来解决这个问题？</li>
</ul>
<ul>
<li>answer作者给出的答案是什么？</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>why作者为什么研究这个课题？    </li>
</ul>
<ul>
<li>how当前研究到了哪一阶段？</li>
</ul>
<ul>
<li>what作者基于什么样的假设（看不懂最后去查）？</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>优点 </li>
</ul>
<ul>
<li>缺点</li>
</ul>
<ul>
<li>展望</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 </li>
</ul>
<ul>
<li>重要指标 </li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li></li>
<li></li>
</ul>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><ul>
<li></li>
<li></li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><h2 id="一些备注"><a href="#一些备注" class="headerlink" title="一些备注"></a>一些备注</h2><p>数学公式不能正确显示的情况：<br>1.</p>
<script type="math/tex; mode=display">
\mathit{L}=\mathit{L}_{cr} + \lambda_1\mathit{L}_{in}+\lambda_2||\Theta||^2_2</script><p>在每个下划线 “ _ ” 前后各加一个空格就好了</p>
<script type="math/tex; mode=display">
\mathit{L}=\mathit{L} _ {cr} + \lambda_1\mathit{L} _ {in}+\lambda_2||\Theta||^2 _ 2</script><p>2.</p>
<p><img src="https://gitee.com/Guadzilla/img-hosting/raw/master/image-20211118185724771.png" alt="image-20211118185724771"></p>
]]></content>
  </entry>
</search>
